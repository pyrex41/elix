# LLM Pipeline System - Setup & Usage Guide

Welcome to the Ash-based LLM Pipeline System! This guide will help you get everything up and running.

## üéØ What's Been Built

A complete node-based pipeline system for LLM workflows, inspired by Flowise/Langflow:

- ‚úÖ **5 Ash Resources**: Pipeline, Node, Edge, PipelineRun, NodeResult
- ‚úÖ **3 Node Types**: Text, HTTP Request, LLM (OpenRouter + xAI)
- ‚úÖ **State Machines**: Automatic status management for runs and results
- ‚úÖ **Oban Background Jobs**: Parallel node execution with dependency resolution
- ‚úÖ **Liquid Templating**: Dynamic variable substitution
- ‚úÖ **RESTful JSON API**: Auto-generated by AshJsonApi
- ‚úÖ **Database Migrations**: All tables ready to go

---

## üìã Prerequisites

- Elixir 1.15+
- Phoenix 1.8+
- SQLite3

---

## üöÄ Installation Steps

### Step 1: Install Dependencies

```bash
cd backend
mix deps.get
```

This will install:
- `ash` - Declarative resource framework
- `ash_phoenix` - Phoenix integration
- `ash_json_api` - JSON API generator
- `ash_sqlite` - SQLite data layer
- `ash_state_machine` - State management
- `oban` - Background job processor
- `solid` - Liquid templating

### Step 2: Run Migrations

```bash
mix ecto.migrate
```

This creates:
- `oban_*` tables - Job queue infrastructure
- `pipelines` - Pipeline definitions
- `nodes` - Pipeline nodes/steps
- `edges` - Connections between nodes
- `pipeline_runs` - Execution instances
- `node_results` - Individual node outputs

### Step 3: Configure API Keys (Optional)

For LLM nodes to work, add API keys to your config:

**Option A: Environment Variables** (Recommended)

```bash
export OPENROUTER_API_KEY="your-key-here"
export XAI_API_KEY="your-xai-key-here"
```

**Option B: Config File** (`config/dev.exs`)

```elixir
config :backend,
  openrouter_api_key: "sk-or-...",
  xai_api_key: "xai-..."
```

### Step 4: Load Example Pipelines

```bash
mix run priv/repo/seeds_pipeline_examples.exs
```

This creates 4 example pipelines:
1. Simple Greeting Pipeline (Text only)
2. HTTP API Fetch Pipeline (HTTP ‚Üí Text)
3. LLM Content Generation Pipeline (Text ‚Üí LLM ‚Üí Text)
4. Image Description Pipeline (Text ‚Üí Vision LLM)

### Step 5: Start the Server

```bash
mix phx.server
```

Server runs at: http://localhost:4000

---

## üéÆ Usage Guide

### API Endpoints

All endpoints require API key authentication (same as existing v3 API).

Base URL: `http://localhost:4000/api/v3`

#### Pipelines

```bash
# List all pipelines
GET /pipelines

# Get specific pipeline
GET /pipelines/:id

# Create pipeline
POST /pipelines
{
  "data": {
    "type": "pipeline",
    "attributes": {
      "name": "My Pipeline",
      "description": "Does cool stuff",
      "status": "active"
    }
  }
}

# Execute pipeline
POST /pipelines/:id/execute
{
  "input_data": {
    "name": "World",
    "project": "Ash"
  }
}
```

#### Nodes

```bash
# Create node
POST /nodes
{
  "data": {
    "type": "node",
    "attributes": {
      "pipeline_id": "uuid-here",
      "name": "My Node",
      "type": "text",
      "config": {
        "content": "Hello {{name}}!"
      },
      "position": {"x": 100, "y": 100}
    }
  }
}

# Update node
PATCH /nodes/:id

# Delete node
DELETE /nodes/:id
```

#### Edges (Connections)

```bash
# Create edge
POST /edges
{
  "data": {
    "type": "edge",
    "attributes": {
      "pipeline_id": "uuid-here",
      "source_node_id": "source-uuid",
      "target_node_id": "target-uuid"
    }
  }
}

# Delete edge
DELETE /edges/:id
```

#### Pipeline Runs

```bash
# Get run status
GET /pipeline_runs/:id

# List all runs for a pipeline
GET /pipeline_runs?filter[pipeline_id]=uuid-here

# Cancel a run
POST /pipeline_runs/:id/cancel
```

#### Node Results

```bash
# Get results for a run
GET /node_results?filter[pipeline_run_id]=uuid-here

# Get specific result
GET /node_results/:id
```

---

## üß™ Testing Examples

### Example 1: Simple Text Pipeline

```bash
# Using curl (replace API_KEY)
curl -X POST http://localhost:4000/api/v3/pipelines/:id/execute \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "input_data": {
      "name": "Alice",
      "project": "Ash Framework"
    }
  }'

# Response:
{
  "run_id": "...",
  "status": "queued"
}

# Check status:
curl http://localhost:4000/api/v3/pipeline_runs/:run_id \
  -H "Authorization: Bearer YOUR_API_KEY"
```

### Example 2: HTTP API Pipeline

```bash
curl -X POST http://localhost:4000/api/v3/pipelines/:http_pipeline_id/execute \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "input_data": {
      "user_id": "1"
    }
  }'
```

This will:
1. Fetch user data from JSONPlaceholder API
2. Format the response as text
3. Return the final output

### Example 3: LLM Pipeline (Requires API Key)

```bash
curl -X POST http://localhost:4000/api/v3/pipelines/:llm_pipeline_id/execute \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "input_data": {
      "style": "haiku",
      "topic": "Elixir programming"
    }
  }'
```

This will:
1. Prepare a prompt with your inputs
2. Call Claude 3.5 Sonnet via OpenRouter
3. Format the generated poem
4. Return the final text

---

## üèóÔ∏è Building Your Own Pipelines

### Node Type: Text

**Purpose:** Output static or templated text

**Config:**
```json
{
  "content": "Hello {{name}}, you have {{count}} messages."
}
```

**Input Variables:** Any data from previous nodes or pipeline input
**Output:** `{"text": "rendered content"}`

---

### Node Type: HTTP Request

**Purpose:** Call external APIs

**Config:**
```json
{
  "url": "https://api.example.com/{{endpoint}}",
  "method": "POST",
  "headers": {
    "Authorization": "Bearer {{api_key}}",
    "Content-Type": "application/json"
  },
  "body": "{\"data\": \"{{input_data}}\"}"
}
```

**Input Variables:** Used in URL, headers, and body templates
**Output:** `{"status": 200, "headers": {...}, "body": {...}}`

---

### Node Type: LLM

**Purpose:** Call LLM APIs (OpenRouter, xAI)

**Config:**
```json
{
  "provider": "openrouter",
  "model": "anthropic/claude-3-5-sonnet",
  "system_prompt": "You are a helpful assistant",
  "user_prompt": "{{user_input}}",
  "temperature": 0.7,
  "max_tokens": 1000
}
```

**Supported Providers:**
- `openrouter` - Access 200+ models via OpenRouter
- `xai` - Use Grok models directly

**Input Variables:** Used in system and user prompts
**Output:** `{"text": "LLM response", "model": "...", "provider": "..."}`

---

## üîç Monitoring & Debugging

### Check Oban Dashboard

If you enabled LiveDashboard in development:

```
http://localhost:4000/dev/dashboard
```

Navigate to "Oban" to see:
- Queued jobs
- Running jobs
- Failed jobs
- Retry attempts

### Check Logs

```bash
tail -f backend/log/dev.log
```

Look for:
- `[PipelineCoordinator]` - Pipeline orchestration
- `[NodeExecutor]` - Individual node execution
- `[LlmNode]` - LLM API calls

### Database Queries

```elixir
# In IEx console
iex -S mix phx.server

# List all pipelines
alias Backend.Pipelines.Resources.Pipeline
Ash.read!(Pipeline)

# Get pipeline with nodes and edges
pipeline = Ash.get!(Pipeline, "uuid-here")
pipeline = Ash.load!(pipeline, [:nodes, :edges, :runs])

# Check run status
alias Backend.Pipelines.Resources.PipelineRun
run = Ash.get!(PipelineRun, "run-uuid")
run = Ash.load!(run, [:node_results])

# See all node results
alias Backend.Pipelines.Resources.NodeResult
results = Ash.read!(NodeResult, filter: [pipeline_run_id: "run-uuid"])
```

---

## üêõ Troubleshooting

### Issue: "No API key found for provider"

**Solution:** Set environment variables or config:

```bash
export OPENROUTER_API_KEY="sk-or-..."
export XAI_API_KEY="xai-..."
```

### Issue: "Node execution failed: Failed to render template"

**Solution:** Check your Liquid template syntax in node config:
- Use `{{variable}}` for variables
- Nested access: `{{body.user.name}}`
- Ensure input variables exist

### Issue: Pipeline run stuck in "running" status

**Solution:** Check Oban jobs:
1. Visit `/dev/dashboard` ‚Üí Oban
2. Look for failed jobs in the `nodes` or `pipelines` queue
3. Check error messages
4. Retry manually or fix the issue and cancel/restart the run

### Issue: "Cannot connect a node to itself"

**Solution:** Edge validation prevents cycles. Make sure:
- Source and target nodes are different
- Both nodes are in the same pipeline
- No circular dependencies

---

## üìä Architecture Overview

```
User Request
    ‚Üì
Phoenix Router (API Key Auth)
    ‚Üì
AshJsonApi (Auto-generated endpoints)
    ‚Üì
Ash Resources (Pipeline, Node, Edge, etc.)
    ‚Üì
Pipeline.execute() action
    ‚Üì
Enqueue PipelineCoordinator Oban Job
    ‚Üì
    ‚îú‚îÄ‚Üí Build dependency graph
    ‚îú‚îÄ‚Üí Find ready nodes
    ‚îú‚îÄ‚Üí Enqueue NodeExecutor jobs (parallel)
    ‚Üì
NodeExecutor Oban Job
    ‚Üì
    ‚îú‚îÄ‚Üí Gather inputs from previous nodes
    ‚îú‚îÄ‚Üí Execute node (Text/HTTP/LLM)
    ‚îú‚îÄ‚Üí Store output in NodeResult
    ‚Üì
PipelineCoordinator checks completion
    ‚Üì
Update PipelineRun status
    ‚Üì
Return results to user
```

---

## üé® Future Enhancements

Ideas for extending the system:

1. **UI** - Phoenix LiveView + SvelteFlow for visual pipeline builder
2. **More Node Types**:
   - Condition nodes (branching)
   - Transform nodes (JSONPath, jq-style)
   - Loop nodes (iterate over arrays)
   - Image generation nodes
   - Vector database nodes
3. **Triggers** - Webhooks, cron schedules, event-driven
4. **Versioning** - Pipeline version control
5. **Templates** - Pre-built pipeline templates
6. **Monitoring** - Detailed metrics and dashboards
7. **Caching** - Cache node results to save API costs

---

## üìö Key Files Reference

### Resources
- `lib/backend/pipelines/resources/pipeline.ex` - Pipeline resource
- `lib/backend/pipelines/resources/node.ex` - Node resource
- `lib/backend/pipelines/resources/edge.ex` - Edge resource
- `lib/backend/pipelines/resources/pipeline_run.ex` - PipelineRun with state machine
- `lib/backend/pipelines/resources/node_result.ex` - NodeResult with state machine

### Node Executors
- `lib/backend/pipelines/node_executor.ex` - Executor protocol
- `lib/backend/pipelines/node_types/text_node.ex` - Text node
- `lib/backend/pipelines/node_types/http_node.ex` - HTTP node
- `lib/backend/pipelines/node_types/llm_node.ex` - LLM node

### Oban Jobs
- `lib/backend/pipelines/jobs/pipeline_coordinator.ex` - Orchestration
- `lib/backend/pipelines/jobs/node_executor_job.ex` - Node execution

### Migrations
- `priv/repo/migrations/20251123210000_create_oban_tables.exs`
- `priv/repo/migrations/20251123210001_create_pipeline_tables.exs`

---

## ü§ù Contributing

To add a new node type:

1. Create `lib/backend/pipelines/node_types/my_node.ex`
2. Implement `Backend.Pipelines.NodeExecutor` behaviour
3. Add to `node_executor.ex` routing
4. Update `Node` resource validation
5. Add to database enum constraint in migration

Example skeleton:

```elixir
defmodule Backend.Pipelines.NodeTypes.MyNode do
  @behaviour Backend.Pipelines.NodeExecutor

  @impl true
  def execute(node, inputs, _context) do
    # Your logic here
    {:ok, output_data, metadata}
  end

  @impl true
  def validate_config(node) do
    # Validation logic
    :ok
  end
end
```

---

## üéâ You're Ready!

The entire LLM Pipeline system is now set up and ready to use. Start by:

1. Running the migrations: `mix ecto.migrate`
2. Loading example pipelines: `mix run priv/repo/seeds_pipeline_examples.exs`
3. Starting the server: `mix phx.server`
4. Testing the API with curl or your favorite HTTP client

Enjoy building amazing LLM workflows! üöÄ

**Questions?** Check `ASH_PIPELINE_PLAN.md` for detailed architecture documentation.
