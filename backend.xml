This file is a merged representation of a subset of the codebase, containing specifically included files, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of a subset of the repository's contents that is considered the most important context.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: backend/**/*.py
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
backend/
  api/
    v3/
      __init__.py
      models.py
      router.py
  cache/
    __init__.py
    redis_cache.py
    sqlite_cache.py
    test_redis_cache.py
    USAGE_EXAMPLE.py
  migrations/
    __init__.py
    add_asset_blob_storage.py
    add_clients_campaigns.py
    add_luigi_state_table.py
    add_source_url.py
    add_thumbnail_blob_id.py
    add_video_job_fields.py
    consolidate_assets_table.py
    run_add_blob_data.py
  models/
    __init__.py
    usage_example.py
    video_generation.py
  prompt_parser_service/
    api/
      v1/
        __init__.py
        batch.py
        briefs.py
        cache_admin.py
        health.py
        metrics.py
        parse.py
        providers.py
        upload.py
      __init__.py
    core/
      __init__.py
      config.py
      dependencies.py
      limiter.py
      logging.py
      metrics.py
    models/
      __init__.py
      request.py
      response.py
    prompts/
      __init__.py
      creative_direction.py
    services/
      llm/
        __init__.py
        base.py
        claude_provider.py
        mock_provider.py
        openai_provider.py
        openrouter_provider.py
      parsers/
        __init__.py
        text_parser.py
      __init__.py
      cache.py
      content_safety.py
      cost_estimator.py
      defaults.py
      edit_handler.py
      image_processor.py
      input_orchestrator.py
      media_utils.py
      scene_generator.py
      validator.py
      video_processor.py
    __init__.py
    main.py
  schemas/
    __init__.py
    assets.py
  services/
    __init__.py
    asset_downloader.py
    musicgen_client.py
    property_photo_selector.py
    replicate_client.py
    scene_audio_generator.py
    scene_generator.py
    scene_prompts.py
    storyboard_generator.py
    sub_job_orchestrator.py
    test_replicate_client.py
    test_storyboard_generator.py
    test_video_renderer.py
    video_combiner.py
    video_exporter.py
    video_renderer.py
    xai_client.py
  tests/
    __init__.py
    test_scene_endpoints.py
    test_scene_generation.py
  workflows/
    __init__.py
    base.py
    campaign_pipeline.py
    fastapi_integration.py
    runner.py
    tasks.py
  __init__.py
  add_team_users.py
  api_routes.py
  asset_metadata.py
  auth.py
  config.py
  database_helpers.py
  database.py
  genesis_renderer.py
  llm_interpreter.py
  main.py
  migrate.py
  scene_converter.py
  setup_auth.py
  test_add_entity.py
  test_genesis_api.py
  test_unified_upload.py
  test_video_models.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="backend/cache/__init__.py">
"""Cache layer for job progress tracking - SQLite-based POC implementation."""

from .sqlite_cache import (
    get_job_with_cache,
    update_job_progress_with_cache,
    invalidate_job_cache,
    invalidate_user_jobs_cache,
    get_cache_stats,
    cleanup_expired,
)

# For compatibility with existing code that checks redis_available
redis_available = False  # We're using SQLite, not Redis

__all__ = [
    "get_job_with_cache",
    "update_job_progress_with_cache",
    "invalidate_job_cache",
    "invalidate_user_jobs_cache",
    "get_cache_stats",
    "cleanup_expired",
    "redis_available",
]
</file>

<file path="backend/cache/redis_cache.py">
"""
Redis caching layer for job progress tracking.

This module provides Redis-based caching to reduce database load from frequent
job status polling. Implements graceful fallback to direct database queries if
Redis is unavailable.

Features:
- Job response caching with 30-second TTL
- Cache invalidation on job updates
- Connection pooling with automatic retry
- Graceful degradation when Redis is unavailable
- Cache statistics tracking
"""

import json
import logging
from typing import Optional, Dict, Any
from datetime import datetime

try:
    import redis
    from redis import ConnectionPool, Redis
    REDIS_AVAILABLE = True
except ImportError:
    REDIS_AVAILABLE = False
    redis = None
    ConnectionPool = None
    Redis = None

from ..config import get_settings
from ..database import get_job, update_job_progress

logger = logging.getLogger(__name__)

# Cache configuration
CACHE_TTL_SECONDS = 30
JOB_CACHE_KEY_PREFIX = "job"
USER_JOBS_KEY_PREFIX = "jobs"
STATS_KEY = "cache:stats"

# Global connection pool
_redis_pool: Optional[ConnectionPool] = None
_redis_client: Optional[Redis] = None
_redis_enabled = False

# Cache statistics
_cache_stats = {
    "hits": 0,
    "misses": 0,
    "errors": 0,
    "invalidations": 0
}


def _initialize_redis() -> bool:
    """
    Initialize Redis connection pool.

    Returns:
        True if Redis is available and initialized, False otherwise
    """
    global _redis_pool, _redis_client, _redis_enabled

    if not REDIS_AVAILABLE:
        logger.warning("redis-py library not installed, caching disabled")
        return False

    if _redis_client is not None:
        return _redis_enabled

    try:
        settings = get_settings()
        redis_url = settings.REDIS_URL

        # Create connection pool
        _redis_pool = ConnectionPool.from_url(
            redis_url,
            max_connections=10,
            decode_responses=True,
            socket_connect_timeout=2,
            socket_timeout=2
        )

        # Create Redis client
        _redis_client = Redis(connection_pool=_redis_pool)

        # Test connection
        _redis_client.ping()

        _redis_enabled = True
        logger.info(f"Redis cache initialized successfully: {redis_url}")
        return True

    except Exception as e:
        logger.warning(f"Redis initialization failed, caching disabled: {e}")
        _redis_enabled = False
        _redis_client = None
        _redis_pool = None
        return False


def redis_available() -> bool:
    """
    Check if Redis is available and operational.

    Returns:
        True if Redis is available, False otherwise
    """
    if not _redis_enabled:
        _initialize_redis()

    return _redis_enabled


def _get_redis_client() -> Optional[Redis]:
    """
    Get Redis client instance, initializing if necessary.

    Returns:
        Redis client instance or None if unavailable
    """
    global _redis_client

    if _redis_client is None:
        _initialize_redis()

    return _redis_client if _redis_enabled else None


def _serialize_job_response(job: Dict[str, Any]) -> str:
    """
    Serialize job response to JSON string for caching.

    Args:
        job: Job dictionary from database

    Returns:
        JSON string representation
    """
    # Create a copy to avoid modifying original
    job_copy = job.copy()

    # Convert datetime objects to ISO format strings
    for key in ["created_at", "updated_at", "approved_at"]:
        if key in job_copy and job_copy[key]:
            if isinstance(job_copy[key], datetime):
                job_copy[key] = job_copy[key].isoformat()

    return json.dumps(job_copy)


def _deserialize_job_response(json_str: str) -> Dict[str, Any]:
    """
    Deserialize job response from JSON string.

    Args:
        json_str: JSON string from cache

    Returns:
        Job dictionary
    """
    job = json.loads(json_str)

    # Convert ISO format strings back to strings (database layer expects strings)
    # The conversion to datetime objects is handled by the API response models

    return job


def get_job_with_cache(job_id: int) -> Optional[Dict[str, Any]]:
    """
    Retrieve job from cache or database.

    This function first checks Redis cache. On cache miss, it fetches from
    the database and stores in cache for future requests.

    Args:
        job_id: The video job ID

    Returns:
        Job dictionary or None if not found
    """
    cache_key = f"{JOB_CACHE_KEY_PREFIX}:{job_id}:progress"
    client = _get_redis_client()

    # Try cache first
    if client:
        try:
            cached_data = client.get(cache_key)
            if cached_data:
                _cache_stats["hits"] += 1
                logger.debug(f"Cache HIT for job {job_id}")
                return _deserialize_job_response(cached_data)
            else:
                _cache_stats["misses"] += 1
                logger.debug(f"Cache MISS for job {job_id}")
        except Exception as e:
            _cache_stats["errors"] += 1
            logger.warning(f"Redis GET error for job {job_id}: {e}")
            # Continue to database fallback

    # Cache miss or Redis unavailable - fetch from database
    job = get_job(job_id)

    if job and client:
        # Store in cache for future requests
        try:
            serialized = _serialize_job_response(job)
            client.setex(cache_key, CACHE_TTL_SECONDS, serialized)
            logger.debug(f"Cached job {job_id} with TTL={CACHE_TTL_SECONDS}s")
        except Exception as e:
            _cache_stats["errors"] += 1
            logger.warning(f"Redis SETEX error for job {job_id}: {e}")

    return job


def update_job_progress_with_cache(job_id: int, progress: dict) -> bool:
    """
    Update job progress in database and invalidate cache.

    This function updates the database using the existing update_job_progress()
    function, then invalidates the Redis cache to ensure fresh data on next read.
    Optionally pre-warms the cache with updated data.

    Args:
        job_id: The video job ID
        progress: Dictionary containing progress information

    Returns:
        True on success, False on failure
    """
    # Update database first
    success = update_job_progress(job_id, progress)

    if not success:
        return False

    # Invalidate cache
    invalidate_job_cache(job_id)

    # Optional: Pre-warm cache with updated data
    client = _get_redis_client()
    if client:
        try:
            # Fetch fresh data from database
            job = get_job(job_id)
            if job:
                cache_key = f"{JOB_CACHE_KEY_PREFIX}:{job_id}:progress"
                serialized = _serialize_job_response(job)
                client.setex(cache_key, CACHE_TTL_SECONDS, serialized)
                logger.debug(f"Pre-warmed cache for job {job_id} after update")
        except Exception as e:
            _cache_stats["errors"] += 1
            logger.warning(f"Cache pre-warm error for job {job_id}: {e}")

    return True


def invalidate_job_cache(job_id: int) -> None:
    """
    Invalidate Redis cache for a specific job.

    This should be called whenever job data is updated to ensure
    clients receive fresh data.

    Args:
        job_id: The video job ID
    """
    client = _get_redis_client()
    if not client:
        return

    try:
        cache_key = f"{JOB_CACHE_KEY_PREFIX}:{job_id}:progress"
        deleted = client.delete(cache_key)
        _cache_stats["invalidations"] += 1

        if deleted:
            logger.debug(f"Invalidated cache for job {job_id}")
        else:
            logger.debug(f"No cache entry to invalidate for job {job_id}")
    except Exception as e:
        _cache_stats["errors"] += 1
        logger.warning(f"Cache invalidation error for job {job_id}: {e}")


def invalidate_user_jobs_cache(client_id: str) -> None:
    """
    Invalidate Redis cache for all jobs belonging to a user/client.

    This is useful when user-level job lists need to be refreshed.
    Currently implemented as a pattern-based delete.

    Args:
        client_id: The client identifier
    """
    client = _get_redis_client()
    if not client:
        return

    try:
        # Delete user's job list cache
        user_cache_key = f"{USER_JOBS_KEY_PREFIX}:{client_id}"
        deleted = client.delete(user_cache_key)
        _cache_stats["invalidations"] += 1

        if deleted:
            logger.debug(f"Invalidated jobs cache for client {client_id}")
        else:
            logger.debug(f"No jobs cache entry for client {client_id}")

        # Note: We don't invalidate individual job caches here as they may be
        # shared across users (jobs are accessed by ID, not client_id)

    except Exception as e:
        _cache_stats["errors"] += 1
        logger.warning(f"User jobs cache invalidation error for client {client_id}: {e}")


def get_cache_stats() -> Dict[str, Any]:
    """
    Get cache performance statistics.

    Returns:
        Dictionary containing cache metrics:
        - hits: Number of cache hits
        - misses: Number of cache misses
        - errors: Number of Redis errors
        - invalidations: Number of cache invalidations
        - hit_rate: Cache hit rate percentage
        - redis_enabled: Whether Redis is currently enabled
        - redis_available: Whether Redis library is installed
    """
    total_requests = _cache_stats["hits"] + _cache_stats["misses"]
    hit_rate = (_cache_stats["hits"] / total_requests * 100) if total_requests > 0 else 0.0

    return {
        "hits": _cache_stats["hits"],
        "misses": _cache_stats["misses"],
        "errors": _cache_stats["errors"],
        "invalidations": _cache_stats["invalidations"],
        "hit_rate": round(hit_rate, 2),
        "total_requests": total_requests,
        "redis_enabled": _redis_enabled,
        "redis_available": REDIS_AVAILABLE,
        "ttl_seconds": CACHE_TTL_SECONDS
    }


def reset_cache_stats() -> None:
    """
    Reset cache statistics counters.

    This is useful for testing or periodic statistics reporting.
    """
    global _cache_stats
    _cache_stats = {
        "hits": 0,
        "misses": 0,
        "errors": 0,
        "invalidations": 0
    }
    logger.info("Cache statistics reset")


def close_redis_connection() -> None:
    """
    Close Redis connection pool.

    This should be called when shutting down the application.
    """
    global _redis_pool, _redis_client, _redis_enabled

    if _redis_client:
        try:
            _redis_client.close()
            logger.info("Redis client closed")
        except Exception as e:
            logger.warning(f"Error closing Redis client: {e}")

    if _redis_pool:
        try:
            _redis_pool.disconnect()
            logger.info("Redis connection pool disconnected")
        except Exception as e:
            logger.warning(f"Error disconnecting Redis pool: {e}")

    _redis_client = None
    _redis_pool = None
    _redis_enabled = False
</file>

<file path="backend/cache/sqlite_cache.py">
"""
SQLite-based cache for job progress tracking
Simple POC implementation - no external dependencies needed
"""

import sqlite3
import json
import time
from pathlib import Path
from typing import Optional, Dict, Any
import logging

logger = logging.getLogger(__name__)

# Cache configuration
CACHE_TTL = 30  # seconds
DB_PATH = Path(__file__).parent.parent / "DATA" / "cache.db"

def _get_connection():
    """Get SQLite connection for cache database"""
    DB_PATH.parent.mkdir(parents=True, exist_ok=True)
    conn = sqlite3.connect(str(DB_PATH))
    conn.row_factory = sqlite3.Row
    return conn

def _init_cache_table():
    """Initialize cache table if it doesn't exist"""
    conn = _get_connection()
    try:
        conn.execute("""
            CREATE TABLE IF NOT EXISTS job_cache (
                cache_key TEXT PRIMARY KEY,
                data TEXT NOT NULL,
                expires_at REAL NOT NULL
            )
        """)
        conn.execute("CREATE INDEX IF NOT EXISTS idx_expires ON job_cache(expires_at)")
        conn.commit()
    except Exception as e:
        logger.error(f"Error initializing cache table: {e}")
    finally:
        conn.close()

# Initialize on module load
_init_cache_table()

def get_cached_job(job_id: int) -> Optional[Dict[str, Any]]:
    """
    Get cached job data if available and not expired

    Args:
        job_id: Job ID to retrieve

    Returns:
        Dict with job data if cache hit, None if miss or expired
    """
    cache_key = f"job:{job_id}"
    conn = _get_connection()

    try:
        cursor = conn.execute(
            "SELECT data FROM job_cache WHERE cache_key = ? AND expires_at > ?",
            (cache_key, time.time())
        )
        row = cursor.fetchone()

        if row:
            logger.debug(f"Cache HIT for job {job_id}")
            return json.loads(row["data"])
        else:
            logger.debug(f"Cache MISS for job {job_id}")
            return None
    except Exception as e:
        logger.error(f"Error reading from cache: {e}")
        return None
    finally:
        conn.close()

def set_cached_job(job_id: int, data: Dict[str, Any], ttl: int = CACHE_TTL):
    """
    Store job data in cache with TTL

    Args:
        job_id: Job ID
        data: Job data dictionary to cache
        ttl: Time to live in seconds (default 30)
    """
    cache_key = f"job:{job_id}"
    expires_at = time.time() + ttl

    conn = _get_connection()
    try:
        conn.execute(
            "INSERT OR REPLACE INTO job_cache (cache_key, data, expires_at) VALUES (?, ?, ?)",
            (cache_key, json.dumps(data), expires_at)
        )
        conn.commit()
        logger.debug(f"Cached job {job_id} with TTL {ttl}s")
    except Exception as e:
        logger.error(f"Error writing to cache: {e}")
    finally:
        conn.close()

def invalidate_job_cache(job_id: int):
    """
    Invalidate cache for specific job

    Args:
        job_id: Job ID to invalidate
    """
    cache_key = f"job:{job_id}"
    conn = _get_connection()

    try:
        conn.execute("DELETE FROM job_cache WHERE cache_key = ?", (cache_key,))
        conn.commit()
        logger.debug(f"Invalidated cache for job {job_id}")
    except Exception as e:
        logger.error(f"Error invalidating cache: {e}")
    finally:
        conn.close()

def invalidate_user_jobs_cache(client_id: str):
    """
    Invalidate all cached jobs for a user

    Args:
        client_id: Client/user ID
    """
    # For simplicity, just clear all job caches
    # In production, you'd track job->user mapping
    conn = _get_connection()

    try:
        conn.execute("DELETE FROM job_cache WHERE cache_key LIKE 'job:%'")
        conn.commit()
        logger.debug(f"Invalidated all job caches for user {client_id}")
    except Exception as e:
        logger.error(f"Error invalidating user caches: {e}")
    finally:
        conn.close()

def cleanup_expired():
    """Remove expired cache entries"""
    conn = _get_connection()

    try:
        cursor = conn.execute("DELETE FROM job_cache WHERE expires_at <= ?", (time.time(),))
        deleted = cursor.rowcount
        conn.commit()
        if deleted > 0:
            logger.debug(f"Cleaned up {deleted} expired cache entries")
    except Exception as e:
        logger.error(f"Error cleaning up cache: {e}")
    finally:
        conn.close()

def get_cache_stats() -> Dict[str, Any]:
    """
    Get cache statistics

    Returns:
        Dict with cache stats (total entries, expired, active)
    """
    conn = _get_connection()

    try:
        cursor = conn.execute("SELECT COUNT(*) as total FROM job_cache")
        total = cursor.fetchone()["total"]

        cursor = conn.execute(
            "SELECT COUNT(*) as active FROM job_cache WHERE expires_at > ?",
            (time.time(),)
        )
        active = cursor.fetchone()["active"]

        expired = total - active

        return {
            "total_entries": total,
            "active_entries": active,
            "expired_entries": expired,
            "cache_type": "sqlite",
            "ttl_seconds": CACHE_TTL
        }
    except Exception as e:
        logger.error(f"Error getting cache stats: {e}")
        return {"error": str(e)}
    finally:
        conn.close()

# Wrapper functions for compatibility with main.py

def get_job_with_cache(job_id: int):
    """
    Get job with cache - compatible with main.py
    Returns cached data or fetches from DB if cache miss
    """
    # Try cache first
    cached = get_cached_job(job_id)
    if cached:
        return cached

    # Cache miss - fetch from database
    try:
        from backend.database import get_job as db_get_job
        job = db_get_job(job_id)

        if job:
            # Store in cache for next time
            set_cached_job(job_id, job)

        return job
    except Exception as e:
        logger.error(f"Error fetching job from database: {e}")
        return None

def update_job_progress_with_cache(job_id: int, progress: dict):
    """
    Update job progress and invalidate cache
    Caller should update database first, then call this
    """
    invalidate_job_cache(job_id)
</file>

<file path="backend/cache/test_redis_cache.py">
"""
Unit tests for Redis caching layer.

Run with: pytest backend/cache/test_redis_cache.py -v
"""

import pytest
import json
from datetime import datetime
from unittest.mock import Mock, patch, MagicMock

from .redis_cache import (
    get_job_with_cache,
    update_job_progress_with_cache,
    invalidate_job_cache,
    invalidate_user_jobs_cache,
    get_cache_stats,
    reset_cache_stats,
    redis_available,
    _serialize_job_response,
    _deserialize_job_response
)


@pytest.fixture
def sample_job():
    """Sample job data for testing."""
    return {
        "id": 123,
        "prompt": "Test video",
        "status": "pending",
        "created_at": "2024-01-01T00:00:00",
        "updated_at": "2024-01-01T00:00:00",
        "progress": {
            "current_stage": "pending",
            "scenes_total": 5,
            "scenes_completed": 0
        },
        "estimated_cost": 1.50,
        "video_url": None
    }


@pytest.fixture
def sample_job_with_datetime():
    """Sample job with datetime objects."""
    return {
        "id": 456,
        "prompt": "Test video 2",
        "status": "completed",
        "created_at": datetime(2024, 1, 1, 12, 0, 0),
        "updated_at": datetime(2024, 1, 1, 13, 0, 0),
        "approved_at": datetime(2024, 1, 1, 12, 30, 0),
        "progress": {},
        "estimated_cost": 2.00,
        "actual_cost": 1.95
    }


class TestSerialization:
    """Test JSON serialization/deserialization."""

    def test_serialize_job_basic(self, sample_job):
        """Test serialization of basic job data."""
        result = _serialize_job_response(sample_job)
        assert isinstance(result, str)

        # Verify it's valid JSON
        parsed = json.loads(result)
        assert parsed["id"] == 123
        assert parsed["prompt"] == "Test video"

    def test_serialize_datetime_objects(self, sample_job_with_datetime):
        """Test serialization converts datetime to ISO format."""
        result = _serialize_job_response(sample_job_with_datetime)
        parsed = json.loads(result)

        # Check datetime conversion
        assert parsed["created_at"] == "2024-01-01T12:00:00"
        assert parsed["updated_at"] == "2024-01-01T13:00:00"
        assert parsed["approved_at"] == "2024-01-01T12:30:00"

    def test_deserialize_job(self, sample_job):
        """Test deserialization of job data."""
        serialized = _serialize_job_response(sample_job)
        result = _deserialize_job_response(serialized)

        assert result["id"] == sample_job["id"]
        assert result["prompt"] == sample_job["prompt"]
        assert result["progress"] == sample_job["progress"]

    def test_roundtrip_serialization(self, sample_job):
        """Test serialize -> deserialize roundtrip."""
        serialized = _serialize_job_response(sample_job)
        deserialized = _deserialize_job_response(serialized)

        # Most fields should match (datetime strings may differ)
        assert deserialized["id"] == sample_job["id"]
        assert deserialized["status"] == sample_job["status"]
        assert deserialized["progress"] == sample_job["progress"]


class TestCacheFallback:
    """Test graceful fallback when Redis is unavailable."""

    @patch('backend.cache.redis_cache._get_redis_client')
    @patch('backend.cache.redis_cache.get_job')
    def test_get_job_redis_unavailable(self, mock_get_job, mock_redis_client, sample_job):
        """Test fallback to database when Redis is unavailable."""
        # Redis returns None (unavailable)
        mock_redis_client.return_value = None
        mock_get_job.return_value = sample_job

        result = get_job_with_cache(123)

        assert result == sample_job
        mock_get_job.assert_called_once_with(123)

    @patch('backend.cache.redis_cache._get_redis_client')
    @patch('backend.cache.redis_cache.update_job_progress')
    def test_update_job_redis_unavailable(self, mock_update, mock_redis_client):
        """Test update works when Redis is unavailable."""
        mock_redis_client.return_value = None
        mock_update.return_value = True

        progress = {"current_stage": "rendering"}
        result = update_job_progress_with_cache(123, progress)

        assert result is True
        mock_update.assert_called_once_with(123, progress)


class TestCacheHitMiss:
    """Test cache hit and miss scenarios."""

    @patch('backend.cache.redis_cache._get_redis_client')
    @patch('backend.cache.redis_cache.get_job')
    def test_cache_hit(self, mock_get_job, mock_redis_client, sample_job):
        """Test successful cache hit."""
        # Setup mock Redis
        mock_client = MagicMock()
        cached_data = _serialize_job_response(sample_job)
        mock_client.get.return_value = cached_data
        mock_redis_client.return_value = mock_client

        reset_cache_stats()
        result = get_job_with_cache(123)

        # Should return cached data without calling database
        assert result["id"] == sample_job["id"]
        mock_client.get.assert_called_once()
        mock_get_job.assert_not_called()

        # Verify stats
        stats = get_cache_stats()
        assert stats["hits"] == 1
        assert stats["misses"] == 0

    @patch('backend.cache.redis_cache._get_redis_client')
    @patch('backend.cache.redis_cache.get_job')
    def test_cache_miss(self, mock_get_job, mock_redis_client, sample_job):
        """Test cache miss and database fallback."""
        # Setup mock Redis with cache miss
        mock_client = MagicMock()
        mock_client.get.return_value = None  # Cache miss
        mock_redis_client.return_value = mock_client
        mock_get_job.return_value = sample_job

        reset_cache_stats()
        result = get_job_with_cache(123)

        # Should fetch from database and cache it
        assert result["id"] == sample_job["id"]
        mock_client.get.assert_called_once()
        mock_get_job.assert_called_once_with(123)
        mock_client.setex.assert_called_once()  # Should cache the result

        # Verify stats
        stats = get_cache_stats()
        assert stats["hits"] == 0
        assert stats["misses"] == 1


class TestCacheInvalidation:
    """Test cache invalidation."""

    @patch('backend.cache.redis_cache._get_redis_client')
    def test_invalidate_job_cache(self, mock_redis_client):
        """Test invalidating a specific job's cache."""
        mock_client = MagicMock()
        mock_client.delete.return_value = 1
        mock_redis_client.return_value = mock_client

        reset_cache_stats()
        invalidate_job_cache(123)

        # Should delete the cache key
        mock_client.delete.assert_called_once_with("job:123:progress")

        stats = get_cache_stats()
        assert stats["invalidations"] == 1

    @patch('backend.cache.redis_cache._get_redis_client')
    def test_invalidate_user_jobs_cache(self, mock_redis_client):
        """Test invalidating user's job list cache."""
        mock_client = MagicMock()
        mock_client.delete.return_value = 1
        mock_redis_client.return_value = mock_client

        invalidate_user_jobs_cache("test_user")

        # Should delete user's jobs cache
        mock_client.delete.assert_called_once_with("jobs:test_user")

    @patch('backend.cache.redis_cache._get_redis_client')
    @patch('backend.cache.redis_cache.update_job_progress')
    @patch('backend.cache.redis_cache.get_job')
    def test_update_invalidates_cache(self, mock_get_job, mock_update, mock_redis_client, sample_job):
        """Test that update invalidates and pre-warms cache."""
        mock_client = MagicMock()
        mock_redis_client.return_value = mock_client
        mock_update.return_value = True
        mock_get_job.return_value = sample_job

        progress = {"current_stage": "rendering"}
        result = update_job_progress_with_cache(123, progress)

        assert result is True
        # Should update database
        mock_update.assert_called_once_with(123, progress)
        # Should delete old cache
        mock_client.delete.assert_called()
        # Should pre-warm with new data
        mock_client.setex.assert_called()


class TestCacheStats:
    """Test cache statistics."""

    def test_initial_stats(self):
        """Test initial cache statistics."""
        reset_cache_stats()
        stats = get_cache_stats()

        assert stats["hits"] == 0
        assert stats["misses"] == 0
        assert stats["errors"] == 0
        assert stats["invalidations"] == 0
        assert stats["hit_rate"] == 0.0
        assert "redis_enabled" in stats
        assert "redis_available" in stats

    def test_hit_rate_calculation(self):
        """Test hit rate percentage calculation."""
        reset_cache_stats()

        # Simulate some cache activity
        with patch('backend.cache.redis_cache._cache_stats', {"hits": 7, "misses": 3, "errors": 0, "invalidations": 0}):
            stats = get_cache_stats()
            assert stats["hit_rate"] == 70.0  # 7/10 = 70%

    def test_reset_stats(self):
        """Test resetting cache statistics."""
        # Modify stats
        with patch('backend.cache.redis_cache._cache_stats', {"hits": 10, "misses": 5, "errors": 1, "invalidations": 2}):
            stats = get_cache_stats()
            assert stats["hits"] == 10

        # Reset
        reset_cache_stats()
        stats = get_cache_stats()
        assert stats["hits"] == 0
        assert stats["misses"] == 0


class TestErrorHandling:
    """Test error handling and resilience."""

    @patch('backend.cache.redis_cache._get_redis_client')
    @patch('backend.cache.redis_cache.get_job')
    def test_redis_error_fallback(self, mock_get_job, mock_redis_client, sample_job):
        """Test fallback to database on Redis error."""
        # Setup Redis to raise exception
        mock_client = MagicMock()
        mock_client.get.side_effect = Exception("Redis connection error")
        mock_redis_client.return_value = mock_client
        mock_get_job.return_value = sample_job

        reset_cache_stats()
        result = get_job_with_cache(123)

        # Should still return data from database
        assert result == sample_job
        mock_get_job.assert_called_once_with(123)

        # Should track error
        stats = get_cache_stats()
        assert stats["errors"] >= 1

    @patch('backend.cache.redis_cache._get_redis_client')
    def test_invalidate_nonexistent_key(self, mock_redis_client):
        """Test invalidating a non-existent cache key."""
        mock_client = MagicMock()
        mock_client.delete.return_value = 0  # Key didn't exist
        mock_redis_client.return_value = mock_client

        # Should not raise exception
        invalidate_job_cache(999)
        mock_client.delete.assert_called_once()


# Integration test (requires actual Redis)
@pytest.mark.integration
@pytest.mark.skipif(not redis_available(), reason="Redis not available")
class TestRealRedis:
    """Integration tests with real Redis (optional)."""

    def test_real_redis_connection(self):
        """Test actual Redis connection."""
        assert redis_available() is True

    def test_real_cache_operations(self, sample_job):
        """Test real cache set/get operations."""
        # This test requires actual Redis running
        # Reset stats
        reset_cache_stats()

        # First call should be a miss
        with patch('backend.cache.redis_cache.get_job', return_value=sample_job):
            result1 = get_job_with_cache(999)
            assert result1["id"] == sample_job["id"]

        stats = get_cache_stats()
        assert stats["misses"] == 1

        # Second call should be a hit (if Redis is working)
        result2 = get_job_with_cache(999)
        if stats["redis_enabled"]:
            stats = get_cache_stats()
            assert stats["hits"] == 1


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
</file>

<file path="backend/cache/USAGE_EXAMPLE.py">
"""
Usage examples for Redis caching layer.

This file demonstrates how to use the caching system in different scenarios.
"""

from backend.cache import (
    get_job_with_cache,
    update_job_progress_with_cache,
    invalidate_job_cache,
    invalidate_user_jobs_cache,
    get_cache_stats,
    redis_available
)


def example_1_basic_usage():
    """Example 1: Basic job retrieval with caching."""
    print("Example 1: Basic Job Retrieval")
    print("-" * 50)

    job_id = 123

    # First request - cache miss, fetches from DB
    print(f"Fetching job {job_id}...")
    job = get_job_with_cache(job_id)

    if job:
        print(f"  Job ID: {job['id']}")
        print(f"  Status: {job['status']}")
        print(f"  Progress: {job.get('progress', {})}")
    else:
        print(f"  Job {job_id} not found")

    # Second request within 30s - cache hit
    print(f"\nFetching job {job_id} again (should be cached)...")
    job = get_job_with_cache(job_id)
    print(f"  Retrieved from cache")

    # Check stats
    stats = get_cache_stats()
    print(f"\nCache Stats:")
    print(f"  Hit rate: {stats['hit_rate']}%")
    print(f"  Hits: {stats['hits']}, Misses: {stats['misses']}")


def example_2_progress_updates():
    """Example 2: Updating job progress with cache invalidation."""
    print("\nExample 2: Progress Updates")
    print("-" * 50)

    job_id = 123

    # Update progress
    progress_data = {
        "current_stage": "rendering",
        "scenes_total": 5,
        "scenes_completed": 3,
        "current_scene": 4,
        "estimated_completion_seconds": 45,
        "message": "Rendering scene 4 of 5..."
    }

    print(f"Updating progress for job {job_id}...")
    success = update_job_progress_with_cache(job_id, progress_data)

    if success:
        print(f"  ✓ Progress updated successfully")
        print(f"  ✓ Cache invalidated")
        print(f"  ✓ Cache pre-warmed with new data")

        # Fetch updated job (will use pre-warmed cache)
        job = get_job_with_cache(job_id)
        print(f"\nUpdated job status:")
        print(f"  Current stage: {job['progress']['current_stage']}")
        print(f"  Scenes completed: {job['progress']['scenes_completed']}/{job['progress']['scenes_total']}")
    else:
        print(f"  ✗ Failed to update progress")


def example_3_cache_invalidation():
    """Example 3: Manual cache invalidation."""
    print("\nExample 3: Cache Invalidation")
    print("-" * 50)

    job_id = 123
    client_id = "user@example.com"

    # Invalidate specific job
    print(f"Invalidating cache for job {job_id}...")
    invalidate_job_cache(job_id)
    print(f"  ✓ Job cache invalidated")

    # Invalidate user's job list (future enhancement)
    print(f"\nInvalidating all jobs for client {client_id}...")
    invalidate_user_jobs_cache(client_id)
    print(f"  ✓ User jobs cache invalidated")


def example_4_monitoring():
    """Example 4: Monitoring cache performance."""
    print("\nExample 4: Cache Monitoring")
    print("-" * 50)

    # Check if Redis is available
    if redis_available():
        print("✓ Redis is available and operational")
    else:
        print("✗ Redis is unavailable (using database fallback)")

    # Get detailed statistics
    stats = get_cache_stats()

    print(f"\nCache Statistics:")
    print(f"  Enabled: {stats['redis_enabled']}")
    print(f"  Available: {stats['redis_available']}")
    print(f"  TTL: {stats['ttl_seconds']} seconds")
    print(f"\nPerformance:")
    print(f"  Total requests: {stats['total_requests']}")
    print(f"  Cache hits: {stats['hits']}")
    print(f"  Cache misses: {stats['misses']}")
    print(f"  Hit rate: {stats['hit_rate']}%")
    print(f"  Errors: {stats['errors']}")
    print(f"  Invalidations: {stats['invalidations']}")


def example_5_error_handling():
    """Example 5: Graceful error handling."""
    print("\nExample 5: Error Handling")
    print("-" * 50)

    # Even if Redis is down, the system continues working
    job_id = 123

    try:
        print(f"Fetching job {job_id}...")
        job = get_job_with_cache(job_id)

        if job:
            print(f"  ✓ Job retrieved successfully")
            print(f"  Status: {job['status']}")

            # If Redis is down, this will log a warning but still work
            if redis_available():
                print(f"  Source: Redis cache")
            else:
                print(f"  Source: Database (Redis unavailable)")
        else:
            print(f"  Job not found")

    except Exception as e:
        print(f"  ✗ Error: {e}")


def example_6_typical_workflow():
    """Example 6: Typical video generation workflow with caching."""
    print("\nExample 6: Typical Workflow")
    print("-" * 50)

    job_id = 456

    # Step 1: Job created (not cached yet)
    print("Step 1: Job created")
    print(f"  Job ID: {job_id}")

    # Step 2: Client starts polling
    print("\nStep 2: Client polling (every 3 seconds)")

    for poll_count in range(1, 6):
        print(f"\n  Poll #{poll_count}")
        job = get_job_with_cache(job_id)

        if job:
            stats = get_cache_stats()
            source = "cache" if stats['hits'] > 0 else "database"
            print(f"    Status: {job['status']}")
            print(f"    Source: {source}")
            print(f"    Hit rate: {stats['hit_rate']}%")

        # Simulate progress update after 2nd poll
        if poll_count == 2:
            print(f"\n  → Progress update triggered")
            update_job_progress_with_cache(job_id, {
                "current_stage": "generating_storyboard",
                "scenes_completed": 2,
                "scenes_total": 5
            })
            print(f"    Cache invalidated and pre-warmed")

    # Step 3: Final statistics
    print("\nStep 3: Final Statistics")
    stats = get_cache_stats()
    print(f"  Total polls: 5")
    print(f"  Database queries: {stats['misses']} (cache misses) + 1 (update)")
    print(f"  Cache hits: {stats['hits']}")
    print(f"  Database load reduction: {(stats['hits'] / stats['total_requests'] * 100):.0f}%")


def example_7_concurrent_users():
    """Example 7: Multiple users polling the same job."""
    print("\nExample 7: Concurrent Users")
    print("-" * 50)

    job_id = 789

    print("Simulating 10 concurrent users polling the same job...")
    print("(All users get cached data after first request)\n")

    for user in range(1, 11):
        job = get_job_with_cache(job_id)
        stats = get_cache_stats()

        source = "database" if user == 1 else "cache"
        print(f"  User {user:2d}: Retrieved job {job_id} from {source}")

    stats = get_cache_stats()
    print(f"\nResult:")
    print(f"  Database queries: 1 (first user)")
    print(f"  Cache hits: {stats['hits']}")
    print(f"  Database load: {(1 / stats['total_requests'] * 100):.0f}% of what it would be without cache")


# Example usage
if __name__ == "__main__":
    print("=" * 50)
    print("Redis Caching Layer - Usage Examples")
    print("=" * 50)

    # Run examples
    example_1_basic_usage()
    example_2_progress_updates()
    example_3_cache_invalidation()
    example_4_monitoring()
    example_5_error_handling()
    example_6_typical_workflow()
    example_7_concurrent_users()

    print("\n" + "=" * 50)
    print("Examples completed!")
    print("=" * 50)


# Integration with FastAPI endpoints
def fastapi_endpoint_examples():
    """
    Examples of how the caching is used in FastAPI endpoints.
    """

    # Example 1: Job status endpoint
    from fastapi import HTTPException

    async def get_job_status(job_id: int):
        """Get job status with caching."""
        # Use cache-aware function
        job = get_job_with_cache(job_id)

        if not job:
            raise HTTPException(status_code=404, detail=f"Job {job_id} not found")

        return {
            "job_id": job["id"],
            "status": job["status"],
            "progress": job["progress"]
        }

    # Example 2: Approve storyboard endpoint
    async def approve_storyboard(job_id: int):
        """Approve storyboard and invalidate cache."""
        # Get job from cache
        job = get_job_with_cache(job_id)

        if not job:
            raise HTTPException(status_code=404, detail="Job not found")

        # Perform approval (database operation)
        from backend.database import approve_storyboard as db_approve
        success = db_approve(job_id)

        if success:
            # Invalidate cache so next request gets fresh data
            invalidate_job_cache(job_id)

        return {"success": success}

    # Example 3: Update progress endpoint
    async def update_progress(job_id: int, progress: dict):
        """Update progress with automatic cache management."""
        # Uses cache-aware function that handles invalidation
        success = update_job_progress_with_cache(job_id, progress)

        if not success:
            raise HTTPException(status_code=500, detail="Failed to update progress")

        return {"success": True}

    # Example 4: Cache stats monitoring endpoint
    async def cache_statistics():
        """Get cache performance metrics."""
        stats = get_cache_stats()

        return {
            "cache_enabled": redis_available(),
            "statistics": stats,
            "message": "Cache is working normally" if stats["redis_enabled"]
                      else "Cache is disabled or unavailable"
        }
</file>

<file path="backend/migrations/__init__.py">
"""Database migrations for the video generation backend."""
</file>

<file path="backend/migrations/add_asset_blob_storage.py">
"""
Migration: Add blob_data column to assets table
Allows storing assets directly in the database as binary blobs
"""

import sqlite3
from pathlib import Path


def up(conn: sqlite3.Connection):
    """Add blob_data column to assets table"""
    cursor = conn.cursor()

    try:
        # Add blob_data column for storing asset binary data
        cursor.execute("""
            ALTER TABLE assets
            ADD COLUMN blob_data BLOB
        """)

        conn.commit()
        print("✓ Added blob_data column to assets table")

    except sqlite3.OperationalError as e:
        if "duplicate column name" in str(e).lower():
            print("⚠ blob_data column already exists, skipping")
        else:
            raise

    cursor.close()


def down(conn: sqlite3.Connection):
    """
    Remove blob_data column from assets table
    Note: SQLite doesn't support DROP COLUMN directly, would need table recreation
    """
    cursor = conn.cursor()

    # SQLite limitation: Can't drop columns easily
    # Would need to:
    # 1. Create new table without blob_data
    # 2. Copy data
    # 3. Drop old table
    # 4. Rename new table

    print("⚠ Downgrade not implemented for SQLite (DROP COLUMN not supported)")
    print("  To remove blob_data column, manually recreate the table")

    cursor.close()


def run_migration(db_path: str = "backend/sim_poc.db"):
    """Run the migration"""
    conn = sqlite3.connect(db_path)
    try:
        up(conn)
    finally:
        conn.close()


if __name__ == "__main__":
    # Run migration if executed directly
    run_migration()
</file>

<file path="backend/migrations/add_clients_campaigns.py">
"""Migration: Add Clients and Campaigns tables for ad-video-gen frontend integration.

This migration adds:
1. clients table - Brand/client management with brand guidelines
2. client_assets table - Client-specific assets (logos, brand docs)
3. campaigns table - Marketing campaigns linked to clients
4. campaign_assets table - Campaign-specific assets
5. Foreign key constraints linking campaigns to clients and videos to campaigns
"""

import sqlite3
import json
from pathlib import Path
import os
from datetime import datetime

# Get data directory from environment variable, default to ./DATA
DATA_DIR = Path(os.getenv("DATA", "./DATA"))
DATA_DIR.mkdir(exist_ok=True)
DB_PATH = DATA_DIR / "scenes.db"


def run_migration():
    """Run the migration to add clients and campaigns tables."""
    conn = sqlite3.connect(str(DB_PATH))
    conn.row_factory = sqlite3.Row

    try:
        print("Starting migration: add_clients_campaigns")

        # 1. Create clients table
        print("Creating clients table...")
        conn.execute("""
            CREATE TABLE IF NOT EXISTS clients (
                id TEXT PRIMARY KEY,
                user_id INTEGER NOT NULL,
                name TEXT NOT NULL,
                description TEXT,
                brand_guidelines TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (user_id) REFERENCES users(id) ON DELETE CASCADE
            )
        """)

        # 2. Create client_assets table
        print("Creating client_assets table...")
        conn.execute("""
            CREATE TABLE IF NOT EXISTS client_assets (
                id TEXT PRIMARY KEY,
                client_id TEXT NOT NULL,
                type TEXT NOT NULL CHECK (type IN ('logo', 'image', 'document')),
                url TEXT NOT NULL,
                name TEXT NOT NULL,
                uploaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (client_id) REFERENCES clients(id) ON DELETE CASCADE
            )
        """)

        # 3. Create campaigns table
        print("Creating campaigns table...")
        conn.execute("""
            CREATE TABLE IF NOT EXISTS campaigns (
                id TEXT PRIMARY KEY,
                client_id TEXT NOT NULL,
                user_id INTEGER NOT NULL,
                name TEXT NOT NULL,
                goal TEXT NOT NULL,
                status TEXT NOT NULL CHECK (status IN ('active', 'archived', 'draft')) DEFAULT 'draft',
                brief TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (client_id) REFERENCES clients(id) ON DELETE CASCADE,
                FOREIGN KEY (user_id) REFERENCES users(id) ON DELETE CASCADE
            )
        """)

        # 4. Create campaign_assets table
        print("Creating campaign_assets table...")
        conn.execute("""
            CREATE TABLE IF NOT EXISTS campaign_assets (
                id TEXT PRIMARY KEY,
                campaign_id TEXT NOT NULL,
                type TEXT NOT NULL CHECK (type IN ('image', 'video', 'document')),
                url TEXT NOT NULL,
                name TEXT NOT NULL,
                uploaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (campaign_id) REFERENCES campaigns(id) ON DELETE CASCADE
            )
        """)

        # 5. Add campaign_id column to generated_videos if it doesn't exist
        print("Adding campaign_id to generated_videos table...")
        try:
            conn.execute("ALTER TABLE generated_videos ADD COLUMN campaign_id TEXT REFERENCES campaigns(id)")
        except sqlite3.OperationalError as e:
            if "duplicate column name" in str(e).lower():
                print("  - campaign_id column already exists, skipping...")
            else:
                raise

        # 6. Add format and duration columns to generated_videos if they don't exist
        print("Adding format column to generated_videos table...")
        try:
            conn.execute("ALTER TABLE generated_videos ADD COLUMN format TEXT CHECK (format IN ('9:16', '1:1', '16:9')) DEFAULT '16:9'")
        except sqlite3.OperationalError as e:
            if "duplicate column name" in str(e).lower():
                print("  - format column already exists, skipping...")
            else:
                raise

        print("Adding duration column to generated_videos table...")
        try:
            conn.execute("ALTER TABLE generated_videos ADD COLUMN duration INTEGER CHECK (duration IN (15, 30, 60)) DEFAULT 30")
        except sqlite3.OperationalError as e:
            if "duplicate column name" in str(e).lower():
                print("  - duration column already exists, skipping...")
            else:
                raise

        # 7. Add video metrics columns
        print("Adding metrics columns to generated_videos table...")
        metrics_columns = [
            ("views", "INTEGER DEFAULT 0"),
            ("clicks", "INTEGER DEFAULT 0"),
            ("ctr", "REAL DEFAULT 0.0"),
            ("conversions", "INTEGER DEFAULT 0")
        ]

        for col_name, col_def in metrics_columns:
            try:
                conn.execute(f"ALTER TABLE generated_videos ADD COLUMN {col_name} {col_def}")
            except sqlite3.OperationalError as e:
                if "duplicate column name" in str(e).lower():
                    print(f"  - {col_name} column already exists, skipping...")
                else:
                    raise

        # 8. Create indexes for performance
        print("Creating indexes...")
        indexes = [
            ("idx_clients_user_id", "clients", "user_id"),
            ("idx_clients_name", "clients", "name"),
            ("idx_client_assets_client_id", "client_assets", "client_id"),
            ("idx_campaigns_client_id", "campaigns", "client_id"),
            ("idx_campaigns_user_id", "campaigns", "user_id"),
            ("idx_campaigns_status", "campaigns", "status"),
            ("idx_campaign_assets_campaign_id", "campaign_assets", "campaign_id"),
            ("idx_videos_campaign_id", "generated_videos", "campaign_id"),
        ]

        for idx_name, table_name, column_name in indexes:
            try:
                conn.execute(f"CREATE INDEX IF NOT EXISTS {idx_name} ON {table_name}({column_name})")
            except Exception as e:
                print(f"  - Warning: Could not create index {idx_name}: {e}")

        # 9. Create triggers for updated_at timestamps
        print("Creating update triggers...")

        # Clients update trigger
        conn.execute("""
            CREATE TRIGGER IF NOT EXISTS update_clients_timestamp
            AFTER UPDATE ON clients
            FOR EACH ROW
            BEGIN
                UPDATE clients SET updated_at = CURRENT_TIMESTAMP WHERE id = NEW.id;
            END
        """)

        # Campaigns update trigger
        conn.execute("""
            CREATE TRIGGER IF NOT EXISTS update_campaigns_timestamp
            AFTER UPDATE ON campaigns
            FOR EACH ROW
            BEGIN
                UPDATE campaigns SET updated_at = CURRENT_TIMESTAMP WHERE id = NEW.id;
            END
        """)

        conn.commit()
        print("Migration completed successfully!")

        # Verify tables were created
        print("\nVerifying tables...")
        cursor = conn.execute("SELECT name FROM sqlite_master WHERE type='table' ORDER BY name")
        tables = [row[0] for row in cursor.fetchall()]
        print(f"Total tables: {len(tables)}")

        required_tables = ['clients', 'client_assets', 'campaigns', 'campaign_assets']
        for table in required_tables:
            if table in tables:
                print(f"  ✓ {table}")
            else:
                print(f"  ✗ {table} - MISSING!")

    except Exception as e:
        print(f"Migration failed: {e}")
        conn.rollback()
        raise
    finally:
        conn.close()


if __name__ == "__main__":
    run_migration()
</file>

<file path="backend/migrations/add_video_job_fields.py">
"""
Database migration to add video generation v2 workflow fields.

This migration adds columns needed for the video generation workflow:
- progress: JSON field for tracking progress
- storyboard_data: JSON field for storing storyboard entries
- approved: Boolean flag for storyboard approval
- approved_at: Timestamp of approval
- estimated_cost: Float for cost estimation
- actual_cost: Float for actual cost
- error_message: Text field for error messages
- updated_at: Timestamp for last update
"""

import sqlite3
from pathlib import Path
import os

# Get database path from environment
DATA_DIR = Path(os.getenv("DATA", "./DATA"))
DB_PATH = DATA_DIR / "scenes.db"


def migrate():
    """Run the migration to add video job workflow fields."""
    conn = sqlite3.connect(str(DB_PATH))
    cursor = conn.cursor()

    try:
        # Add progress field (JSON)
        try:
            cursor.execute("ALTER TABLE generated_videos ADD COLUMN progress TEXT")
            print("✓ Added 'progress' column")
        except sqlite3.OperationalError:
            print("- 'progress' column already exists")

        # Add storyboard_data field (JSON)
        try:
            cursor.execute("ALTER TABLE generated_videos ADD COLUMN storyboard_data TEXT")
            print("✓ Added 'storyboard_data' column")
        except sqlite3.OperationalError:
            print("- 'storyboard_data' column already exists")

        # Add approved field (Boolean, default False)
        try:
            cursor.execute("ALTER TABLE generated_videos ADD COLUMN approved BOOLEAN DEFAULT 0")
            print("✓ Added 'approved' column")
        except sqlite3.OperationalError:
            print("- 'approved' column already exists")

        # Add approved_at field (Timestamp)
        try:
            cursor.execute("ALTER TABLE generated_videos ADD COLUMN approved_at TIMESTAMP")
            print("✓ Added 'approved_at' column")
        except sqlite3.OperationalError:
            print("- 'approved_at' column already exists")

        # Add estimated_cost field (Float, default 0.0)
        try:
            cursor.execute("ALTER TABLE generated_videos ADD COLUMN estimated_cost REAL DEFAULT 0.0")
            print("✓ Added 'estimated_cost' column")
        except sqlite3.OperationalError:
            print("- 'estimated_cost' column already exists")

        # Add actual_cost field (Float)
        try:
            cursor.execute("ALTER TABLE generated_videos ADD COLUMN actual_cost REAL")
            print("✓ Added 'actual_cost' column")
        except sqlite3.OperationalError:
            print("- 'actual_cost' column already exists")

        # Add error_message field (Text)
        try:
            cursor.execute("ALTER TABLE generated_videos ADD COLUMN error_message TEXT")
            print("✓ Added 'error_message' column")
        except sqlite3.OperationalError:
            print("- 'error_message' column already exists")

        # Add updated_at field (Timestamp, default CURRENT_TIMESTAMP)
        try:
            cursor.execute("ALTER TABLE generated_videos ADD COLUMN updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP")
            print("✓ Added 'updated_at' column")
        except sqlite3.OperationalError:
            print("- 'updated_at' column already exists")

        # Create trigger to auto-update updated_at timestamp
        try:
            cursor.execute("""
                CREATE TRIGGER IF NOT EXISTS update_generated_videos_timestamp
                AFTER UPDATE ON generated_videos
                FOR EACH ROW
                BEGIN
                    UPDATE generated_videos
                    SET updated_at = CURRENT_TIMESTAMP
                    WHERE id = NEW.id;
                END;
            """)
            print("✓ Created auto-update trigger for 'updated_at'")
        except sqlite3.OperationalError as e:
            print(f"- Trigger creation skipped: {e}")

        conn.commit()
        print("\n✅ Migration completed successfully!")

    except Exception as e:
        conn.rollback()
        print(f"\n❌ Migration failed: {e}")
        raise
    finally:
        conn.close()


if __name__ == "__main__":
    migrate()
</file>

<file path="backend/migrations/run_add_blob_data.py">
#!/usr/bin/env python3
"""
Safe migration to add blob_data column to assets table
Checks if column exists before attempting to add it
"""

import sqlite3
import sys
from pathlib import Path


def column_exists(cursor, table_name, column_name):
    """Check if a column exists in a table"""
    cursor.execute(f"PRAGMA table_info({table_name})")
    columns = [row[1] for row in cursor.fetchall()]
    return column_name in columns


def add_blob_data_column(db_path: str):
    """Add blob_data column to assets table if it doesn't exist"""

    print(f"Connecting to database: {db_path}")
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()

    try:
        # Check if column already exists
        if column_exists(cursor, 'assets', 'blob_data'):
            print("✓ blob_data column already exists in assets table")
            print("  No migration needed")
            return True

        # Add the column
        print("Adding blob_data column to assets table...")
        cursor.execute("ALTER TABLE assets ADD COLUMN blob_data BLOB")
        conn.commit()

        # Verify it was added
        if column_exists(cursor, 'assets', 'blob_data'):
            print("✓ Successfully added blob_data column to assets table")
            return True
        else:
            print("✗ Failed to add blob_data column")
            return False

    except sqlite3.OperationalError as e:
        print(f"✗ Migration failed: {e}")
        return False
    finally:
        cursor.close()
        conn.close()


def main():
    # Determine database path
    if len(sys.argv) > 1:
        db_path = sys.argv[1]
    else:
        # Default paths to check
        possible_paths = [
            "/data/scenes.db",  # Production path on Fly.io
            "backend/sim_poc.db",  # Local dev path
            "sim_poc.db",  # Alternative local path
        ]

        db_path = None
        for path in possible_paths:
            if Path(path).exists():
                db_path = path
                break

        if not db_path:
            print("Error: Could not find database file")
            print("Usage: python run_add_blob_data.py [path/to/database.db]")
            print("\nSearched paths:")
            for path in possible_paths:
                print(f"  - {path}")
            sys.exit(1)

    print(f"Database path: {db_path}\n")

    # Run migration
    success = add_blob_data_column(db_path)

    if success:
        print("\n✓ Migration completed successfully")
        sys.exit(0)
    else:
        print("\n✗ Migration failed")
        sys.exit(1)


if __name__ == "__main__":
    main()
</file>

<file path="backend/models/__init__.py">
"""
Backend models package.

This package contains Pydantic models for the video generation API.
"""

from backend.models.video_generation import (
    VideoStatus,
    Scene,
    StoryboardEntry,
    GenerationRequest,
    VideoProgress,
    JobResponse,
)

__all__ = [
    "VideoStatus",
    "Scene",
    "StoryboardEntry",
    "GenerationRequest",
    "VideoProgress",
    "JobResponse",
]
</file>

<file path="backend/models/usage_example.py">
#!/usr/bin/env python3
"""
Usage examples for video generation Pydantic models.

This demonstrates how the models will be used in the actual API implementation.
"""

from datetime import datetime
from backend.models.video_generation import (
    VideoStatus,
    Scene,
    StoryboardEntry,
    GenerationRequest,
    VideoProgress,
    JobResponse,
)


def example_1_create_generation_request():
    """Example 1: Creating a video generation request."""
    print("=" * 60)
    print("Example 1: Creating a Generation Request")
    print("=" * 60)

    request = GenerationRequest(
        prompt="Create a promotional video showcasing our new AI-powered analytics platform. "
               "Show data visualizations, team collaboration, and business insights.",
        duration=45,
        style="cinematic",
        aspect_ratio="16:9",
        client_id="acme-analytics",
        brand_guidelines={
            "primary_color": "#0066CC",
            "secondary_color": "#FF6B35",
            "logo_url": "https://example.com/logo.png",
            "tone": "professional yet approachable"
        }
    )

    print(f"Prompt: {request.prompt[:80]}...")
    print(f"Duration: {request.duration}s")
    print(f"Style: {request.style}")
    print(f"Aspect Ratio: {request.aspect_ratio}")
    print(f"Client: {request.client_id}")
    print(f"Brand Guidelines: {request.brand_guidelines}")
    print()


def example_2_build_storyboard():
    """Example 2: Building a storyboard with scenes."""
    print("=" * 60)
    print("Example 2: Building a Storyboard")
    print("=" * 60)

    scenes_data = [
        {
            "scene_number": 1,
            "description": "Opening shot showing modern office with data dashboards",
            "duration": 5.0,
            "image_prompt": "Modern tech office, large screens displaying colorful data analytics dashboards, professional lighting, wide angle shot"
        },
        {
            "scene_number": 2,
            "description": "Close-up of AI processing data in real-time",
            "duration": 4.5,
            "image_prompt": "Futuristic AI visualization, flowing data streams, neural network graphics, blue and purple color scheme"
        },
        {
            "scene_number": 3,
            "description": "Team collaborating around interactive display",
            "duration": 6.0,
            "image_prompt": "Diverse business team gathered around large touchscreen display, collaborative atmosphere, modern office setting"
        }
    ]

    storyboard = []
    for scene_data in scenes_data:
        scene = Scene(**scene_data)
        entry = StoryboardEntry(
            scene=scene,
            generation_status="pending"
        )
        storyboard.append(entry)

    print(f"Created storyboard with {len(storyboard)} scenes:")
    for entry in storyboard:
        print(f"  Scene {entry.scene.scene_number}: {entry.scene.description[:60]}...")
        print(f"    Duration: {entry.scene.duration}s | Status: {entry.generation_status}")
    print()


def example_3_track_progress():
    """Example 3: Tracking video generation progress."""
    print("=" * 60)
    print("Example 3: Progress Tracking")
    print("=" * 60)

    # Initial state
    progress1 = VideoProgress(
        current_stage=VideoStatus.PARSING,
        scenes_total=0,
        scenes_completed=0,
        message="Analyzing prompt and extracting requirements..."
    )
    print(f"Stage 1: {progress1.current_stage}")
    print(f"  {progress1.message}")
    print()

    # Storyboard generation
    progress2 = VideoProgress(
        current_stage=VideoStatus.GENERATING_STORYBOARD,
        scenes_total=6,
        scenes_completed=3,
        current_scene=4,
        estimated_completion_seconds=45,
        message="Generating scene 4 of 6"
    )
    print(f"Stage 2: {progress2.current_stage}")
    print(f"  Progress: {progress2.scenes_completed}/{progress2.scenes_total} scenes")
    print(f"  Current: Scene {progress2.current_scene}")
    print(f"  ETA: {progress2.estimated_completion_seconds}s")
    print(f"  {progress2.message}")
    print()

    # Rendering
    progress3 = VideoProgress(
        current_stage=VideoStatus.RENDERING,
        scenes_total=6,
        scenes_completed=6,
        estimated_completion_seconds=120,
        message="Rendering final video..."
    )
    print(f"Stage 3: {progress3.current_stage}")
    print(f"  All {progress3.scenes_completed} scenes completed")
    print(f"  {progress3.message}")
    print(f"  ETA: {progress3.estimated_completion_seconds}s")
    print()


def example_4_complete_job_response():
    """Example 4: Complete job response."""
    print("=" * 60)
    print("Example 4: Complete Job Response")
    print("=" * 60)

    # Create scenes
    scene1 = Scene(
        scene_number=1,
        description="Opening corporate shot",
        duration=5.0,
        image_prompt="Professional corporate office environment, cinematic lighting"
    )

    scene2 = Scene(
        scene_number=2,
        description="Product showcase",
        duration=7.5,
        image_prompt="Modern AI analytics dashboard, sleek interface, data visualization"
    )

    # Create storyboard
    storyboard = [
        StoryboardEntry(
            scene=scene1,
            image_url="https://storage.example.com/jobs/12345/scene_1.jpg",
            generation_status="completed"
        ),
        StoryboardEntry(
            scene=scene2,
            image_url="https://storage.example.com/jobs/12345/scene_2.jpg",
            generation_status="completed"
        )
    ]

    # Create progress
    progress = VideoProgress(
        current_stage=VideoStatus.COMPLETED,
        scenes_total=2,
        scenes_completed=2,
        message="Video generation complete!"
    )

    # Create complete job response
    job = JobResponse(
        job_id=12345,
        status=VideoStatus.COMPLETED,
        progress=progress,
        storyboard=storyboard,
        video_url="https://storage.example.com/jobs/12345/final_video.mp4",
        estimated_cost=24.50,
        actual_cost=23.75,
        created_at=datetime(2025, 11, 15, 14, 30, 0),
        updated_at=datetime(2025, 11, 15, 14, 45, 0),
        approved=True
    )

    print(f"Job ID: {job.job_id}")
    print(f"Status: {job.status}")
    print(f"Created: {job.created_at.isoformat()}")
    print(f"Updated: {job.updated_at.isoformat()}")
    print(f"Approved: {job.approved}")
    print(f"Estimated Cost: ${job.estimated_cost:.2f}")
    print(f"Actual Cost: ${job.actual_cost:.2f}")
    print(f"Savings: ${job.estimated_cost - job.actual_cost:.2f}")
    print(f"\nStoryboard: {len(job.storyboard)} scenes")
    for entry in job.storyboard:
        print(f"  Scene {entry.scene.scene_number}: {entry.generation_status}")
        print(f"    Image: {entry.image_url}")
    print(f"\nVideo URL: {job.video_url}")
    print()


def example_5_json_serialization():
    """Example 5: JSON serialization and deserialization."""
    print("=" * 60)
    print("Example 5: JSON Serialization")
    print("=" * 60)

    request = GenerationRequest(
        prompt="Create a video about sustainable technology innovations",
        duration=30,
        style="documentary",
        aspect_ratio="16:9"
    )

    # Serialize to JSON
    json_str = request.model_dump_json(indent=2)
    print("Serialized to JSON:")
    print(json_str)
    print()

    # Deserialize from JSON
    restored = GenerationRequest.model_validate_json(json_str)
    print("Restored from JSON:")
    print(f"  Prompt: {restored.prompt[:60]}...")
    print(f"  Duration: {restored.duration}s")
    print(f"  Style: {restored.style}")
    print()


def example_6_validation_errors():
    """Example 6: Validation error handling."""
    print("=" * 60)
    print("Example 6: Validation Error Handling")
    print("=" * 60)

    from pydantic import ValidationError

    # Invalid prompt (too short)
    try:
        GenerationRequest(prompt="Short", duration=30)
    except ValidationError as e:
        print("Error 1: Prompt too short")
        print(f"  {e.errors()[0]['msg']}")
        print()

    # Invalid duration (too long)
    try:
        GenerationRequest(
            prompt="A valid prompt that is long enough",
            duration=500  # Exceeds 300s limit
        )
    except ValidationError as e:
        print("Error 2: Duration too long")
        print(f"  {e.errors()[0]['msg']}")
        print()

    # Invalid aspect ratio
    try:
        GenerationRequest(
            prompt="A valid prompt that is long enough",
            duration=30,
            aspect_ratio="21:9"  # Not in allowed list
        )
    except ValidationError as e:
        print("Error 3: Invalid aspect ratio")
        print(f"  {e.errors()[0]['msg']}")
        print()

    # Invalid scene number
    try:
        Scene(
            scene_number=0,  # Must be >= 1
            description="Test scene",
            duration=5.0,
            image_prompt="Test prompt"
        )
    except ValidationError as e:
        print("Error 4: Invalid scene number")
        print(f"  {e.errors()[0]['msg']}")
        print()


def main():
    """Run all examples."""
    print("\n")
    print("*" * 60)
    print("VIDEO GENERATION PYDANTIC MODELS - USAGE EXAMPLES")
    print("*" * 60)
    print("\n")

    example_1_create_generation_request()
    example_2_build_storyboard()
    example_3_track_progress()
    example_4_complete_job_response()
    example_5_json_serialization()
    example_6_validation_errors()

    print("=" * 60)
    print("All examples completed successfully!")
    print("=" * 60)
    print()


if __name__ == "__main__":
    main()
</file>

<file path="backend/models/video_generation.py">
"""
Video Generation API Models.

This module contains Pydantic models for the v2 video generation workflow,
including request/response models, status tracking, and progress monitoring.
"""

from __future__ import annotations

from datetime import datetime
from enum import Enum
from typing import Any, Optional

from pydantic import BaseModel, Field, field_validator


class VideoStatus(str, Enum):
    """
    Video generation workflow status.

    Tracks the current state of a video generation job through its lifecycle.
    """
    PENDING = "pending"
    PARSING = "parsing"
    GENERATING_STORYBOARD = "generating_storyboard"
    STORYBOARD_READY = "storyboard_ready"
    RENDERING = "rendering"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELED = "canceled"


class Scene(BaseModel):
    """
    Individual scene definition within a video.

    Represents a single scene with its narrative description and visual prompt
    for image generation.
    """
    scene_number: int = Field(..., ge=1, description="Sequential scene number starting from 1")
    description: str = Field(..., min_length=1, max_length=1000, description="Narrative description of the scene")
    duration: float = Field(..., gt=0, le=60, description="Scene duration in seconds")
    image_prompt: str = Field(..., min_length=1, max_length=2000, description="Detailed prompt for image generation")

    @field_validator('duration')
    @classmethod
    def validate_duration(cls, v: float) -> float:
        """Ensure duration is positive and reasonable."""
        if v <= 0:
            raise ValueError("Scene duration must be greater than 0 seconds")
        if v > 60:
            raise ValueError("Scene duration cannot exceed 60 seconds")
        return round(v, 2)  # Round to 2 decimal places


class StoryboardEntry(BaseModel):
    """
    Storyboard entry tracking scene generation progress.

    Links a scene definition with its generated image and tracks the
    generation status and any errors encountered.
    """
    scene: Scene = Field(..., description="Scene definition")
    image_url: Optional[str] = Field(None, description="URL to generated image (if completed)")
    generation_status: str = Field(
        default="pending",
        pattern="^(pending|generating|completed|failed)$",
        description="Current status of image generation"
    )
    error: Optional[str] = Field(None, max_length=500, description="Error message if generation failed")

    @field_validator('generation_status')
    @classmethod
    def validate_status(cls, v: str) -> str:
        """Ensure status is one of the allowed values."""
        allowed_statuses = {"pending", "generating", "completed", "failed"}
        if v not in allowed_statuses:
            raise ValueError(f"Status must be one of: {', '.join(allowed_statuses)}")
        return v


class GenerationRequest(BaseModel):
    """
    Video generation request payload.

    Contains all parameters needed to initiate a video generation job,
    including the concept prompt, duration, style preferences, and optional
    client-specific branding guidelines.
    """
    prompt: str = Field(
        ...,
        min_length=10,
        max_length=5000,
        description="Video concept description or narrative"
    )
    duration: int = Field(
        default=30,
        ge=5,
        le=300,
        description="Total video duration in seconds (5-300s)"
    )
    style: Optional[str] = Field(
        None,
        max_length=100,
        description="Visual style (e.g., 'cinematic', 'cartoon', 'documentary')"
    )
    aspect_ratio: Optional[str] = Field(
        default="16:9",
        pattern="^(16:9|9:16|1:1|4:3)$",
        description="Video aspect ratio"
    )
    client_id: Optional[str] = Field(
        None,
        max_length=100,
        description="Client identifier for multi-tenant support"
    )
    brand_guidelines: Optional[dict[str, Any]] = Field(
        None,
        description="Client-specific brand guidelines and style preferences"
    )

    @field_validator('prompt')
    @classmethod
    def validate_prompt(cls, v: str) -> str:
        """Ensure prompt has meaningful content."""
        v = v.strip()
        if len(v) < 10:
            raise ValueError("Prompt must be at least 10 characters long")
        return v

    @field_validator('duration')
    @classmethod
    def validate_duration(cls, v: int) -> int:
        """Ensure duration is within acceptable range."""
        if v < 5:
            raise ValueError("Video duration must be at least 5 seconds")
        if v > 300:
            raise ValueError("Video duration cannot exceed 300 seconds (5 minutes)")
        return v

    @field_validator('aspect_ratio')
    @classmethod
    def validate_aspect_ratio(cls, v: Optional[str]) -> str:
        """Validate and normalize aspect ratio."""
        if v is None:
            return "16:9"

        allowed_ratios = {"16:9", "9:16", "1:1", "4:3"}
        if v not in allowed_ratios:
            raise ValueError(f"Aspect ratio must be one of: {', '.join(allowed_ratios)}")
        return v


class VideoProgress(BaseModel):
    """
    Real-time progress tracking for video generation.

    Provides detailed information about the current state of the generation
    process, including scene completion counts and time estimates.
    """
    current_stage: VideoStatus = Field(..., description="Current workflow stage")
    scenes_total: int = Field(default=0, ge=0, description="Total number of scenes in the video")
    scenes_completed: int = Field(default=0, ge=0, description="Number of scenes completed")
    current_scene: Optional[int] = Field(None, ge=1, description="Currently processing scene number")
    estimated_completion_seconds: Optional[int] = Field(
        None,
        ge=0,
        description="Estimated seconds until completion"
    )
    message: Optional[str] = Field(
        None,
        max_length=200,
        description="Human-readable progress message"
    )

    @field_validator('scenes_completed')
    @classmethod
    def validate_scenes_completed(cls, v: int, info) -> int:
        """Ensure scenes_completed doesn't exceed scenes_total."""
        # Note: We can't access scenes_total here in field_validator
        # This will be validated in model_validator if needed
        if v < 0:
            raise ValueError("scenes_completed cannot be negative")
        return v


class JobResponse(BaseModel):
    """
    Complete video generation job response.

    Comprehensive response model containing job metadata, status, progress,
    storyboard, final video URL, and cost information.
    """
    job_id: int = Field(..., ge=1, description="Unique job identifier")
    status: VideoStatus = Field(..., description="Current job status")
    progress: VideoProgress = Field(..., description="Detailed progress information")
    storyboard: Optional[list[StoryboardEntry]] = Field(
        None,
        description="Scene-by-scene storyboard with generated images"
    )
    video_url: Optional[str] = Field(
        None,
        description="URL to final rendered video (when completed)"
    )
    estimated_cost: float = Field(
        ...,
        ge=0,
        description="Estimated cost in USD for the generation job"
    )
    actual_cost: Optional[float] = Field(
        None,
        ge=0,
        description="Actual cost in USD (populated after completion)"
    )
    created_at: datetime = Field(..., description="Job creation timestamp")
    updated_at: datetime = Field(..., description="Last update timestamp")
    approved: bool = Field(default=False, description="Whether the storyboard has been approved")
    error_message: Optional[str] = Field(
        None,
        max_length=1000,
        description="Error details if job failed"
    )

    @field_validator('estimated_cost', 'actual_cost')
    @classmethod
    def validate_cost(cls, v: Optional[float]) -> Optional[float]:
        """Ensure costs are non-negative and reasonable."""
        if v is not None:
            if v < 0:
                raise ValueError("Cost cannot be negative")
            if v > 10000:  # Sanity check for maximum cost
                raise ValueError("Cost exceeds maximum allowed value")
            return round(v, 2)  # Round to 2 decimal places
        return v

    class Config:
        """Pydantic model configuration."""
        json_encoders = {
            datetime: lambda v: v.isoformat()
        }
        use_enum_values = True
</file>

<file path="backend/prompt_parser_service/api/v1/batch.py">
"""Batch parse endpoint."""

from __future__ import annotations

import asyncio
from typing import Any, List

from fastapi import APIRouter, Depends, HTTPException, status

from api.v1.parse import process_parse_request
from ...core.dependencies import get_cache_manager, get_llm_provider_registry
from ...models.request import ParseRequest
from ...services.cache import CacheManager
from ...services.llm.base import LLMProvider

router = APIRouter()


@router.post("/parse/batch")
async def parse_batch(
    requests: List[ParseRequest],
    cache: CacheManager = Depends(get_cache_manager),
    llm_providers: dict[str, LLMProvider] = Depends(get_llm_provider_registry),
):
    if len(requests) > 10:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Batch size exceeds maximum of 10 prompts.",
        )

    async def _process(req: ParseRequest):
        return await process_parse_request(req, cache=cache, llm_providers=llm_providers)

    results = await asyncio.gather(*[_process(req) for req in requests], return_exceptions=True)
    formatted = []
    for req, result in zip(requests, results):
        if isinstance(result, Exception):
            formatted.append({"request": req, "status": "error", "error": str(result)})
        else:
            formatted.append({"request": req, "status": "success", "response": result})

    return {
        "status": "partial_success"
        if any(item["status"] == "error" for item in formatted)
        else "success",
        "results": formatted,
    }
</file>

<file path="backend/prompt_parser_service/api/v1/briefs.py">
"""Brief management endpoints."""

from typing import List, Dict, Any, Optional

from fastapi import APIRouter, Depends, HTTPException, Query
from pydantic import BaseModel

from ...core.dependencies import get_cache_manager, get_llm_provider_registry
from ...services.cache import CacheManager
from ...services.llm.base import LLMProvider
from ....database import get_user_briefs, get_creative_brief, update_brief, get_brief_count, delete_brief
from ....auth import verify_auth

router = APIRouter()


class BriefRefinementRequest(BaseModel):
    """Request model for brief refinement."""
    text: Optional[str] = None
    image_url: Optional[str] = None
    video_url: Optional[str] = None
    creative_direction: Optional[Dict[str, Any]] = None
    scenes: Optional[List[Dict[str, Any]]] = None


class BriefsResponse(BaseModel):
    """Response model for briefs list."""
    briefs: List[Dict[str, Any]]
    totalPages: int


@router.get("/briefs", response_model=BriefsResponse)
async def get_user_creative_briefs(
    page: int = Query(1, ge=1, description="Page number"),
    limit: int = Query(50, ge=1, le=100, description="Items per page"),
    current_user: Dict[str, Any] = Depends(verify_auth),
) -> BriefsResponse:
    """
    Get paginated list of user's creative briefs.
    Requires authentication.
    """
    try:
        print(f"DEBUG: Getting briefs for user {current_user['id']}, page {page}, limit {limit}")
        offset = (page - 1) * limit
        briefs = get_user_briefs(current_user["id"], limit=limit, offset=offset)
        total_count = get_brief_count(current_user["id"])
        total_pages = (total_count + limit - 1) // limit  # Ceiling division
        print(f"DEBUG: Found {len(briefs)} briefs, total pages: {total_pages}")
        return BriefsResponse(briefs=briefs, totalPages=total_pages)
    except Exception as e:
        print(f"DEBUG: Error in get_user_creative_briefs: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to retrieve briefs: {str(e)}")


@router.get("/briefs/count")
async def get_user_brief_count(
    current_user: Dict[str, Any] = Depends(verify_auth),
) -> Dict[str, int]:
    """
    Get the total count of briefs for the authenticated user.
    Requires authentication.
    """
    try:
        count = get_brief_count(current_user["id"])
        return {"count": count}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get brief count: {str(e)}")


@router.get("/briefs/{brief_id}", response_model=Dict[str, Any])
async def get_creative_brief_by_id(
    brief_id: str,
    current_user: Dict[str, Any] = Depends(verify_auth),
) -> Dict[str, Any]:
    """
    Get a specific creative brief by ID.
    Requires authentication and ownership.
    """
    try:
        brief = get_creative_brief(brief_id, current_user["id"])
        if not brief:
            raise HTTPException(status_code=404, detail="Brief not found")
        return brief
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to retrieve brief: {str(e)}")


@router.post("/briefs/{brief_id}/refine", response_model=Dict[str, Any])
async def refine_creative_brief(
    brief_id: str,
    refinement: BriefRefinementRequest,
    current_user: Dict[str, Any] = Depends(verify_auth),
    cache: CacheManager = Depends(get_cache_manager),
    llm_providers: Dict[str, LLMProvider] = Depends(get_llm_provider_registry),
) -> Dict[str, Any]:
    """
    Refine an existing creative brief with additional input.
    Requires authentication and ownership.
    """
    try:
        # First check if brief exists and belongs to user
        existing_brief = get_creative_brief(brief_id, current_user["id"])
        if not existing_brief:
            raise HTTPException(status_code=404, detail="Brief not found")

        # Prepare refinement data
        refinement_data = {}
        if refinement.text is not None:
            refinement_data["prompt_text"] = refinement.text
        if refinement.image_url is not None:
            refinement_data["image_url"] = refinement.image_url
        if refinement.video_url is not None:
            refinement_data["video_url"] = refinement.video_url
        if refinement.creative_direction is not None:
            import json
            refinement_data["creative_direction"] = json.dumps(refinement.creative_direction)
        if refinement.scenes is not None:
            import json
            refinement_data["scenes"] = json.dumps(refinement.scenes)

        if not refinement_data:
            raise HTTPException(status_code=400, detail="No refinement data provided")

        # Update the brief
        success = update_brief(brief_id, current_user["id"], **refinement_data)
        if not success:
            raise HTTPException(status_code=404, detail="Brief not found or update failed")

        # Invalidate cache for this brief
        cache_key = f"brief_{brief_id}"
        await cache.delete(cache_key)

        # Return updated brief
        updated_brief = get_creative_brief(brief_id, current_user["id"])
        if not updated_brief:
            raise HTTPException(status_code=500, detail="Failed to retrieve updated brief")

        return updated_brief

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to refine brief: {str(e)}")


@router.delete("/briefs/{brief_id}")
async def delete_creative_brief(
    brief_id: str,
    current_user: Dict[str, Any] = Depends(verify_auth),
    cache: CacheManager = Depends(get_cache_manager),
) -> Dict[str, bool]:
    """
    Delete a creative brief.
    Requires authentication and ownership.
    """
    try:
        # Delete the brief
        success = delete_brief(brief_id, current_user["id"])
        if not success:
            raise HTTPException(status_code=404, detail="Brief not found or already deleted")

        # Invalidate cache for this brief
        cache_key = f"brief_{brief_id}"
        await cache.delete(cache_key)

        return {"success": True}

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to delete brief: {str(e)}")
</file>

<file path="backend/prompt_parser_service/api/v1/cache_admin.py">
"""Cache admin endpoints."""

from fastapi import APIRouter, Depends, HTTPException

from ...services.cache import CacheManager
from ...core.dependencies import get_cache_manager

router = APIRouter()


@router.post("/cache/clear")
async def clear_cache(cache: CacheManager = Depends(get_cache_manager)):
    cleared = await cache.clear_all()
    return {"status": "success", "cleared": cleared}
</file>

<file path="backend/prompt_parser_service/api/v1/health.py">
"""Health endpoint."""

from __future__ import annotations

from fastapi import APIRouter, Depends

from ...core.dependencies import get_cache_manager, get_llm_provider_registry
from ...services.cache import CacheManager
from ...services.llm.base import LLMProvider

router = APIRouter()


@router.get("/health")
async def health(
    cache: CacheManager = Depends(get_cache_manager),
    llm_providers: dict[str, LLMProvider] = Depends(get_llm_provider_registry),
) -> dict[str, str | bool]:
    redis_ok = True
    try:
        await cache.redis.ping()
    except Exception:  # pragma: no cover
        redis_ok = False

    provider_ok = any([await provider.is_available() for provider in llm_providers.values()])
    status = "healthy" if redis_ok and provider_ok else "degraded"

    return {
        "status": status,
        "redis": redis_ok,
        "llm_available": provider_ok,
    }
</file>

<file path="backend/prompt_parser_service/api/v1/metrics.py">
"""Metrics endpoint."""

from fastapi import APIRouter
from prometheus_client import CONTENT_TYPE_LATEST, generate_latest
from fastapi.responses import Response

router = APIRouter()


@router.get("/metrics")
async def metrics():
    return Response(generate_latest(), media_type=CONTENT_TYPE_LATEST)
</file>

<file path="backend/prompt_parser_service/api/v1/parse.py">
"""Parse endpoint."""

import json
from typing import Any, Tuple, List, Dict, Optional

import structlog
from fastapi import APIRouter, Depends, HTTPException, status, Request
from pydantic import ValidationError

# Import auth from main backend
from ....auth import verify_auth

from ....config import get_settings
from ...core.dependencies import get_cache_manager, get_llm_provider_registry
from ...models.request import ParseRequest, PromptInput
from ...models.response import ParseResponse, Scene
from ...prompts.creative_direction import (
    CREATIVE_DIRECTION_SYSTEM_PROMPT,
    build_creative_direction_prompt,
)
from ...services.cache import CacheManager, generate_cache_key
from ...core.limiter import limiter
from ...services.defaults import apply_smart_defaults
from ...services.llm.base import LLMProvider
from ...services.input_orchestrator import analyze_inputs
from ...services.parsers.text_parser import parse_text_prompt
from ...services.scene_generator import generate_scenes
from ...services.validator import calculate_confidence, validate_scenes
from ...services.edit_handler import merge_iterative_edit
from ...services.cost_estimator import estimate_cost
from ...services.content_safety import ensure_prompt_safe, ContentSafetyError

# Import database functions
from ....database import save_creative_brief
import uuid

logger = structlog.get_logger(__name__)

router = APIRouter()


async def process_parse_request(
    parse_request: ParseRequest,
    cache: CacheManager,
    llm_providers: dict[str, LLMProvider],
    bypass_cache: bool = False,
    model_name: str = None,
) -> ParseResponse:
    payload = parse_request.model_dump()
    if model_name:
        payload['model'] = model_name  # Include model in key for bypass
    cache_key = generate_cache_key(payload)
    if not bypass_cache:
        cached = await cache.get(cache_key)
        if cached:
            cached["metadata"]["cache_hit"] = True
            return ParseResponse(**cached)

    parsed_prompt = parse_text_prompt(parse_request.prompt.text or "")
    defaults = apply_smart_defaults(parsed_prompt.to_dict())
    input_analysis = await analyze_inputs(parse_request.prompt)
    merged_context = None
    if parse_request.context and parse_request.context.previous_config:
        merged_context = merge_iterative_edit(parse_request.context.previous_config, parse_request.prompt.text or "")

    user_prompt = build_creative_direction_prompt(
        parse_request.prompt.text or "",
        extracted_parameters=parsed_prompt.to_dict(),
        applied_defaults=defaults,
        visual_context=input_analysis.reference_summary if input_analysis else None,
        previous_config=merged_context,
    )

    settings = get_settings()
    default_provider = "mock" if settings.USE_MOCK_LLM else settings.DEFAULT_LLM_PROVIDER
    primary_name = parse_request.options.llm_provider or default_provider
    provider_order: list[tuple[str, LLMProvider]] = []
    if provider := llm_providers.get(primary_name):
        provider_order.append((primary_name, provider))
    # add fallback providers if not already queued
    for name, provider in llm_providers.items():
        if all(provider is existing for _, existing in provider_order):
            continue
        provider_order.append((name, provider))

    creative_direction = None
    provider_used_name: Optional[str] = None
    last_error: Exception | None = None
    tried = []
    for provider_name, provider in provider_order:
        tried.append(provider_name)
        try:
            completion = await provider.complete(
                user_prompt,
                system_prompt=CREATIVE_DIRECTION_SYSTEM_PROMPT,
                response_format={"type": "json_object"},
            )
            creative_direction = json.loads(completion or "{}")
            provider_used_name = provider_name
            break
        except Exception as exc:  # pragma: no cover
            last_error = exc
            continue

    if creative_direction is None:
        raise HTTPException(
            status_code=status.HTTP_502_BAD_GATEWAY,
            detail=f"LLM providers failed: {last_error}",
        ) from last_error

    visual_direction = creative_direction.setdefault("visual_direction", {})
    if input_analysis:
        visual_direction = creative_direction.setdefault("visual_direction", {})
        visual_direction["style_source"] = input_analysis.style_source

    scenes, scene_warnings = _prepare_scenes(creative_direction)
    warnings = scene_warnings + validate_scenes(creative_direction, scenes)
    confidence = calculate_confidence(parsed_prompt.to_dict(), scenes, warnings)
    metadata = {
        "cache_hit": False,
        "defaults_used": defaults.get("metadata", {}).get("defaults_used", []),
        "warnings": warnings,
        **confidence,
    }
    if provider_used_name:
        metadata["llm_provider_used"] = provider_used_name

    response_dict: dict[str, Any] = {
        "status": "success",
        "creative_direction": creative_direction,
        "scenes": scenes,
        "metadata": metadata,
    }
    if parse_request.cost_estimate is not None:
        response_dict["cost_estimate"] = parse_request.cost_estimate
    elif parse_request.options.include_cost_estimate and parse_request.options.cost_fallback_enabled:
        response_dict["cost_estimate"] = estimate_cost(scenes)
    if input_analysis:
        response_dict["extracted_references"] = input_analysis.extracted_references

    await cache.set(cache_key, response_dict)
    return ParseResponse(**response_dict)


# derive rate limit from settings so tests can override RATE_LIMIT_PER_MINUTE
_parse_rate_limit = f"{get_settings().RATE_LIMIT_PER_MINUTE}/minute"


@router.post("/parse", response_model=ParseResponse)
@limiter.limit(_parse_rate_limit)
async def parse_prompt(
    request: Request,
    parse_request: ParseRequest,
    bypass_cache: bool = False,
    current_user: Dict[str, Any] = Depends(verify_auth),  # Add authentication
    cache: CacheManager = Depends(get_cache_manager),
    llm_providers: dict[str, LLMProvider] = Depends(get_llm_provider_registry),
) -> ParseResponse:
    # Content safety check
    try:
        await ensure_prompt_safe(parse_request.prompt.text or "")
    except ContentSafetyError as exc:
        raise HTTPException(status_code=400, detail=str(exc)) from exc

    # Determine primary LLM provider
    settings = get_settings()
    default_provider = "mock" if settings.USE_MOCK_LLM else settings.DEFAULT_LLM_PROVIDER
    primary_name = parse_request.options.llm_provider or default_provider

    # Attempt full processing first
    try:
        response = await process_parse_request(parse_request, cache, llm_providers, bypass_cache=bypass_cache, model_name=primary_name)

        # Save brief to database
        brief_id = None
        try:
            brief_id = str(uuid.uuid4())
            save_creative_brief(
                brief_id=brief_id,
                user_id=current_user["id"],
                prompt_text=parse_request.prompt.text,
                image_url=parse_request.prompt.image_url,
                video_url=parse_request.prompt.video_url,
                image_data=None,  # From upload endpoint
                video_data=None,
                creative_direction=response.creative_direction,
                scenes=[scene.model_dump() for scene in response.scenes] if response.scenes else None,
                confidence_score=response.metadata.confidence_score if response.metadata else None
            )
            logger.info(f"Saved creative brief {brief_id} for user {current_user['id']}")
        except Exception as db_error:
            logger.error(f"Failed to save brief to database: {db_error}")
            # Don't fail the request if DB save fails, just log it

        # NOTE: Physics scene auto-generation is disabled here
        # Physics scenes should be generated separately via the /api/physics/generate endpoint
        # This keeps the creative brief generation focused on prompt parsing and brief creation
        # The generation_prompt in scenes can be used later for physics simulation generation

        # Add brief_id to response
        response.briefId = brief_id
        return response

    except Exception as e:
        logger.warning(f"Full processing failed: {e}, attempting fallback to text-only")

        # Fallback: Create text-only request if media processing failed
        if parse_request.prompt.image_url or parse_request.prompt.video_url or parse_request.prompt.image_base64 or parse_request.prompt.video_base64:
            text_only_request = ParseRequest(
                prompt=PromptInput(
                    text=parse_request.prompt.text,
                    # Exclude media fields for fallback
                ),
                options=parse_request.options,
                context=parse_request.context
            )

            try:
                logger.info("Attempting text-only processing as fallback")
                response = await process_parse_request(text_only_request, cache, llm_providers, bypass_cache, primary_name)

                # Save fallback brief to database
                brief_id = None
                try:
                    brief_id = str(uuid.uuid4())
                    save_creative_brief(
                        brief_id=brief_id,
                        user_id=current_user["id"],
                        prompt_text=parse_request.prompt.text,
                        # No media URLs for fallback
                        creative_direction=response.creative_direction,
                        scenes=[scene.model_dump() for scene in response.scenes] if response.scenes else None,
                        confidence_score=response.metadata.confidence_score if response.metadata else None
                    )
                    logger.info(f"Saved fallback creative brief {brief_id} for user {current_user['id']}")
                except Exception as db_error:
                    logger.error(f"Failed to save fallback brief to database: {db_error}")

                # Add brief_id to response
                response.briefId = brief_id
                return response

            except Exception as fallback_error:
                logger.error(f"Text-only fallback also failed: {fallback_error}")
                raise HTTPException(
                    status_code=500,
                    detail="Processing failed for both full and text-only modes. Please check your input and try again."
                ) from fallback_error
        else:
            # No media to fall back from, re-raise original error
            raise HTTPException(status_code=500, detail=f"Processing failed: {str(e)}") from e


def _prepare_scenes(creative_direction: dict[str, Any]) -> Tuple[List[dict[str, Any]], List[str]]:
    """Normalize LLM scenes or regenerate defaults if invalid."""
    raw_scenes = creative_direction.get("scenes")
    if not raw_scenes:
        generated = generate_scenes(creative_direction)
        creative_direction["scenes"] = generated
        return generated, [
            "Scenes auto-generated because LLM response omitted required fields."
        ]

    normalized: List[dict[str, Any]] = []
    for idx, raw in enumerate(raw_scenes):
        try:
            scene = Scene.model_validate(raw)
        except ValidationError as exc:
            logger.warning(
                "scene_validation_failed",
                scene_index=idx,
                errors=exc.errors(),
            )
            generated = generate_scenes(creative_direction)
            creative_direction["scenes"] = generated
            return generated, [
                "Scenes regenerated because LLM output did not match the schema."
            ]
        normalized.append(scene.model_dump())

    creative_direction["scenes"] = normalized
    return normalized, []
</file>

<file path="backend/prompt_parser_service/api/v1/providers.py">
"""Providers endpoint."""

from fastapi import APIRouter, Depends

from ...core.dependencies import get_llm_provider_registry

router = APIRouter()


@router.get("/providers")
async def list_providers(providers=Depends(get_llm_provider_registry)):
    data = []
    for name, provider in providers.items():
        data.append(
            {
                "id": name,
                "name": provider.__class__.__name__,
                "estimated_latency_ms": provider.get_estimated_latency(),
            }
        )
    return {"providers": data}
</file>

<file path="backend/prompt_parser_service/api/v1/upload.py">
from fastapi import APIRouter, Depends, UploadFile, File, Form, HTTPException
from typing import Optional
import base64
from ....auth import verify_auth
from ....database import save_creative_brief  # For direct save if needed

router = APIRouter(prefix="/creative", tags=["creative"])

@router.post("/upload")
async def upload_media(
    brief_id: str = Form(...),
    file: Optional[UploadFile] = File(None),
    base64_data: Optional[str] = Form(None),
    is_image: bool = Form(True),
    current_user = Depends(verify_auth)
):
    if not file and not base64_data:
        raise HTTPException(400, "Provide file or base64_data")
    
    data = None
    if file:
        content = await file.read()
        if len(content) > 10 * 1024 * 1024:  # 10MB
            raise HTTPException(413, "File too large")
        data = content
    elif base64_data:
        try:
            data = base64.b64decode(base64_data)
            if len(data) > 10 * 1024 * 1024:
                raise HTTPException(413, "Base64 data too large")
        except:
            raise HTTPException(400, "Invalid base64")
    
    if not data:
        raise HTTPException(400, "No data received")
    
    # Save to brief BLOB (assume brief exists)
    from ...database import get_creative_brief, update_brief
    brief = get_creative_brief(brief_id, current_user["id"])
    if not brief:
        raise HTTPException(404, "Brief not found")
    
    if is_image:
        update_brief(brief_id, current_user["id"], image_data=data)
    else:
        update_brief(brief_id, current_user["id"], video_data=data)
    
    return {"brief_id": brief_id, "size": len(data), "type": "image" if is_image else "video"}
</file>

<file path="backend/prompt_parser_service/core/config.py">
"""Application configuration."""

from functools import lru_cache
from typing import Literal, Optional, Union

from pydantic import Field, field_validator
from pydantic_settings import BaseSettings, SettingsConfigDict


class Settings(BaseSettings):
    APP_ENV: Literal["development", "staging", "production"] = "development"
    LOG_LEVEL: str = "INFO"
    PORT: int = Field(8080, ge=1, le=65535)
    OPENAI_API_KEY: Optional[str] = None
    ANTHROPIC_API_KEY: Optional[str] = None
    OPENROUTER_API_KEY: Optional[str] = None
    REDIS_URL: str = "redis://localhost:6379/0"
    RATE_LIMIT_PER_MINUTE: int = Field(60, ge=1)
    USE_MOCK_LLM: bool = False
    DEFAULT_LLM_PROVIDER: str = Field("openrouter", description="Default LLM provider")

    model_config = SettingsConfigDict(
        env_file=".env",
        env_file_encoding="utf-8",
        case_sensitive=False,
        extra="allow",  # Allow extra env vars from main backend
    )

    @field_validator("RATE_LIMIT_PER_MINUTE", mode="before")
    @classmethod
    def _clean_rate_limit(cls, value: Union[int, str, None]) -> Optional[int]:
        if isinstance(value, str):
            value = value.strip()
            if value == "":
                return None
        return int(value) if value is not None else None

    @field_validator("USE_MOCK_LLM", mode="before")
    @classmethod
    def _clean_use_mock(cls, value):
        if isinstance(value, str):
            normalized = value.strip().lower()
            if normalized in {"1", "true", "yes", "on"}:
                return True
            if normalized in {"0", "false", "no", "off", ""}:
                return False
        return value
        if isinstance(value, str):
            normalized = value.strip().lower()
            if normalized in {"1", "true", "yes", "on"}:
                return True
            if normalized in {"0", "false", "no", "off", ""}:
                return False
        return value


@lru_cache
def get_settings() -> Settings:
    """Return cached settings instance."""
    return Settings()
</file>

<file path="backend/prompt_parser_service/core/dependencies.py">
"""FastAPI dependency providers."""

from __future__ import annotations

from functools import lru_cache

from ...config import get_settings
from ..services.cache import CacheManager
from ..services.llm.base import LLMProvider
from ..services.llm.openai_provider import OpenAIProvider
from ..services.llm.claude_provider import ClaudeProvider
from ..services.llm.openrouter_provider import OpenRouterProvider
from ..services.llm.mock_provider import MockProvider


@lru_cache
def _cache_manager() -> CacheManager:
    # Use SQLite-based cache instead of Redis
    return CacheManager("./cache.db")


@lru_cache
def _llm_providers() -> dict[str, LLMProvider]:
    settings = get_settings()
    providers: dict[str, LLMProvider] = {}
    if settings.USE_MOCK_LLM:
        providers["mock"] = MockProvider()
        return providers

    # Register OpenRouter with GPT-5-nano as the primary provider
    if settings.OPENROUTER_API_KEY:
        providers["openrouter"] = OpenRouterProvider(model="openai/gpt-5-nano-2025-08-07")

    # Fallback providers
    if settings.OPENAI_API_KEY:
        providers["openai"] = OpenAIProvider()
    if settings.ANTHROPIC_API_KEY:
        providers["claude"] = ClaudeProvider()
    return providers


def get_cache_manager() -> CacheManager:
    return _cache_manager()


def get_llm_provider_registry() -> dict[str, LLMProvider]:
    return _llm_providers()
</file>

<file path="backend/prompt_parser_service/core/limiter.py">
"""Rate limiter instance."""

from slowapi import Limiter
from slowapi.util import get_remote_address

from .config import get_settings

settings = get_settings()
limiter = Limiter(
    key_func=get_remote_address,
    default_limits=[f"{settings.RATE_LIMIT_PER_MINUTE}/minute"],
)
</file>

<file path="backend/prompt_parser_service/core/logging.py">
"""Logging configuration helpers."""

import logging

import structlog


def configure_logging(log_level: str = "INFO") -> None:
    """Configure stdlib + structlog logging."""
    logging.basicConfig(
        level=getattr(logging, log_level.upper(), logging.INFO),
        format="%(message)s",
        force=True,
    )

    structlog.configure(
        processors=[
            structlog.processors.TimeStamper(fmt="iso"),
            structlog.processors.add_log_level,
            structlog.processors.StackInfoRenderer(),
            structlog.processors.format_exc_info,
            structlog.processors.JSONRenderer(),
        ],
        logger_factory=structlog.stdlib.LoggerFactory(),
        wrapper_class=structlog.stdlib.BoundLogger,
        cache_logger_on_first_use=True,
    )
</file>

<file path="backend/prompt_parser_service/core/metrics.py">
"""Metrics registry (no-op for now)."""

# No-op metrics since we're not using Prometheus
class NoOpMetric:
    def __init__(self, *args, **kwargs):
        pass

    def inc(self, *args, **kwargs):
        pass

    def dec(self, *args, **kwargs):
        pass

    def observe(self, *args, **kwargs):
        pass

    def time(self, *args, **kwargs):
        return lambda: None

REQUEST_LATENCY = NoOpMetric()
REQUEST_ERRORS = NoOpMetric()
CACHE_HITS = NoOpMetric()
CACHE_MISSES = NoOpMetric()
</file>

<file path="backend/prompt_parser_service/models/request.py">
"""Request models."""

from __future__ import annotations

from typing import Any, Optional

from pydantic import BaseModel, Field, model_validator, field_validator
import re
import base64
from urllib.parse import urlparse


class PromptInput(BaseModel):
    text: Optional[str] = Field(None, max_length=5000)
    image_url: Optional[str] = None
    image_base64: Optional[str] = None
    video_url: Optional[str] = None
    video_base64: Optional[str] = None

    @field_validator('image_url', 'video_url', mode='before')
    @classmethod
    def validate_and_sanitize_url(cls, v):
        if v is None:
            return v

        # Convert to string and strip whitespace
        v = str(v).strip()

        if not v:
            return None

        # Basic URL validation
        try:
            parsed = urlparse(v)
            if not parsed.scheme or not parsed.netloc:
                raise ValueError("Invalid URL format")
        except Exception:
            raise ValueError("Invalid URL format")

        # Only allow http and https
        if parsed.scheme not in ['http', 'https']:
            raise ValueError("Only HTTP and HTTPS URLs are allowed")

        # Basic security checks - block localhost/private IPs
        hostname = parsed.hostname
        if hostname and isinstance(hostname, str):
            hostname = hostname.lower()
            if hostname in ['localhost', '127.0.0.1', '::1'] or hostname.startswith('192.168.') or hostname.startswith('10.') or hostname.startswith('172.'):
                raise ValueError("Local/private network URLs are not allowed")

        # Check for potentially malicious patterns
        suspicious_patterns = [
            r'<script', r'javascript:', r'data:', r'vbscript:',
            r'on\w+\s*=', r'&#', r'%3C', r'%3E'
        ]

        for pattern in suspicious_patterns:
            if re.search(pattern, v, re.IGNORECASE):
                raise ValueError("Potentially malicious URL detected")

        return v

    @field_validator('image_base64', 'video_base64', mode='before')
    @classmethod
    def validate_base64_data(cls, v):
        if v is None:
            return v

        # Convert to string and strip whitespace
        v = str(v).strip()

        if not v:
            return None

        # Basic base64 validation
        try:
            # Remove data URL prefix if present
            if v.startswith('data:'):
                v = v.split(',', 1)[1] if ',' in v else v

            # Validate base64 format
            base64.b64decode(v, validate=True)

            # Basic security check - reject if too large (prevent DoS)
            if len(v) > 10 * 1024 * 1024:  # 10MB limit
                raise ValueError("Base64 data too large")

        except Exception:
            raise ValueError("Invalid base64 data")

        return v

    @model_validator(mode="after")
    def validate_input(cls, values):
        if not any(
            [
                values.text,
                values.image_url,
                values.image_base64,
                values.video_url,
                values.video_base64,
            ]
        ):
            raise ValueError("At least one of text, image, or video input must be provided.")
        return values


class ParseOptions(BaseModel):
    llm_provider: Optional[str] = None
    include_cost_estimate: bool = False
    cost_fallback_enabled: bool = True


class ParseContext(BaseModel):
    previous_config: Optional[dict[str, Any]] = None


class ParseRequest(BaseModel):
    prompt: PromptInput
    options: ParseOptions = ParseOptions()
    cost_estimate: Optional[dict[str, Any]] = None
    context: Optional[ParseContext] = None
</file>

<file path="backend/prompt_parser_service/models/response.py">
"""Response models."""

from __future__ import annotations

from typing import Any, List, Optional

from pydantic import BaseModel, Field


class SceneVisual(BaseModel):
    shot_type: Optional[str] = None
    subject: Optional[str] = None
    generation_prompt: Optional[str] = None


class Scene(BaseModel):
    id: str
    scene_number: int
    purpose: str
    duration: float
    visual: SceneVisual


class Metadata(BaseModel):
    cache_hit: bool = False
    defaults_used: List[str] = Field(default_factory=list)
    warnings: List[str] = Field(default_factory=list)
    confidence_score: Optional[float] = None
    confidence_breakdown: Optional[dict[str, float]] = None
    llm_provider_used: Optional[str] = None
    auto_generated_scene: Optional[dict[str, Any]] = None


class ParseResponse(BaseModel):
    status: str = "success"
    creative_direction: dict[str, Any]
    scenes: List[Scene]
    metadata: Metadata
    cost_estimate: Optional[dict[str, Any]] = None
    extracted_references: Optional[dict[str, Any]] = None
    briefId: Optional[str] = None  # Added for frontend compatibility
</file>

<file path="backend/prompt_parser_service/prompts/creative_direction.py">
"""Prompt templates for creative direction generation."""

from __future__ import annotations

import json
from textwrap import dedent
from typing import Any


CREATIVE_DIRECTION_SYSTEM_PROMPT = dedent(
    """
    You are an award-winning ad creative director.
    Always respond with valid JSON matching the creative_direction schema:
    {
      "product": {"name": "", "category": "", "description": "", "price_tier": ""},
      "technical_specs": {"duration": 0, "aspect_ratio": "", "platform": "", "resolution": "", "fps": 30},
      "visual_direction": {
        "aesthetic": "",
        "style_source": "",
        "color_palette": [{"hex": "", "role": ""}],
        "lighting_style": "",
        "camera_style": "",
        "scene_types": []
      },
      "audio_direction": {
        "music_genre": "",
        "mood": [],
        "tempo": "",
        "intensity_curve": "",
        "instruments": []
      },
      "text_strategy": {
        "overlays": [],
        "font_family": "",
        "text_color": "",
        "outline_color": ""
      },
      "pacing": {
        "overall": "",
        "scene_duration_avg": 0,
        "transition_style": "",
        "cuts_per_minute": 0,
        "energy_curve": ""
      },
      "cta": {"text": "", "start_time": 0, "duration": 0, "style": "", "action": ""},
      "scenes": [
        {
          "id": "scene_1",
          "scene_number": 1,
          "purpose": "",
          "duration": 5.0,
          "visual": {
            "shot_type": "",
            "subject": "",
            "generation_prompt": ""
          }
        }
      ]
    }
    IMPORTANT: Each scene in the "scenes" array must have:
    - id: string (e.g., "scene_1", "scene_2")
    - scene_number: integer (1, 2, 3, etc.)
    - purpose: string describing the scene's role
    - duration: float in seconds
    - visual: object with shot_type, subject, and generation_prompt fields

    Include a "metadata" section containing warnings, defaults_used, and reasoning summaries.
    """
).strip()


def build_creative_direction_prompt(
    user_prompt: str,
    *,
    extracted_parameters: dict[str, Any],
    applied_defaults: dict[str, Any],
    visual_context: dict[str, Any] | None = None,
    previous_config: dict[str, Any] | None = None,
) -> str:
    """Return user prompt for LLM completion."""
    previous_section = ""
    if previous_config:
        previous_section = f"""
        Previous creative direction to update:
        {json.dumps(previous_config, indent=2)}
        """
    visual_section = ""
    if visual_context:
        visual_section = f"""
        Visual references summary:
        {json.dumps(visual_context, indent=2)}
        """

    return dedent(
        f"""
        User prompt:
        \"\"\"{user_prompt}\"\"\"

        Extracted parameters:
        {json.dumps(extracted_parameters, indent=2)}

        Defaults applied:
        {json.dumps(applied_defaults, indent=2)}

        {visual_section}
        {previous_section}

        Instructions:
        - Merge the extracted parameters with defaults intelligently.
        - Fill in missing details while staying faithful to user intent.
        - Produce coherent scene order with hooks, product showcase, benefits, CTA.
        - Include confidence rationale in metadata.confidence_breakdown.
        - Mention any assumptions in metadata.warnings or defaults_used.
        """
    ).strip()
</file>

<file path="backend/prompt_parser_service/services/llm/base.py">
"""LLM provider abstraction."""

from __future__ import annotations

from abc import ABC, abstractmethod
from typing import Any, Optional


class LLMProvider(ABC):
    """Base interface for LLM providers."""

    name: str

    @abstractmethod
    async def complete(
        self,
        prompt: str,
        *,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        response_format: dict[str, Any] | None = None,
    ) -> str:
        """Generate a completion for the given prompt."""

    @abstractmethod
    async def analyze_image(self, image_b64: str, question: str) -> dict[str, Any]:
        """Analyze an image along with textual instructions."""

    @abstractmethod
    async def is_available(self) -> bool:
        """Return current availability."""

    @abstractmethod
    def get_estimated_latency(self) -> int:
        """Return estimated latency in milliseconds."""
</file>

<file path="backend/prompt_parser_service/services/llm/claude_provider.py">
"""Claude provider implementation."""

from __future__ import annotations

import json
from typing import Any, Optional

from anthropic import AsyncAnthropic
import structlog

from ...core.config import get_settings
from .base import LLMProvider

logger = structlog.get_logger(__name__)


class ClaudeProvider(LLMProvider):
    """Wrapper around Claude Sonnet."""

    def __init__(self, model: str = "claude-3-sonnet-20240229", *, client: AsyncAnthropic | None = None) -> None:
        settings = get_settings()
        api_key = settings.ANTHROPIC_API_KEY
        if client is None and not api_key:
            raise RuntimeError("ANTHROPIC_API_KEY is required for ClaudeProvider")

        self.client = client or AsyncAnthropic(api_key=api_key)
        self.model = model
        self._available = True
        self._latency_ms = 4000

    async def complete(
        self,
        prompt: str,
        *,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        response_format: dict[str, Any] | None = None,
    ) -> str:
        try:
            response = await self.client.messages.create(
                model=self.model,
                system=system_prompt or "You are an expert creative director.",
                max_tokens=4000,
                temperature=temperature,
                messages=[{"role": "user", "content": prompt}],
            )
            self._available = True
            content = response.content[0].text if response.content else ""
            return content
        except Exception as exc:  # pragma: no cover
            self._available = False
            logger.warning("claude.complete_failed", error=str(exc))
            raise

    async def analyze_image(self, image_b64: str, question: str) -> dict[str, Any]:
        try:
            response = await self.client.messages.create(
                model=self.model,
                max_tokens=2000,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {"type": "image", "source": {"type": "base64", "media_type": "image/jpeg", "data": image_b64}},
                            {"type": "text", "text": question},
                        ],
                    }
                ],
            )
            self._available = True
            raw = response.content[0].text if response.content else "{}"
            return json.loads(raw)
        except Exception as exc:  # pragma: no cover
            self._available = False
            logger.warning("claude.analyze_image_failed", error=str(exc))
            raise

    async def is_available(self) -> bool:
        return self._available

    def get_estimated_latency(self) -> int:
        return self._latency_ms
</file>

<file path="backend/prompt_parser_service/services/llm/mock_provider.py">
"""Mock LLM provider for local testing / load tests."""

from __future__ import annotations

import asyncio
import json

from .base import LLMProvider


class MockProvider(LLMProvider):
    """Simple deterministic provider that avoids external LLM calls."""

    async def complete(self, prompt: str, system_prompt: Optional[str] = None, response_format: Optional[dict] = None) -> str:  # noqa: ARG002
        await asyncio.sleep(0)  # keep signature async-friendly
        fake_response = {
            "product": {"name": "Mock Product", "category": "mock_category", "description": "Generated by MockProvider"},
            "technical_specs": {"duration": 20, "aspect_ratio": "9:16", "platform": "tiktok", "resolution": "1080x1920"},
            "visual_direction": {"aesthetic": "mock", "style_source": "text"},
            "scenes": [
                {
                    "id": "scene_1",
                    "scene_number": 1,
                    "start_time": 0.0,
                    "duration": 5.0,
                    "purpose": "hook",
                    "visual": {"shot_type": "medium_shot", "generation_prompt": "mock scene"},
                }
            ],
        }
        return json.dumps(fake_response)

    async def analyze_image(self, image_data: bytes, question: str) -> dict:  # noqa: ARG002
        await asyncio.sleep(0)
        return {
            "dominant_colors": ["#FFFFFF"],
            "lighting": "mock",
            "mood": "mock",
        }

    async def is_available(self) -> bool:
        return True

    def get_estimated_latency(self) -> int:
        return 50
</file>

<file path="backend/prompt_parser_service/services/llm/openai_provider.py">
"""OpenAI provider implementation."""

from __future__ import annotations

import json
from typing import Any, Optional

from openai import AsyncOpenAI
import structlog

from ...core.config import get_settings
from .base import LLMProvider

logger = structlog.get_logger(__name__)


class OpenAIProvider(LLMProvider):
    """Wrapper around OpenAI GPT-4o endpoints."""

    def __init__(self, model: str = "gpt-4o", *, client: AsyncOpenAI | None = None) -> None:
        settings = get_settings()
        api_key = settings.OPENAI_API_KEY
        if client is None and not api_key:
            raise RuntimeError("OPENAI_API_KEY is required for OpenAIProvider")

        self.client = client or AsyncOpenAI(api_key=api_key)
        self.model = model
        self._available = True
        self._latency_ms = 3000

    async def complete(
        self,
        prompt: str,
        *,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        response_format: dict[str, Any] | None = None,
    ) -> str:
        try:
            params: dict[str, Any] = {
                "model": self.model,
                "messages": [
                    {"role": "system", "content": system_prompt or "You are a helpful assistant."},
                    {"role": "user", "content": prompt},
                ],
                "temperature": temperature,
            }
            if response_format:
                params["response_format"] = response_format

            response = await self.client.chat.completions.create(**params)
            self._available = True
            return response.choices[0].message.content or ""
        except Exception as exc:  # pragma: no cover - network errors mocked in tests
            self._available = False
            logger.warning("openai.complete_failed", error=str(exc))
            raise

    async def analyze_image(self, image_b64: str, question: str) -> dict[str, Any]:
        try:
            if not image_b64.startswith("data:"):
                image_b64 = f"data:image/jpeg;base64,{image_b64}"
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": question},
                            {"type": "image_url", "image_url": {"url": image_b64}},
                        ],
                    }
                ],
                response_format={"type": "json_object"},
            )
            self._available = True
            raw = response.choices[0].message.content or "{}"
            return json.loads(raw)
        except Exception as exc:  # pragma: no cover
            self._available = False
            logger.warning("openai.analyze_image_failed", error=str(exc))
            raise

    async def is_available(self) -> bool:
        return self._available

    def get_estimated_latency(self) -> int:
        return self._latency_ms
</file>

<file path="backend/prompt_parser_service/services/llm/openrouter_provider.py">
"""OpenRouter provider implementation."""

from __future__ import annotations

import json
from typing import Any, Optional

from openai import AsyncOpenAI
import structlog

from ...core.config import get_settings
from .base import LLMProvider

logger = structlog.get_logger(__name__)


class OpenRouterProvider(LLMProvider):
    """Wrapper around OpenRouter API endpoints."""

    def __init__(self, model: str = "openai/gpt-5-nano-2025-08-07", *, client: AsyncOpenAI | None = None) -> None:
        settings = get_settings()
        api_key = settings.OPENROUTER_API_KEY
        if client is None and not api_key:
            raise RuntimeError("OPENROUTER_API_KEY is required for OpenRouterProvider")

        # OpenRouter uses OpenAI-compatible API
        self.client = client or AsyncOpenAI(
            api_key=api_key,
            base_url="https://openrouter.ai/api/v1"
        )
        self.model = model
        self._available = True
        self._latency_ms = 2000  # OpenRouter is typically fast

    async def complete(
        self,
        prompt: str,
        *,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        response_format: dict[str, Any] | None = None,
    ) -> str:
        try:
            params: dict[str, Any] = {
                "model": self.model,
                "messages": [
                    {"role": "system", "content": system_prompt or "You are a helpful assistant."},
                    {"role": "user", "content": prompt},
                ],
                "temperature": temperature,
            }
            if response_format:
                params["response_format"] = response_format

            response = await self.client.chat.completions.create(**params)
            self._available = True
            return response.choices[0].message.content or ""
        except Exception as exc:  # pragma: no cover - network errors mocked in tests
            self._available = False
            logger.warning("openrouter.complete_failed", error=str(exc), model=self.model)
            raise

    async def analyze_image(self, image_b64: str, question: str) -> dict[str, Any]:
        try:
            if not image_b64.startswith("data:"):
                image_b64 = f"data:image/jpeg;base64,{image_b64}"
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": question},
                            {"type": "image_url", "image_url": {"url": image_b64}},
                        ],
                    }
                ],
                response_format={"type": "json_object"},
            )
            self._available = True
            raw = response.choices[0].message.content or "{}"
            return json.loads(raw)
        except Exception as exc:  # pragma: no cover
            self._available = False
            logger.warning("openrouter.analyze_image_failed", error=str(exc), model=self.model)
            raise

    async def is_available(self) -> bool:
        return self._available

    def get_estimated_latency(self) -> int:
        return self._latency_ms
</file>

<file path="backend/prompt_parser_service/services/parsers/text_parser.py">
"""Text prompt parsing utilities."""

from __future__ import annotations

import re
from dataclasses import dataclass, field
from typing import Dict, List, Optional


DURATION_PATTERN = re.compile(r"(?P<value>\d+)\s*(seconds?|secs?|s|minutes?|mins?|m)")
PLATFORM_KEYWORDS = {
    "instagram": ["instagram", "reels"],
    "tiktok": ["tiktok"],
    "youtube": ["youtube"],
    "facebook": ["facebook"],
}


@dataclass
class ParsedPrompt:
    duration: Optional[int] = None
    platform: Optional[str] = None
    product: Optional[str] = None
    aesthetic_keywords: List[str] = field(default_factory=list)
    raw_text: str = ""

    def to_dict(self) -> Dict[str, Optional[str]]:
        return {
            "duration": self.duration,
            "platform": self.platform,
            "product": self.product,
            "aesthetic_keywords": self.aesthetic_keywords,
        }


def extract_duration(text: str) -> Optional[int]:
    match = DURATION_PATTERN.search(text.lower())
    if not match:
        return None
    value = int(match.group("value"))
    unit = match.group(0)
    if "min" in unit:
        return value * 60
    return value


def extract_platform(text: str) -> Optional[str]:
    lower = text.lower()
    for platform, keywords in PLATFORM_KEYWORDS.items():
        if any(keyword in lower for keyword in keywords):
            return platform
    return None


def extract_product(text: str) -> Optional[str]:
    match = re.search(r"ad for (?P<product>[a-zA-Z\s]+)", text.lower())
    if match:
        product = match.group("product").strip()
        return product.title()
    return None


def extract_aesthetic_keywords(text: str) -> List[str]:
    keywords = []
    for token in re.findall(r"[a-zA-Z]+", text.lower()):
        if token in {"luxury", "energetic", "minimal", "modern", "bold", "calm"}:
            keywords.append(token)
    return keywords


def parse_text_prompt(text: str) -> ParsedPrompt:
    parsed = ParsedPrompt(
        duration=extract_duration(text),
        platform=extract_platform(text),
        product=extract_product(text),
        aesthetic_keywords=extract_aesthetic_keywords(text),
        raw_text=text,
    )
    return parsed
</file>

<file path="backend/prompt_parser_service/services/cache.py">
"""Redis cache manager for prompt parser."""

from __future__ import annotations

import asyncio
import hashlib
import json
import time
from copy import deepcopy
from typing import Any, Optional

import sqlite3
import structlog

from ..core.metrics import CACHE_HITS, CACHE_MISSES


logger = structlog.get_logger(__name__)


class CacheManager:
    """SQLite-based cache manager."""

    def __init__(self, db_path: str = "./cache.db", default_ttl: int = 1800) -> None:
        self.default_ttl = default_ttl
        self.db_path = db_path
        self._init_db()

    def _init_db(self) -> None:
        """Initialize SQLite database and create cache table."""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS cache (
                    key TEXT PRIMARY KEY,
                    value TEXT NOT NULL,
                    expires_at REAL NOT NULL
                )
            """)
            conn.execute("CREATE INDEX IF NOT EXISTS idx_expires_at ON cache(expires_at)")
            conn.commit()

    async def get(self, key: str) -> Optional[dict[str, Any]]:
        """Get value from cache."""
        CACHE_MISSES.inc()  # Will be corrected if hit

        def _get():
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.execute(
                    "SELECT value, expires_at FROM cache WHERE key = ?",
                    (key,)
                )
                row = cursor.fetchone()
                if row:
                    value_str, expires_at = row
                    if expires_at > time.time():
                        CACHE_HITS.inc()
                        CACHE_MISSES.dec()
                        return json.loads(value_str)
                    else:
                        # Expired, clean up
                        conn.execute("DELETE FROM cache WHERE key = ?", (key,))
                        conn.commit()
                return None

        return await asyncio.get_event_loop().run_in_executor(None, _get)

    async def set(self, key: str, value: dict[str, Any], ttl: Optional[int] = None) -> bool:
        """Set value in cache with TTL."""
        ttl = ttl or self.default_ttl
        expires_at = time.time() + ttl
        serialized = json.dumps(value)

        def _set():
            with sqlite3.connect(self.db_path) as conn:
                conn.execute(
                    "INSERT OR REPLACE INTO cache (key, value, expires_at) VALUES (?, ?, ?)",
                    (key, serialized, expires_at)
                )
                conn.commit()
                return True

        return await asyncio.get_event_loop().run_in_executor(None, _set)

    async def delete(self, key: str) -> bool:
        """Delete key from cache."""

        def _delete():
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.execute("DELETE FROM cache WHERE key = ?", (key,))
                conn.commit()
                return cursor.rowcount > 0

        return await asyncio.get_event_loop().run_in_executor(None, _delete)

    async def clear_expired(self) -> int:
        """Clear expired entries."""

        def _clear():
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.execute("DELETE FROM cache WHERE expires_at < ?", (time.time(),))
                conn.commit()
                return cursor.rowcount

        return await asyncio.get_event_loop().run_in_executor(None, _clear)


def generate_cache_key(request_payload: dict[str, Any]) -> str:
    """Create deterministic cache key from request."""
    prompt = request_payload.get("prompt", {})
    options = request_payload.get("options", {})
    cacheable = {
        "text": prompt.get("text"),
        "image_url": prompt.get("image_url"),
        "video_url": prompt.get("video_url"),
        "target_category": options.get("target_category"),
        "llm_provider": options.get("llm_provider", "openai"),
    }
    cacheable = {k: v for k, v in cacheable.items() if v is not None}
    normalized = json.dumps(cacheable, sort_keys=True, separators=(",", ":"))
    digest = hashlib.sha256(normalized.encode("utf-8")).hexdigest()
    return f"prompt_parse:v1:{digest}"
</file>

<file path="backend/prompt_parser_service/services/content_safety.py">
"""Prompt content safety checks."""

from __future__ import annotations

import structlog
from openai import AsyncOpenAI

from ..core.config import get_settings

logger = structlog.get_logger(__name__)


class ContentSafetyError(Exception):
    """Raised when prompt violates content policy."""


async def ensure_prompt_safe(prompt_text: str) -> None:
    settings = get_settings()
    if not settings.OPENAI_API_KEY or not prompt_text:
        return

    client = AsyncOpenAI(api_key=settings.OPENAI_API_KEY)
    try:
        response = await client.moderations.create(
            model="omni-moderation-latest",
            input=prompt_text,
        )
    except Exception as exc:  # pragma: no cover
        logger.warning("content_safety.moderation_failed", error=str(exc))
        return

    result = response.results[0]
    if result.flagged:
        raise ContentSafetyError("Prompt violates content policy.")
</file>

<file path="backend/prompt_parser_service/services/cost_estimator.py">
"""Cost estimation fallback."""

from __future__ import annotations

from typing import Any, Dict, List

DEFAULT_VIDEO_SCENE_COST = 0.3
DEFAULT_AUDIO_COST = 0.1


def estimate_cost(scenes: List[dict[str, Any]], include_audio: bool = True) -> Dict[str, Any]:
    total_video = len(scenes) * DEFAULT_VIDEO_SCENE_COST
    total_audio = DEFAULT_AUDIO_COST if include_audio else 0

    return {
        "total_usd": round(total_video + total_audio, 2),
        "breakdown": {
            "video_generation": round(total_video, 2),
            "audio_generation": round(total_audio, 2),
        },
        "assumptions": [
            f"{len(scenes)} scenes at ${DEFAULT_VIDEO_SCENE_COST:.2f} each",
            "Audio placeholder cost added" if include_audio else "Audio cost omitted",
        ],
        "confidence": "low",
    }
</file>

<file path="backend/prompt_parser_service/services/defaults.py">
"""Smart defaults for creative direction."""

from __future__ import annotations

from typing import Any, Dict

PLATFORM_DEFAULTS: Dict[str, Dict[str, Any]] = {
    "instagram": {
        "aspect_ratio": "9:16",
        "duration": 30,
        "fps": 30,
        "pacing": "moderate",
        "cuts_per_minute": 12,
    },
    "tiktok": {
        "aspect_ratio": "9:16",
        "duration": 15,
        "fps": 30,
        "pacing": "fast",
        "cuts_per_minute": 20,
    },
    "youtube": {
        "aspect_ratio": "16:9",
        "duration": 30,
        "fps": 30,
        "pacing": "moderate",
        "cuts_per_minute": 10,
    },
}

CATEGORY_DEFAULTS: Dict[str, Dict[str, Any]] = {
    "luxury": {
        "pacing": "slow",
        "transition_style": "dissolve",
        "lighting_style": "dramatic_soft",
        "music_genre": "classical",
    },
    "tech": {
        "pacing": "dynamic",
        "transition_style": "cut",
        "lighting_style": "clean_studio",
        "music_genre": "electronic",
    },
    "fitness": {
        "pacing": "fast",
        "transition_style": "cut",
        "lighting_style": "high_contrast",
        "music_genre": "edm",
    },
}


def detect_category(parsed_prompt: dict) -> str | None:
    product = (parsed_prompt.get("product") or "").lower()
    keywords = parsed_prompt.get("aesthetic_keywords", [])
    if "luxury" in keywords or "luxury" in product:
        return "luxury"
    if any(k in product for k in ["tech", "app", "software"]):
        return "tech"
    if any(k in product for k in ["fitness", "gym", "athletic"]):
        return "fitness"
    return None


def apply_smart_defaults(parsed_prompt: dict) -> Dict[str, Any]:
    platform = parsed_prompt.get("platform")
    platform_defaults = PLATFORM_DEFAULTS.get(platform or "", {})
    category = detect_category(parsed_prompt)
    category_defaults = CATEGORY_DEFAULTS.get(category or "", {})

    defaults = {
        "technical_specs": {
            "duration": parsed_prompt.get("duration") or platform_defaults.get("duration", 30),
            "aspect_ratio": platform_defaults.get("aspect_ratio", "9:16"),
            "platform": platform or "instagram",
            "fps": platform_defaults.get("fps", 30),
        },
        "pacing": {
            "overall": category_defaults.get("pacing", platform_defaults.get("pacing", "moderate")),
            "cuts_per_minute": platform_defaults.get("cuts_per_minute", 12),
            "transition_style": category_defaults.get("transition_style", "cut"),
        },
        "audio_direction": {
            "music_genre": category_defaults.get("music_genre", "electronic"),
        },
        "metadata": {
            "defaults_used": [],
        },
    }

    for section, values in defaults.items():
        if section == "metadata":
            continue
        for key, value in values.items():
            if parsed_prompt.get(section, {}).get(key) is None:
                defaults["metadata"]["defaults_used"].append(f"{section}.{key}")

    defaults["category"] = category
    return defaults
</file>

<file path="backend/prompt_parser_service/services/edit_handler.py">
"""Iterative editing helper."""

from __future__ import annotations

import copy
from typing import Any, Dict


def merge_iterative_edit(previous_config: Dict[str, Any], new_prompt: str) -> Dict[str, Any]:
    """Stub: merge user instructions into previous config."""
    config = copy.deepcopy(previous_config)
    notes = config.setdefault("metadata", {}).setdefault("iteration_notes", [])
    notes.append(f"Applied edit: {new_prompt}")
    return config
</file>

<file path="backend/prompt_parser_service/services/image_processor.py">
"""Image processing for style extraction."""

from __future__ import annotations

import base64
import io
from typing import Any, Dict, Optional

import httpx
from PIL import Image

from .media_utils import extract_dominant_color, resize_for_analysis


async def _load_image_bytes(image_url: Optional[str], image_base64: Optional[str]) -> bytes:
    if image_base64:
        return base64.b64decode(image_base64)
    if image_url:
        async with httpx.AsyncClient(timeout=10) as client:
            response = await client.get(image_url)
            response.raise_for_status()
            return response.content
    raise ValueError("No image data provided")


async def process_image_primary(
    *,
    image_url: Optional[str] = None,
    image_base64: Optional[str] = None,
    text_context: Optional[str] = None,
) -> Dict[str, Any]:
    image_bytes = await _load_image_bytes(image_url, image_base64)
    image = Image.open(io.BytesIO(image_bytes))
    image = resize_for_analysis(image)

    dominant = extract_dominant_color(image)
    width, height = image.size
    mode = "RGB" if image.mode == "RGB" else image.mode

    analysis = {
        "dominant_colors": [dominant],
        "dimensions": {"width": width, "height": height},
        "mode": mode,
        "text_context": text_context,
    }

    return {
        "source": "image_url" if image_url else "image_base64",
        "reference": image_url or "inline_base64_image",
        "analysis": analysis,
    }
</file>

<file path="backend/prompt_parser_service/services/input_orchestrator.py">
"""Determine primary input modality."""

from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, Optional

from ..models.request import PromptInput
from .image_processor import process_image_primary

# Video processing is optional (requires cv2)
try:
    from .video_processor import process_video_input
    VIDEO_PROCESSING_AVAILABLE = True
except ImportError:
    VIDEO_PROCESSING_AVAILABLE = False
    process_video_input = None  # type: ignore


@dataclass
class InputAnalysis:
    style_source: str
    reference_summary: Dict[str, Any]
    extracted_references: Dict[str, Any]


async def analyze_inputs(prompt: PromptInput) -> Optional[InputAnalysis]:
    if (prompt.video_url or prompt.video_base64) and VIDEO_PROCESSING_AVAILABLE:
        try:
            video_data = await process_video_input(
                video_url=prompt.video_url,
                video_base64=prompt.video_base64,
            )
            summary = {
                "primary_reference": video_data["reference"],
                "frames": video_data["frames"],
            }
            return InputAnalysis(
                style_source="video",
                reference_summary=summary,
                extracted_references={"videos": [video_data]},
            )
        except Exception:
            pass

    if prompt.image_url or prompt.image_base64:
        try:
            image_data = await process_image_primary(
                image_url=prompt.image_url,
                image_base64=prompt.image_base64,
                text_context=prompt.text,
            )
            summary = {
                "primary_reference": image_data["reference"],
                "analysis": image_data["analysis"],
            }
            return InputAnalysis(
                style_source="image",
                reference_summary=summary,
                extracted_references={"images": [image_data]},
            )
        except Exception:
            pass

    return None
</file>

<file path="backend/prompt_parser_service/services/media_utils.py">
"""Common media helpers."""

from __future__ import annotations

import io
from typing import Tuple

from PIL import Image, ImageStat


def extract_dominant_color(image: Image.Image) -> str:
    image = image.convert("RGB")
    stat = ImageStat.Stat(image)
    r, g, b = stat.mean
    return f"#{int(r):02x}{int(g):02x}{int(b):02x}"


def resize_for_analysis(image: Image.Image, max_size: int = 1024) -> Image.Image:
    if max(image.size) > max_size:
        image = image.copy()
        image.thumbnail((max_size, max_size))
    return image


def load_image_from_bytes(data: bytes) -> Image.Image:
    return Image.open(io.BytesIO(data))
</file>

<file path="backend/prompt_parser_service/services/scene_generator.py">
"""Scene generator for creative direction."""

from __future__ import annotations

from typing import Any, List


def generate_scenes(creative_direction: dict[str, Any]) -> List[dict[str, Any]]:
    specs = creative_direction.get("technical_specs", {})
    total_duration = specs.get("duration", 30)
    scene_count = max(3, min(8, int(total_duration // 5) or 3))
    duration_per_scene = total_duration / scene_count

    scenes: List[dict[str, Any]] = []
    for idx in range(scene_count):
        scenes.append(
            {
                "id": f"scene_{idx + 1}",
                "scene_number": idx + 1,
                "purpose": _purpose_for_index(idx, scene_count),
                "duration": round(duration_per_scene, 2),
                "visual": {
                    "shot_type": "close_up" if idx == 0 else "medium",
                    "subject": "product",
                    "generation_prompt": f"Scene {idx + 1} for {creative_direction.get('product', {}).get('name', 'product')}",
                },
            }
        )
    return scenes


def _purpose_for_index(index: int, total: int) -> str:
    if index == 0:
        return "hook"
    if index == total - 1:
        return "cta"
    if index == 1:
        return "context"
    return "product_showcase"
</file>

<file path="backend/prompt_parser_service/services/validator.py">
"""Validation and confidence scoring."""

from __future__ import annotations

from typing import Any, Dict, List


def validate_scenes(creative_direction: dict[str, Any], scenes: List[dict[str, Any]]) -> List[str]:
    warnings: List[str] = []
    target_duration = creative_direction.get("technical_specs", {}).get("duration", 30)
    total_duration = sum(scene.get("duration", 0) for scene in scenes)
    if abs(total_duration - target_duration) > 2:
        warnings.append("Scene timing mismatch vs technical specs duration.")

    for scene in scenes:
        if scene.get("duration", 0) < 2 and scene.get("purpose") == "cta":
            warnings.append(f"CTA scene {scene['scene_number']} might be too short.")
    return warnings


def calculate_confidence(parsed_prompt: dict[str, Any], scenes: List[dict[str, Any]], warnings: List[str]) -> Dict[str, float]:
    product_confidence = 0.7 if parsed_prompt.get("product") else 0.4
    style_confidence = 0.9 if parsed_prompt.get("aesthetic_keywords") else 0.6
    feasibility = max(0.5, 1 - len(warnings) * 0.1)
    overall = round((product_confidence * 0.3) + (style_confidence * 0.4) + (feasibility * 0.3), 2)
    return {
        "confidence_score": overall,
        "confidence_breakdown": {
            "product_understanding": round(product_confidence, 2),
            "style_clarity": round(style_confidence, 2),
            "technical_feasibility": round(feasibility, 2),
        },
    }
</file>

<file path="backend/prompt_parser_service/services/video_processor.py">
"""Video frame extraction for style guidance."""

from __future__ import annotations

import base64
import io
from typing import Any, Dict, Optional
import tempfile
import os

import cv2
import httpx

from .media_utils import extract_dominant_color
from PIL import Image


async def _load_video_bytes(video_url: Optional[str], video_base64: Optional[str]) -> bytes:
    if video_base64:
        return base64.b64decode(video_base64)
    if video_url:
        async with httpx.AsyncClient(timeout=10) as client:
            response = await client.get(video_url)
            response.raise_for_status()
            return response.content
    raise ValueError("No video data provided")


def _frame_to_image(frame) -> Image.Image:
    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    return Image.fromarray(rgb)


async def process_video_input(
    *,
    video_url: Optional[str] = None,
    video_base64: Optional[str] = None,
) -> Dict[str, Any]:
    video_bytes = await _load_video_bytes(video_url, video_base64)
    tmp_path = None
    video = None
    try:
        with tempfile.NamedTemporaryFile(suffix=".mp4", delete=False) as tmp:
            tmp.write(video_bytes)
            tmp.flush()
            tmp_path = tmp.name
        video = cv2.VideoCapture(tmp_path)
        if not video.isOpened():
            raise ValueError("Unable to read video data")

        total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))
        frames_to_extract = [0, max(total_frames - 1, 0)]
        extracted = []

        for idx, frame_index in enumerate(frames_to_extract):
            video.set(cv2.CAP_PROP_POS_FRAMES, frame_index)
            ret, frame = video.read()
            if not ret:
                continue
            image = _frame_to_image(frame)
            dominant = extract_dominant_color(image)
            extracted.append(
                {
                    "source": "video_frame",
                    "frame_type": "first" if idx == 0 else "last",
                    "analysis": {
                        "dominant_color": dominant,
                    },
                }
            )
    finally:
        if video is not None:
            video.release()
        if tmp_path and os.path.exists(tmp_path):
            os.remove(tmp_path)

    return {
        "source": "video_url" if video_url else "video_base64",
        "reference": video_url or "inline_video",
        "frames": extracted,
        "video_metadata": {
            "total_frames": total_frames,
        },
    }
</file>

<file path="backend/prompt_parser_service/main.py">
"""Prompt Parser API entrypoint."""

from contextlib import asynccontextmanager

from fastapi import FastAPI
from slowapi.errors import RateLimitExceeded

from .core.config import Settings, get_settings
from .core.logging import configure_logging
from .core.limiter import limiter
from api.v1 import parse as parse_api
from api.v1 import health as health_api


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Initialize global services."""
    settings = get_settings()
    configure_logging(settings.LOG_LEVEL)
    yield


def create_app() -> FastAPI:
    """Application factory."""
    app = FastAPI(
        title="Prompt Parser API",
        version="0.1.0",
        description="Transforms prompts into structured creative direction",
        lifespan=lifespan,
    )

    from api.v1 import batch as batch_api
    from api.v1 import metrics as metrics_api
    from api.v1 import providers as providers_api
    from api.v1 import cache_admin as cache_admin_api

    from fastapi.responses import JSONResponse

    @app.exception_handler(RateLimitExceeded)
    async def rate_limit_handler(request, exc):
        return JSONResponse({"detail": "Too many requests"}, status_code=429)

    app.state.limiter = limiter

    app.include_router(parse_api.router, prefix="/v1", tags=["parse"])
    app.include_router(batch_api.router, prefix="/v1", tags=["batch"])
    app.include_router(metrics_api.router, tags=["metrics"])
    app.include_router(providers_api.router, prefix="/v1", tags=["providers"])
    app.include_router(cache_admin_api.router, prefix="/v1", tags=["cache"])
    app.include_router(health_api.router, prefix="/v1", tags=["health"])

    return app


app = create_app()
</file>

<file path="backend/schemas/__init__.py">
"""
Pydantic schemas for API request/response validation
"""

from .assets import (
    Asset,
    AssetType,
    AudioAsset,
    AudioAssetTag,
    AudioFormat,
    BaseAsset,
    DocumentAsset,
    DocumentFormat,
    ImageAsset,
    ImageFormat,
    VideoAsset,
    VideoFormat,
    VisualAssetTag,
    AssetTag,
    AssetWithMetadata,
    UploadAssetInput,
)

__all__ = [
    "Asset",
    "AssetType",
    "AudioAsset",
    "AudioAssetTag",
    "AudioFormat",
    "BaseAsset",
    "DocumentAsset",
    "DocumentFormat",
    "ImageAsset",
    "ImageFormat",
    "VideoAsset",
    "VideoFormat",
    "VisualAssetTag",
    "AssetTag",
    "AssetWithMetadata",
    "UploadAssetInput",
]
</file>

<file path="backend/services/__init__.py">
"""
Backend services package for Replicate API integration and other services.
"""

from .replicate_client import ReplicateClient
from .storyboard_generator import generate_storyboard_task, parse_prompt_to_scenes
from .video_renderer import render_video_task

__all__ = ['ReplicateClient', 'generate_storyboard_task', 'parse_prompt_to_scenes', 'render_video_task']
</file>

<file path="backend/services/storyboard_generator.py">
"""
Storyboard Generation Background Task.

This module handles the background task for generating a storyboard (scene breakdown
and images) from a user's video prompt. It orchestrates:
1. Prompt parsing into scenes
2. Image generation for each scene
3. Progress tracking
4. Error handling and retries
"""

import logging
import time
import json
from typing import List, Optional, Dict, Any
from datetime import datetime

from ..models.video_generation import Scene, StoryboardEntry, VideoStatus, VideoProgress
from ..services.replicate_client import ReplicateClient
from ..database import (
    get_job,
    update_job_progress,
    mark_job_failed,
    increment_retry_count,
    get_db
)

# Configure logging
logger = logging.getLogger(__name__)

# Configuration constants
MAX_RETRIES = 3
PARSING_TIMEOUT = 30  # seconds
IMAGE_GENERATION_TIMEOUT = 120  # seconds per image
DEFAULT_SCENE_DURATION = 5.0  # seconds per scene


def generate_storyboard_task(job_id: int) -> None:
    """
    Main background task to generate storyboard from video prompt.

    This function orchestrates the entire storyboard generation workflow:
    1. Fetches job from database
    2. Updates status to 'parsing'
    3. Parses prompt into scenes
    4. Generates images for each scene
    5. Updates progress after each image
    6. Stores storyboard data in database
    7. Updates status to 'storyboard_ready'

    Args:
        job_id: The video generation job ID

    Error Handling:
        - Marks job as failed on critical errors
        - Implements retry logic for transient failures
        - Logs all errors for debugging
    """
    logger.info(f"Starting storyboard generation for job {job_id}")

    try:
        # 1. Fetch job from database
        job = get_job(job_id)
        if not job:
            logger.error(f"Job {job_id} not found")
            return

        prompt = job.get("prompt", "")
        parameters = job.get("parameters", {})
        duration = parameters.get("duration", 30)
        style = parameters.get("style")
        aspect_ratio = parameters.get("aspect_ratio", "16:9")

        logger.info(f"Job {job_id}: prompt='{prompt[:50]}...', duration={duration}s")

        # 2. Update status to 'parsing'
        _update_status(job_id, VideoStatus.PARSING, "Parsing prompt into scenes...")

        # 3. Parse prompt into scenes
        logger.info(f"Job {job_id}: Parsing prompt into scenes")
        start_time = time.time()

        try:
            scenes = parse_prompt_to_scenes(prompt, duration, style)
            parse_duration = time.time() - start_time

            if parse_duration > PARSING_TIMEOUT:
                logger.warning(f"Job {job_id}: Parsing took {parse_duration:.1f}s (timeout: {PARSING_TIMEOUT}s)")

            logger.info(f"Job {job_id}: Parsed into {len(scenes)} scenes")
        except Exception as e:
            logger.error(f"Job {job_id}: Failed to parse prompt: {e}")
            mark_job_failed(job_id, f"Failed to parse prompt: {str(e)}")
            return

        # 4. Update status to 'generating_storyboard'
        _update_status(
            job_id,
            VideoStatus.GENERATING_STORYBOARD,
            f"Generating images for {len(scenes)} scenes..."
        )

        # Initialize storyboard entries
        storyboard: List[StoryboardEntry] = []
        for scene in scenes:
            storyboard.append(StoryboardEntry(
                scene=scene,
                image_url=None,
                generation_status="pending",
                error=None
            ))

        # 5. Generate images for each scene with progress tracking
        image_start_times: List[float] = []
        replicate_client = ReplicateClient()

        for idx, entry in enumerate(storyboard):
            scene_num = idx + 1
            logger.info(f"Job {job_id}: Generating image for scene {scene_num}/{len(scenes)}")

            # Update progress
            _update_progress(
                job_id,
                current_stage=VideoStatus.GENERATING_STORYBOARD,
                scenes_total=len(scenes),
                scenes_completed=idx,
                current_scene=scene_num,
                image_start_times=image_start_times,
                message=f"Generating image for scene {scene_num}/{len(scenes)}"
            )

            # Mark scene as generating
            entry.generation_status = "generating"
            _save_storyboard(job_id, storyboard)

            # Generate image with retry logic
            image_result = _generate_image_with_retry(
                replicate_client,
                entry.scene.image_prompt,
                job_id,
                scene_num,
                max_retries=MAX_RETRIES
            )

            # Track generation time
            if image_result.get("success"):
                if len(image_start_times) > 0:
                    # Calculate actual generation time
                    gen_time = time.time() - max(image_start_times)
                    image_start_times.append(gen_time)
                else:
                    # First image, use default estimate
                    image_start_times.append(IMAGE_GENERATION_TIMEOUT / 2)

            # Update storyboard entry
            if image_result.get("success"):
                entry.image_url = image_result.get("image_url")
                entry.generation_status = "completed"
                entry.error = None
                logger.info(f"Job {job_id}: Scene {scene_num} completed - {entry.image_url}")
            else:
                entry.generation_status = "failed"
                entry.error = image_result.get("error", "Unknown error")[:500]  # Truncate to 500 chars
                logger.error(f"Job {job_id}: Scene {scene_num} failed - {entry.error}")

            # Save updated storyboard after each image
            _save_storyboard(job_id, storyboard)

        # 6. Check if all images were generated successfully
        failed_scenes = [e for e in storyboard if e.generation_status == "failed"]
        completed_scenes = [e for e in storyboard if e.generation_status == "completed"]

        logger.info(
            f"Job {job_id}: Storyboard generation complete - "
            f"{len(completed_scenes)} successful, {len(failed_scenes)} failed"
        )

        # 7. Update final status
        if len(failed_scenes) == len(scenes):
            # All scenes failed
            mark_job_failed(job_id, "All scene images failed to generate")
        elif len(failed_scenes) > 0:
            # Partial failure - still mark as storyboard_ready but with warnings
            _update_status(
                job_id,
                VideoStatus.STORYBOARD_READY,
                f"Storyboard ready ({len(failed_scenes)} scenes failed)"
            )
        else:
            # Complete success
            _update_status(
                job_id,
                VideoStatus.STORYBOARD_READY,
                "Storyboard complete, awaiting approval"
            )

        logger.info(f"Job {job_id}: Storyboard generation task completed")

    except Exception as e:
        logger.exception(f"Job {job_id}: Unexpected error in storyboard generation")
        mark_job_failed(job_id, f"Unexpected error: {str(e)}")


def parse_prompt_to_scenes(
    prompt: str,
    duration: int,
    style: Optional[str] = None
) -> List[Scene]:
    """
    Parse a video prompt into a list of scenes.

    This function breaks down the video prompt into individual scenes with
    descriptions and image generation prompts. It uses a simple rule-based
    approach that can be enhanced with LLM-based parsing later.

    Args:
        prompt: The user's video concept description
        duration: Total video duration in seconds
        style: Optional visual style (e.g., 'cinematic', 'cartoon')

    Returns:
        List of Scene objects with scene_number, description, duration, and image_prompt

    Algorithm:
        1. Determine number of scenes based on duration (1 scene per 5 seconds)
        2. Distribute duration evenly across scenes
        3. Generate scene descriptions and image prompts
        4. Apply style modifiers if specified
    """
    # Determine number of scenes (1 scene per 5 seconds, min 3, max 10)
    num_scenes = max(3, min(10, int(duration / 5)))
    scene_duration = duration / num_scenes

    logger.info(f"Parsing prompt into {num_scenes} scenes ({scene_duration:.1f}s each)")

    scenes: List[Scene] = []

    # Simple rule-based scene generation
    # This can be enhanced with LLM-based parsing in the future
    for i in range(num_scenes):
        scene_num = i + 1

        # Determine scene purpose based on position
        if scene_num == 1:
            purpose = "Opening/Hook"
            scene_desc = f"Opening scene: {prompt[:100]}"
            image_prompt = f"Opening establishing shot, {prompt[:150]}"
        elif scene_num == num_scenes:
            purpose = "Closing/CTA"
            scene_desc = f"Closing scene with call to action"
            image_prompt = f"Closing shot, final moment, {prompt[:150]}"
        elif scene_num == 2:
            purpose = "Context/Setup"
            scene_desc = f"Setting up context for: {prompt[:100]}"
            image_prompt = f"Context establishing shot, {prompt[:150]}"
        else:
            purpose = f"Scene {scene_num}"
            scene_desc = f"Scene {scene_num}: {prompt[:100]}"
            image_prompt = f"Scene {scene_num}, {prompt[:150]}"

        # Apply style modifiers to image prompt
        if style:
            image_prompt = f"{image_prompt}, {style} style"

        # Add cinematic quality descriptors
        image_prompt = f"{image_prompt}, high quality, professional cinematography"

        scenes.append(Scene(
            scene_number=scene_num,
            description=scene_desc,
            duration=round(scene_duration, 2),
            image_prompt=image_prompt[:2000]  # Truncate to max length
        ))

    return scenes


def _generate_image_with_retry(
    client: ReplicateClient,
    prompt: str,
    job_id: int,
    scene_num: int,
    max_retries: int = MAX_RETRIES
) -> Dict[str, Any]:
    """
    Generate an image with retry logic for transient failures.

    Args:
        client: ReplicateClient instance
        prompt: Image generation prompt
        job_id: Job ID for logging
        scene_num: Scene number for logging
        max_retries: Maximum number of retry attempts

    Returns:
        Dict with 'success', 'image_url', and 'error' keys
    """
    for attempt in range(max_retries):
        try:
            logger.info(f"Job {job_id}, Scene {scene_num}: Image generation attempt {attempt + 1}/{max_retries}")

            result = client.generate_image(prompt)

            if result.get("success"):
                logger.info(f"Job {job_id}, Scene {scene_num}: Image generated successfully")
                return result
            else:
                error = result.get("error", "Unknown error")
                logger.warning(f"Job {job_id}, Scene {scene_num}: Attempt {attempt + 1} failed - {error}")

                # Check if we should retry
                if attempt < max_retries - 1:
                    # Exponential backoff: 2s, 4s, 8s
                    backoff_delay = 2 ** (attempt + 1)
                    logger.info(f"Job {job_id}, Scene {scene_num}: Retrying in {backoff_delay}s...")
                    time.sleep(backoff_delay)
                else:
                    # All retries exhausted
                    logger.error(f"Job {job_id}, Scene {scene_num}: All retries exhausted")
                    return result

        except Exception as e:
            logger.error(f"Job {job_id}, Scene {scene_num}: Attempt {attempt + 1} exception - {e}")

            if attempt < max_retries - 1:
                backoff_delay = 2 ** (attempt + 1)
                logger.info(f"Job {job_id}, Scene {scene_num}: Retrying after exception in {backoff_delay}s...")
                time.sleep(backoff_delay)
            else:
                return {
                    "success": False,
                    "image_url": None,
                    "error": f"Exception after {max_retries} retries: {str(e)}"
                }

    # Should not reach here, but just in case
    return {
        "success": False,
        "image_url": None,
        "error": "Max retries exceeded"
    }


def _update_status(job_id: int, status: VideoStatus, message: str) -> None:
    """
    Update job status and progress message.

    Args:
        job_id: Job ID
        status: New VideoStatus
        message: Progress message
    """
    try:
        with get_db() as conn:
            conn.execute(
                "UPDATE generated_videos SET status = ? WHERE id = ?",
                (status.value, job_id)
            )
            conn.commit()

        # Update progress
        update_job_progress(job_id, {
            "current_stage": status.value,
            "message": message
        })

        logger.info(f"Job {job_id}: Status updated to {status.value}")
    except Exception as e:
        logger.error(f"Job {job_id}: Failed to update status - {e}")


def _update_progress(
    job_id: int,
    current_stage: VideoStatus,
    scenes_total: int,
    scenes_completed: int,
    current_scene: Optional[int] = None,
    image_start_times: Optional[List[float]] = None,
    message: Optional[str] = None
) -> None:
    """
    Update job progress with detailed tracking information.

    Args:
        job_id: Job ID
        current_stage: Current VideoStatus
        scenes_total: Total number of scenes
        scenes_completed: Number of completed scenes
        current_scene: Currently processing scene number
        image_start_times: List of image generation times for ETA estimation
        message: Optional progress message
    """
    # Estimate completion time
    estimated_seconds = None
    if image_start_times and len(image_start_times) > 0:
        # Calculate average generation time
        avg_time = sum(image_start_times) / len(image_start_times)
        remaining_scenes = scenes_total - scenes_completed
        estimated_seconds = int(avg_time * remaining_scenes)

    progress = VideoProgress(
        current_stage=current_stage,
        scenes_total=scenes_total,
        scenes_completed=scenes_completed,
        current_scene=current_scene,
        estimated_completion_seconds=estimated_seconds,
        message=message
    )

    try:
        update_job_progress(job_id, progress.model_dump())
    except Exception as e:
        logger.error(f"Job {job_id}: Failed to update progress - {e}")


def _save_storyboard(job_id: int, storyboard: List[StoryboardEntry]) -> None:
    """
    Save storyboard data to database as JSON.

    Args:
        job_id: Job ID
        storyboard: List of StoryboardEntry objects
    """
    try:
        # Convert storyboard to JSON-serializable format
        storyboard_data = [entry.model_dump() for entry in storyboard]

        with get_db() as conn:
            conn.execute(
                "UPDATE generated_videos SET storyboard_data = ? WHERE id = ?",
                (json.dumps(storyboard_data), job_id)
            )
            conn.commit()

        logger.debug(f"Job {job_id}: Storyboard data saved")
    except Exception as e:
        logger.error(f"Job {job_id}: Failed to save storyboard - {e}")
</file>

<file path="backend/services/test_replicate_client.py">
"""
Test file for ReplicateClient - demonstrates usage and basic validation.

Note: These are example tests. In a production environment, you would use
pytest with mocking to avoid hitting the actual Replicate API during tests.
"""

import logging
import sys
from pathlib import Path

# Add parent directory to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent))

from services.replicate_client import ReplicateClient

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)


def test_initialization():
    """Test client initialization."""
    print("\n=== Test 1: Client Initialization ===")

    # Test with environment variable
    try:
        client = ReplicateClient()
        print("✓ Client initialized successfully from environment variable")
        return True
    except ValueError as e:
        print(f"✗ Client initialization failed: {e}")
        print("  Make sure REPLICATE_API_KEY is set in your environment")
        return False


def test_cost_estimation():
    """Test cost estimation."""
    print("\n=== Test 2: Cost Estimation ===")

    try:
        client = ReplicateClient(api_key="dummy_key_for_testing")

        # Test case 1: 10 images, 20 second video
        cost1 = client.estimate_cost(num_images=10, video_duration=20)
        expected1 = 10 * 0.003 + 20 * 0.10  # $0.03 + $2.00 = $2.03
        assert abs(cost1 - expected1) < 0.001, f"Expected {expected1}, got {cost1}"
        print(f"✓ Cost for 10 images + 20s video: ${cost1:.2f}")

        # Test case 2: 5 images, 10 second video
        cost2 = client.estimate_cost(num_images=5, video_duration=10)
        expected2 = 5 * 0.003 + 10 * 0.10  # $0.015 + $1.00 = $1.015
        assert abs(cost2 - expected2) < 0.001, f"Expected {expected2}, got {cost2}"
        print(f"✓ Cost for 5 images + 10s video: ${cost2:.2f}")

        # Test case 3: No images, 30 second video
        cost3 = client.estimate_cost(num_images=0, video_duration=30)
        expected3 = 0 * 0.003 + 30 * 0.10  # $0.00 + $3.00 = $3.00
        assert abs(cost3 - expected3) < 0.001, f"Expected {expected3}, got {cost3}"
        print(f"✓ Cost for 0 images + 30s video: ${cost3:.2f}")

        print("✓ All cost estimation tests passed")
        return True

    except Exception as e:
        print(f"✗ Cost estimation test failed: {e}")
        return False


def test_error_handling():
    """Test error handling for various scenarios."""
    print("\n=== Test 3: Error Handling ===")

    try:
        # Test missing API key
        try:
            import os
            old_key = os.environ.get('REPLICATE_API_KEY')
            if 'REPLICATE_API_KEY' in os.environ:
                del os.environ['REPLICATE_API_KEY']

            client = ReplicateClient()
            print("✗ Should have raised ValueError for missing API key")
            return False
        except ValueError as e:
            print("✓ Correctly raises ValueError for missing API key")

            # Restore old key
            if old_key:
                os.environ['REPLICATE_API_KEY'] = old_key

        # Test empty image URLs for video generation
        client = ReplicateClient(api_key="dummy_key_for_testing")
        result = client.generate_video([])
        assert result['success'] is False, "Should fail with empty image URLs"
        assert "No image URLs provided" in result['error']
        print("✓ Correctly handles empty image URLs")

        print("✓ All error handling tests passed")
        return True

    except Exception as e:
        print(f"✗ Error handling test failed: {e}")
        return False


def test_context_manager():
    """Test context manager support."""
    print("\n=== Test 4: Context Manager ===")

    try:
        with ReplicateClient(api_key="dummy_key_for_testing") as client:
            assert client is not None
            print("✓ Context manager entry works")

        print("✓ Context manager exit works")
        return True

    except Exception as e:
        print(f"✗ Context manager test failed: {e}")
        return False


def demonstrate_usage():
    """Demonstrate typical usage patterns."""
    print("\n=== Usage Examples ===")

    # Example 1: Basic initialization
    print("\nExample 1: Initialize client")
    print("```python")
    print("from services.replicate_client import ReplicateClient")
    print("")
    print("# Initialize with environment variable")
    print("client = ReplicateClient()")
    print("")
    print("# Or with explicit API key")
    print("client = ReplicateClient(api_key='your-api-key-here')")
    print("```")

    # Example 2: Generate image
    print("\nExample 2: Generate an image")
    print("```python")
    print("result = client.generate_image('a red sports car in a futuristic city')")
    print("if result['success']:")
    print("    print(f\"Image URL: {result['image_url']}\")")
    print("    print(f\"Prediction ID: {result['prediction_id']}\")")
    print("else:")
    print("    print(f\"Error: {result['error']}\")")
    print("```")

    # Example 3: Generate video
    print("\nExample 3: Generate a video from images")
    print("```python")
    print("image_urls = [")
    print("    'https://example.com/frame1.jpg',")
    print("    'https://example.com/frame2.jpg',")
    print("    'https://example.com/frame3.jpg'")
    print("]")
    print("")
    print("result = client.generate_video(image_urls)")
    print("if result['success']:")
    print("    print(f\"Video URL: {result['video_url']}\")")
    print("    print(f\"Duration: {result['duration_seconds']}s\")")
    print("else:")
    print("    print(f\"Error: {result['error']}\")")
    print("```")

    # Example 4: Cost estimation
    print("\nExample 4: Estimate costs")
    print("```python")
    print("# Estimate cost for 10 images and a 30-second video")
    print("cost = client.estimate_cost(num_images=10, video_duration=30)")
    print("print(f\"Estimated cost: ${cost:.2f}\")")
    print("# Output: Estimated cost: $3.03")
    print("```")

    # Example 5: Using context manager
    print("\nExample 5: Use context manager for automatic cleanup")
    print("```python")
    print("with ReplicateClient() as client:")
    print("    result = client.generate_image('a beautiful sunset')")
    print("    # Session automatically closed when exiting context")
    print("```")


def main():
    """Run all tests."""
    print("=" * 70)
    print("ReplicateClient Test Suite")
    print("=" * 70)

    results = []

    # Run tests
    results.append(("Initialization", test_initialization()))
    results.append(("Cost Estimation", test_cost_estimation()))
    results.append(("Error Handling", test_error_handling()))
    results.append(("Context Manager", test_context_manager()))

    # Show usage examples
    demonstrate_usage()

    # Print summary
    print("\n" + "=" * 70)
    print("Test Summary")
    print("=" * 70)

    passed = sum(1 for _, result in results if result)
    total = len(results)

    for test_name, result in results:
        status = "PASS" if result else "FAIL"
        symbol = "✓" if result else "✗"
        print(f"{symbol} {test_name}: {status}")

    print(f"\nTotal: {passed}/{total} tests passed")

    if passed == total:
        print("\n🎉 All tests passed!")
        return 0
    else:
        print(f"\n⚠️  {total - passed} test(s) failed")
        return 1


if __name__ == "__main__":
    sys.exit(main())
</file>

<file path="backend/services/test_storyboard_generator.py">
"""
Unit tests for storyboard_generator module.

Tests the background task for generating storyboards from video prompts,
including prompt parsing, image generation, progress tracking, and error handling.
"""

import unittest
import json
import time
from unittest.mock import Mock, patch, MagicMock
from pathlib import Path
import sys

# Add parent directory to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from backend.services.storyboard_generator import (
    generate_storyboard_task,
    parse_prompt_to_scenes,
    _generate_image_with_retry,
    _update_status,
    _update_progress,
    _save_storyboard
)
from backend.models.video_generation import Scene, StoryboardEntry, VideoStatus, VideoProgress


class TestParsePromptToScenes(unittest.TestCase):
    """Test the parse_prompt_to_scenes function."""

    def test_basic_parsing(self):
        """Test basic prompt parsing with default parameters."""
        prompt = "A robot exploring Mars with dramatic red landscapes"
        duration = 30

        scenes = parse_prompt_to_scenes(prompt, duration)

        # Should generate appropriate number of scenes
        self.assertGreaterEqual(len(scenes), 3)
        self.assertLessEqual(len(scenes), 10)

        # Each scene should be valid
        for scene in scenes:
            self.assertIsInstance(scene, Scene)
            self.assertGreater(scene.scene_number, 0)
            self.assertGreater(scene.duration, 0)
            self.assertIsNotNone(scene.description)
            self.assertIsNotNone(scene.image_prompt)

    def test_duration_distribution(self):
        """Test that scene durations sum to total duration."""
        prompt = "Test video"
        duration = 30

        scenes = parse_prompt_to_scenes(prompt, duration)

        total_scene_duration = sum(s.duration for s in scenes)

        # Allow small rounding error
        self.assertAlmostEqual(total_scene_duration, duration, delta=0.1)

    def test_short_duration(self):
        """Test minimum scene count for short videos."""
        prompt = "Quick video"
        duration = 5

        scenes = parse_prompt_to_scenes(prompt, duration)

        # Should generate at least 3 scenes
        self.assertGreaterEqual(len(scenes), 3)

    def test_long_duration(self):
        """Test maximum scene count for long videos."""
        prompt = "Long video"
        duration = 120

        scenes = parse_prompt_to_scenes(prompt, duration)

        # Should cap at 10 scenes
        self.assertLessEqual(len(scenes), 10)

    def test_style_modifier(self):
        """Test that style is included in image prompts."""
        prompt = "A sunset over mountains"
        duration = 30
        style = "cinematic"

        scenes = parse_prompt_to_scenes(prompt, duration, style)

        # Style should appear in image prompts
        for scene in scenes:
            self.assertIn(style, scene.image_prompt.lower())

    def test_scene_numbering(self):
        """Test that scenes are numbered sequentially."""
        prompt = "Test video"
        duration = 30

        scenes = parse_prompt_to_scenes(prompt, duration)

        for idx, scene in enumerate(scenes):
            self.assertEqual(scene.scene_number, idx + 1)


class TestGenerateImageWithRetry(unittest.TestCase):
    """Test the _generate_image_with_retry function."""

    @patch('backend.services.storyboard_generator.time.sleep')
    def test_success_on_first_attempt(self, mock_sleep):
        """Test successful image generation on first try."""
        mock_client = Mock()
        mock_client.generate_image.return_value = {
            "success": True,
            "image_url": "https://example.com/image.jpg",
            "error": None
        }

        result = _generate_image_with_retry(
            mock_client,
            "test prompt",
            job_id=1,
            scene_num=1,
            max_retries=3
        )

        self.assertTrue(result["success"])
        self.assertEqual(result["image_url"], "https://example.com/image.jpg")
        self.assertEqual(mock_client.generate_image.call_count, 1)
        mock_sleep.assert_not_called()

    @patch('backend.services.storyboard_generator.time.sleep')
    def test_success_after_retries(self, mock_sleep):
        """Test successful image generation after retries."""
        mock_client = Mock()
        mock_client.generate_image.side_effect = [
            {"success": False, "error": "Temporary error"},
            {"success": False, "error": "Temporary error"},
            {"success": True, "image_url": "https://example.com/image.jpg", "error": None}
        ]

        result = _generate_image_with_retry(
            mock_client,
            "test prompt",
            job_id=1,
            scene_num=1,
            max_retries=3
        )

        self.assertTrue(result["success"])
        self.assertEqual(mock_client.generate_image.call_count, 3)
        self.assertEqual(mock_sleep.call_count, 2)  # Slept between retries

    @patch('backend.services.storyboard_generator.time.sleep')
    def test_failure_after_max_retries(self, mock_sleep):
        """Test failure after exhausting all retries."""
        mock_client = Mock()
        mock_client.generate_image.return_value = {
            "success": False,
            "error": "Persistent error"
        }

        result = _generate_image_with_retry(
            mock_client,
            "test prompt",
            job_id=1,
            scene_num=1,
            max_retries=3
        )

        self.assertFalse(result["success"])
        self.assertEqual(mock_client.generate_image.call_count, 3)
        self.assertEqual(mock_sleep.call_count, 2)

    @patch('backend.services.storyboard_generator.time.sleep')
    def test_exponential_backoff(self, mock_sleep):
        """Test exponential backoff delays between retries."""
        mock_client = Mock()
        mock_client.generate_image.return_value = {
            "success": False,
            "error": "Temporary error"
        }

        _generate_image_with_retry(
            mock_client,
            "test prompt",
            job_id=1,
            scene_num=1,
            max_retries=3
        )

        # Check backoff delays: 2s, 4s
        calls = mock_sleep.call_args_list
        self.assertEqual(calls[0][0][0], 2)  # First retry: 2s
        self.assertEqual(calls[1][0][0], 4)  # Second retry: 4s


class TestStoryboardGenerator(unittest.TestCase):
    """Test the main generate_storyboard_task function."""

    @patch('backend.services.storyboard_generator.ReplicateClient')
    @patch('backend.services.storyboard_generator.get_job')
    @patch('backend.services.storyboard_generator.mark_job_failed')
    @patch('backend.services.storyboard_generator._update_status')
    @patch('backend.services.storyboard_generator._save_storyboard')
    @patch('backend.services.storyboard_generator._update_progress')
    def test_successful_storyboard_generation(
        self,
        mock_update_progress,
        mock_save_storyboard,
        mock_update_status,
        mock_mark_failed,
        mock_get_job,
        mock_replicate_client
    ):
        """Test successful end-to-end storyboard generation."""
        # Mock job data
        mock_get_job.return_value = {
            "id": 1,
            "prompt": "A robot exploring Mars",
            "parameters": {"duration": 15, "style": "cinematic"},
            "status": "pending"
        }

        # Mock image generation
        mock_client_instance = Mock()
        mock_client_instance.generate_image.return_value = {
            "success": True,
            "image_url": "https://example.com/image.jpg",
            "error": None
        }
        mock_replicate_client.return_value = mock_client_instance

        # Run task
        generate_storyboard_task(1)

        # Verify status updates
        self.assertGreaterEqual(mock_update_status.call_count, 2)  # At least parsing and storyboard_ready

        # Verify storyboard was saved
        self.assertGreater(mock_save_storyboard.call_count, 0)

        # Verify no failure
        mock_mark_failed.assert_not_called()

    @patch('backend.services.storyboard_generator.get_job')
    @patch('backend.services.storyboard_generator.mark_job_failed')
    def test_job_not_found(self, mock_mark_failed, mock_get_job):
        """Test handling of missing job."""
        mock_get_job.return_value = None

        generate_storyboard_task(999)

        # Should not mark as failed (job doesn't exist)
        mock_mark_failed.assert_not_called()

    @patch('backend.services.storyboard_generator.get_job')
    @patch('backend.services.storyboard_generator.mark_job_failed')
    @patch('backend.services.storyboard_generator.parse_prompt_to_scenes')
    def test_parsing_failure(self, mock_parse, mock_mark_failed, mock_get_job):
        """Test handling of prompt parsing failure."""
        mock_get_job.return_value = {
            "id": 1,
            "prompt": "Test prompt",
            "parameters": {"duration": 30},
            "status": "pending"
        }

        # Simulate parsing error
        mock_parse.side_effect = Exception("Parsing failed")

        generate_storyboard_task(1)

        # Should mark job as failed
        mock_mark_failed.assert_called_once()
        error_msg = mock_mark_failed.call_args[0][1]
        self.assertIn("parse", error_msg.lower())


class TestHelperFunctions(unittest.TestCase):
    """Test helper functions."""

    @patch('backend.services.storyboard_generator.get_db')
    @patch('backend.services.storyboard_generator.update_job_progress')
    def test_update_status(self, mock_update_progress, mock_get_db):
        """Test _update_status function."""
        mock_conn = MagicMock()
        mock_get_db.return_value.__enter__.return_value = mock_conn

        _update_status(1, VideoStatus.PARSING, "Parsing prompt...")

        # Verify database update
        mock_conn.execute.assert_called()
        mock_conn.commit.assert_called()

        # Verify progress update
        mock_update_progress.assert_called_once()

    @patch('backend.services.storyboard_generator.update_job_progress')
    def test_update_progress(self, mock_update_progress):
        """Test _update_progress function."""
        _update_progress(
            job_id=1,
            current_stage=VideoStatus.GENERATING_STORYBOARD,
            scenes_total=5,
            scenes_completed=2,
            current_scene=3,
            message="Generating image..."
        )

        mock_update_progress.assert_called_once()
        progress_data = mock_update_progress.call_args[0][1]

        self.assertEqual(progress_data["scenes_total"], 5)
        self.assertEqual(progress_data["scenes_completed"], 2)
        self.assertEqual(progress_data["current_scene"], 3)

    @patch('backend.services.storyboard_generator.get_db')
    def test_save_storyboard(self, mock_get_db):
        """Test _save_storyboard function."""
        mock_conn = MagicMock()
        mock_get_db.return_value.__enter__.return_value = mock_conn

        # Create sample storyboard
        scene = Scene(
            scene_number=1,
            description="Test scene",
            duration=5.0,
            image_prompt="A test scene"
        )
        entry = StoryboardEntry(
            scene=scene,
            image_url="https://example.com/image.jpg",
            generation_status="completed",
            error=None
        )

        _save_storyboard(1, [entry])

        # Verify database update
        mock_conn.execute.assert_called()
        mock_conn.commit.assert_called()

        # Verify JSON serialization
        call_args = mock_conn.execute.call_args[0]
        storyboard_json = call_args[1][0]
        storyboard_data = json.loads(storyboard_json)

        self.assertEqual(len(storyboard_data), 1)
        self.assertEqual(storyboard_data[0]["scene"]["scene_number"], 1)
        self.assertEqual(storyboard_data[0]["generation_status"], "completed")


def run_tests():
    """Run all tests."""
    # Create test suite
    suite = unittest.TestLoader().loadTestsFromModule(sys.modules[__name__])

    # Run tests
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(suite)

    # Return exit code
    return 0 if result.wasSuccessful() else 1


if __name__ == "__main__":
    sys.exit(run_tests())
</file>

<file path="backend/services/test_video_renderer.py">
"""
Unit tests for video rendering background task.

Tests cover:
- Happy path rendering workflow
- Error handling and retry logic
- Progress tracking
- Cost calculation
- Video download and validation
"""

import pytest
import json
import time
from pathlib import Path
from unittest.mock import Mock, patch, MagicMock, call
from datetime import datetime

from backend.services.video_renderer import (
    render_video_task,
    download_video,
    _render_video_with_retry,
    _calculate_actual_cost,
    _update_status,
    _update_progress,
    EXPONENTIAL_BACKOFF_BASE,
    MAX_RETRIES
)
from backend.models.video_generation import VideoStatus, VideoProgress


# ===== Fixtures =====

@pytest.fixture
def mock_job_approved():
    """Mock job data with approved storyboard."""
    return {
        "id": 123,
        "prompt": "Test video prompt",
        "parameters": {"duration": 30, "style": "cinematic"},
        "approved": True,
        "estimated_cost": 2.5,
        "storyboard_data": json.dumps([
            {
                "scene": {
                    "scene_number": 1,
                    "description": "Opening scene",
                    "duration": 5.0,
                    "image_prompt": "Opening shot"
                },
                "image_url": "https://example.com/image1.jpg",
                "generation_status": "completed",
                "error": None
            },
            {
                "scene": {
                    "scene_number": 2,
                    "description": "Middle scene",
                    "duration": 5.0,
                    "image_prompt": "Middle shot"
                },
                "image_url": "https://example.com/image2.jpg",
                "generation_status": "completed",
                "error": None
            }
        ])
    }


@pytest.fixture
def mock_job_not_approved():
    """Mock job data without storyboard approval."""
    return {
        "id": 456,
        "prompt": "Test video prompt",
        "parameters": {},
        "approved": False,
        "storyboard_data": json.dumps([])
    }


@pytest.fixture
def mock_replicate_client():
    """Mock ReplicateClient for testing."""
    with patch('backend.services.video_renderer.ReplicateClient') as mock:
        client = Mock()
        client.generate_video.return_value = {
            "success": True,
            "video_url": "https://replicate.delivery/test-video.mp4",
            "error": None,
            "prediction_id": "pred123",
            "duration_seconds": 10
        }
        mock.return_value = client
        yield mock


# ===== Test render_video_task =====

def test_render_video_task_success(mock_job_approved, mock_replicate_client):
    """Test successful video rendering workflow."""
    with patch('backend.services.video_renderer.get_job') as mock_get_job, \
         patch('backend.services.video_renderer.download_video') as mock_download, \
         patch('backend.services.video_renderer.get_db') as mock_db, \
         patch('backend.services.video_renderer._update_status') as mock_status, \
         patch('backend.services.video_renderer._update_progress') as mock_progress:

        mock_get_job.return_value = mock_job_approved
        mock_download.return_value = "/api/videos/123/data"

        # Mock database connection
        mock_conn = MagicMock()
        mock_db.return_value.__enter__.return_value = mock_conn

        # Execute
        render_video_task(123)

        # Verify job was fetched
        mock_get_job.assert_called_once_with(123)

        # Verify video was generated
        client = mock_replicate_client.return_value
        client.generate_video.assert_called_once()
        image_urls = client.generate_video.call_args[0][0]
        assert len(image_urls) == 2
        assert image_urls[0] == "https://example.com/image1.jpg"
        assert image_urls[1] == "https://example.com/image2.jpg"

        # Verify video was downloaded
        mock_download.assert_called_once_with(
            "https://replicate.delivery/test-video.mp4",
            123
        )

        # Verify database was updated with video URL and cost
        assert mock_conn.execute.called
        # Find the UPDATE call
        update_calls = [c for c in mock_conn.execute.call_args_list
                       if 'UPDATE generated_videos' in str(c)]
        assert len(update_calls) > 0

        # Verify status updates
        status_calls = mock_status.call_args_list
        assert any(VideoStatus.RENDERING in str(c) for c in status_calls)


def test_render_video_task_not_approved(mock_job_not_approved):
    """Test that rendering fails if storyboard is not approved."""
    with patch('backend.services.video_renderer.get_job') as mock_get_job, \
         patch('backend.services.video_renderer.mark_job_failed') as mock_fail:

        mock_get_job.return_value = mock_job_not_approved

        # Execute
        render_video_task(456)

        # Verify job was marked as failed
        mock_fail.assert_called_once()
        args = mock_fail.call_args[0]
        assert args[0] == 456
        assert "approved" in args[1].lower()


def test_render_video_task_missing_storyboard():
    """Test that rendering fails if storyboard data is missing."""
    job = {
        "id": 789,
        "approved": True,
        "storyboard_data": None
    }

    with patch('backend.services.video_renderer.get_job') as mock_get_job, \
         patch('backend.services.video_renderer.mark_job_failed') as mock_fail:

        mock_get_job.return_value = job

        # Execute
        render_video_task(789)

        # Verify job was marked as failed
        mock_fail.assert_called_once()
        args = mock_fail.call_args[0]
        assert args[0] == 789
        assert "storyboard" in args[1].lower()


def test_render_video_task_missing_image_url(mock_replicate_client):
    """Test that rendering fails if any scene is missing image_url."""
    job = {
        "id": 999,
        "approved": True,
        "estimated_cost": 1.0,
        "storyboard_data": json.dumps([
            {
                "scene": {"scene_number": 1, "description": "Scene 1", "duration": 5.0, "image_prompt": "Test"},
                "image_url": "https://example.com/image1.jpg",
                "generation_status": "completed",
                "error": None
            },
            {
                "scene": {"scene_number": 2, "description": "Scene 2", "duration": 5.0, "image_prompt": "Test"},
                "image_url": None,  # Missing!
                "generation_status": "failed",
                "error": "Generation failed"
            }
        ])
    }

    with patch('backend.services.video_renderer.get_job') as mock_get_job, \
         patch('backend.services.video_renderer.mark_job_failed') as mock_fail:

        mock_get_job.return_value = job

        # Execute
        render_video_task(999)

        # Verify job was marked as failed
        mock_fail.assert_called_once()
        args = mock_fail.call_args[0]
        assert args[0] == 999
        assert "scene 2" in args[1].lower()
        assert "missing" in args[1].lower()


# ===== Test _render_video_with_retry =====

def test_render_video_with_retry_success(mock_replicate_client):
    """Test successful video rendering on first attempt."""
    with patch('backend.services.video_renderer._update_progress'):
        result = _render_video_with_retry(
            123,
            ["https://example.com/img1.jpg", "https://example.com/img2.jpg"],
            max_retries=2
        )

        assert result["success"] is True
        assert result["video_url"] == "https://replicate.delivery/test-video.mp4"
        assert result["duration_seconds"] == 10


def test_render_video_with_retry_eventual_success(mock_replicate_client):
    """Test video rendering succeeds after retries."""
    with patch('backend.services.video_renderer._update_progress'), \
         patch('backend.services.video_renderer.increment_retry_count'), \
         patch('time.sleep'):  # Mock sleep to speed up test

        client = mock_replicate_client.return_value

        # Fail first attempt, succeed on second
        client.generate_video.side_effect = [
            {"success": False, "error": "Temporary failure"},
            {
                "success": True,
                "video_url": "https://replicate.delivery/test-video.mp4",
                "duration_seconds": 10
            }
        ]

        result = _render_video_with_retry(
            123,
            ["https://example.com/img1.jpg"],
            max_retries=2
        )

        assert result["success"] is True
        assert client.generate_video.call_count == 2


def test_render_video_with_retry_max_retries_exceeded(mock_replicate_client):
    """Test video rendering fails after max retries."""
    with patch('backend.services.video_renderer._update_progress'), \
         patch('backend.services.video_renderer.increment_retry_count'), \
         patch('time.sleep'):

        client = mock_replicate_client.return_value

        # Always fail
        client.generate_video.return_value = {
            "success": False,
            "error": "Persistent failure"
        }

        result = _render_video_with_retry(
            123,
            ["https://example.com/img1.jpg"],
            max_retries=2
        )

        assert result["success"] is False
        assert "Persistent failure" in result["error"]
        assert client.generate_video.call_count == 3  # Initial + 2 retries


def test_render_video_with_retry_exponential_backoff(mock_replicate_client):
    """Test that retry logic uses exponential backoff."""
    with patch('backend.services.video_renderer._update_progress'), \
         patch('backend.services.video_renderer.increment_retry_count'), \
         patch('time.sleep') as mock_sleep:

        client = mock_replicate_client.return_value

        # Fail first two attempts
        client.generate_video.side_effect = [
            {"success": False, "error": "Fail 1"},
            {"success": False, "error": "Fail 2"},
            {"success": True, "video_url": "https://test.mp4", "duration_seconds": 5}
        ]

        _render_video_with_retry(123, ["https://img.jpg"], max_retries=2)

        # Verify sleep was called with exponential backoff
        # First retry: 30s, Second retry: 90s (30 * 3^1)
        sleep_calls = [c[0][0] for c in mock_sleep.call_args_list]
        assert 30 in sleep_calls  # First backoff
        assert 90 in sleep_calls  # Second backoff (30 * 3)


# ===== Test download_video =====

def test_download_video_success(tmp_path):
    """Test successful video download and validation."""
    # Create mock video data with MP4 signature
    mock_video_data = b'\x00\x00\x00\x18ftypmp4\x20' + b'\x00' * 1024

    with patch('requests.get') as mock_get, \
         patch('backend.services.video_renderer.Path') as mock_path_class:

        # Mock response
        mock_response = Mock()
        mock_response.iter_content.return_value = [mock_video_data]
        mock_response.raise_for_status = Mock()
        mock_get.return_value = mock_response

        # Mock Path operations
        mock_video_dir = tmp_path / "videos" / "123"
        mock_video_dir.mkdir(parents=True, exist_ok=True)
        mock_video_path = mock_video_dir / "final.mp4"
        mock_temp_path = mock_video_dir / "final.tmp"

        mock_path_instance = Mock()
        mock_path_instance.__truediv__ = lambda self, other: mock_video_dir / other if other != "final.mp4" else mock_video_path
        mock_path_instance.mkdir = Mock()
        mock_path_class.return_value.__truediv__.return_value.__truediv__.return_value.__truediv__.return_value = mock_video_dir

        # Patch open to write to temp file
        with patch('builtins.open', create=True) as mock_open:
            mock_file = MagicMock()
            mock_file.read.return_value = mock_video_data
            mock_open.return_value.__enter__.return_value = mock_file

            with patch.object(Path, 'with_suffix', return_value=mock_temp_path), \
                 patch.object(Path, 'replace'), \
                 patch.object(Path, 'exists', return_value=False):

                result = download_video("https://example.com/video.mp4", 123)

                assert result == "/api/videos/123/data"
                mock_get.assert_called_once()


def test_download_video_empty_file():
    """Test that download fails for empty files."""
    with patch('requests.get') as mock_get, \
         patch('builtins.open', create=True):

        mock_response = Mock()
        mock_response.iter_content.return_value = []  # Empty
        mock_get.return_value = mock_response

        with pytest.raises(ValueError, match="empty"):
            download_video("https://example.com/video.mp4", 123)


def test_download_video_invalid_format():
    """Test that download fails for invalid video format."""
    # Create data with invalid magic bytes
    invalid_data = b'INVALID_VIDEO_DATA' + b'\x00' * 1024

    with patch('requests.get') as mock_get, \
         patch('builtins.open', create=True) as mock_open, \
         patch('backend.services.video_renderer.Path'):

        mock_response = Mock()
        mock_response.iter_content.return_value = [invalid_data]
        mock_response.raise_for_status = Mock()
        mock_get.return_value = mock_response

        mock_file = MagicMock()
        mock_file.read.return_value = invalid_data
        mock_open.return_value.__enter__.return_value = mock_file

        with pytest.raises(ValueError, match="valid video"):
            download_video("https://example.com/video.mp4", 123)


def test_download_video_network_error():
    """Test that download handles network errors."""
    import requests

    with patch('requests.get') as mock_get:
        mock_get.side_effect = requests.exceptions.ConnectionError("Network error")

        with pytest.raises(ValueError, match="Network error"):
            download_video("https://example.com/video.mp4", 123)


# ===== Test _calculate_actual_cost =====

def test_calculate_actual_cost():
    """Test cost calculation."""
    # 10 images * $0.003 = $0.03
    # 20 seconds * $0.10 = $2.00
    # Total = $2.03
    cost = _calculate_actual_cost(num_images=10, video_duration=20)
    assert cost == 2.03


def test_calculate_actual_cost_zero():
    """Test cost calculation with zero values."""
    cost = _calculate_actual_cost(num_images=0, video_duration=0)
    assert cost == 0.0


def test_calculate_actual_cost_rounding():
    """Test that cost is rounded to 2 decimal places."""
    # 3 images * $0.003 = $0.009
    # 7 seconds * $0.10 = $0.70
    # Total = $0.709 -> $0.71
    cost = _calculate_actual_cost(num_images=3, video_duration=7)
    assert cost == 0.71


# ===== Test helper functions =====

def test_update_status():
    """Test status update function."""
    with patch('backend.services.video_renderer.get_db') as mock_db, \
         patch('backend.services.video_renderer.update_job_progress') as mock_progress:

        mock_conn = MagicMock()
        mock_db.return_value.__enter__.return_value = mock_conn

        _update_status(123, VideoStatus.RENDERING, "Test message")

        # Verify database update
        mock_conn.execute.assert_called_once()
        sql = mock_conn.execute.call_args[0][0]
        assert "UPDATE generated_videos" in sql
        assert "status" in sql

        # Verify progress update
        mock_progress.assert_called_once()


def test_update_progress():
    """Test progress update function."""
    with patch('backend.services.video_renderer.update_job_progress') as mock_progress:

        _update_progress(
            123,
            current_stage=VideoStatus.RENDERING,
            message="Rendering in progress"
        )

        mock_progress.assert_called_once()
        progress_data = mock_progress.call_args[0][1]
        assert progress_data["current_stage"] == VideoStatus.RENDERING
        assert progress_data["message"] == "Rendering in progress"


# ===== Integration-style tests =====

def test_full_workflow_integration(mock_job_approved, mock_replicate_client):
    """Test complete workflow from start to finish."""
    with patch('backend.services.video_renderer.get_job') as mock_get_job, \
         patch('backend.services.video_renderer.download_video') as mock_download, \
         patch('backend.services.video_renderer.get_db') as mock_db, \
         patch('backend.services.video_renderer._update_status'), \
         patch('backend.services.video_renderer._update_progress'):

        mock_get_job.return_value = mock_job_approved
        mock_download.return_value = "/api/videos/123/data"
        mock_conn = MagicMock()
        mock_db.return_value.__enter__.return_value = mock_conn

        # Execute
        render_video_task(123)

        # Verify complete workflow
        assert mock_get_job.called
        assert mock_replicate_client.return_value.generate_video.called
        assert mock_download.called
        assert mock_conn.execute.called
        assert mock_conn.commit.called


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
</file>

<file path="backend/services/video_exporter.py">
"""
Video Export Service.

This module handles video export functionality with ffmpeg for format conversion
and quality presets. Supports multiple output formats and quality settings.
"""

import logging
import subprocess
import os
from pathlib import Path
from typing import Optional, Literal, Tuple

logger = logging.getLogger(__name__)

# Quality presets (width x height, bitrate)
QUALITY_PRESETS = {
    "low": {
        "resolution": "854x480",
        "video_bitrate": "1000k",
        "audio_bitrate": "128k"
    },
    "medium": {
        "resolution": "1280x720",
        "video_bitrate": "2500k",
        "audio_bitrate": "192k"
    },
    "high": {
        "resolution": "1920x1080",
        "video_bitrate": "5000k",
        "audio_bitrate": "256k"
    }
}

# Format configurations
FORMAT_CONFIGS = {
    "mp4": {
        "codec": "libx264",
        "audio_codec": "aac",
        "ext": "mp4"
    },
    "mov": {
        "codec": "libx264",
        "audio_codec": "aac",
        "ext": "mov"
    },
    "webm": {
        "codec": "libvpx-vp9",
        "audio_codec": "libopus",
        "ext": "webm"
    }
}


def check_ffmpeg_available() -> bool:
    """
    Check if ffmpeg is available on the system.

    Returns:
        True if ffmpeg is available, False otherwise
    """
    try:
        result = subprocess.run(
            ["ffmpeg", "-version"],
            capture_output=True,
            timeout=5
        )
        return result.returncode == 0
    except (FileNotFoundError, subprocess.TimeoutExpired):
        return False


def get_video_info(video_path: str) -> Optional[dict]:
    """
    Get video information using ffprobe.

    Args:
        video_path: Path to the video file

    Returns:
        Dictionary with video metadata or None if failed
    """
    try:
        result = subprocess.run(
            [
                "ffprobe",
                "-v", "quiet",
                "-print_format", "json",
                "-show_format",
                "-show_streams",
                video_path
            ],
            capture_output=True,
            timeout=10,
            text=True
        )

        if result.returncode == 0:
            import json
            return json.loads(result.stdout)
        return None
    except Exception as e:
        logger.error(f"Failed to get video info: {e}")
        return None


def export_video(
    input_path: str,
    output_path: str,
    format: Literal["mp4", "mov", "webm"] = "mp4",
    quality: Literal["low", "medium", "high"] = "medium",
    overwrite: bool = True
) -> Tuple[bool, Optional[str]]:
    """
    Export video with specified format and quality using ffmpeg.

    Args:
        input_path: Path to the input video file
        output_path: Path where the output video will be saved
        format: Output format (mp4, mov, webm)
        quality: Quality preset (low, medium, high)
        overwrite: Whether to overwrite existing output file

    Returns:
        Tuple of (success: bool, error_message: Optional[str])
    """
    # Validate inputs
    if not os.path.exists(input_path):
        return False, f"Input video not found: {input_path}"

    if format not in FORMAT_CONFIGS:
        return False, f"Unsupported format: {format}"

    if quality not in QUALITY_PRESETS:
        return False, f"Invalid quality preset: {quality}"

    # Check ffmpeg availability
    if not check_ffmpeg_available():
        return False, "ffmpeg is not available on this system"

    # Create output directory if it doesn't exist
    output_dir = os.path.dirname(output_path)
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)

    # Get format and quality settings
    fmt_config = FORMAT_CONFIGS[format]
    quality_preset = QUALITY_PRESETS[quality]

    # Build ffmpeg command
    cmd = [
        "ffmpeg",
        "-i", input_path,
        "-c:v", fmt_config["codec"],
        "-b:v", quality_preset["video_bitrate"],
        "-vf", f"scale={quality_preset['resolution']}",
        "-c:a", fmt_config["audio_codec"],
        "-b:a", quality_preset["audio_bitrate"],
    ]

    if overwrite:
        cmd.append("-y")

    cmd.append(output_path)

    logger.info(f"Exporting video: {input_path} -> {output_path} ({format}, {quality})")

    try:
        # Run ffmpeg
        result = subprocess.run(
            cmd,
            capture_output=True,
            timeout=300,  # 5 minutes max
            text=True
        )

        if result.returncode == 0:
            if os.path.exists(output_path):
                logger.info(f"Video exported successfully: {output_path}")
                return True, None
            else:
                error_msg = "Export command succeeded but output file not found"
                logger.error(error_msg)
                return False, error_msg
        else:
            error_msg = f"ffmpeg error: {result.stderr}"
            logger.error(error_msg)
            return False, error_msg

    except subprocess.TimeoutExpired:
        error_msg = "Video export timed out (exceeded 5 minutes)"
        logger.error(error_msg)
        return False, error_msg
    except Exception as e:
        error_msg = f"Unexpected error during export: {str(e)}"
        logger.error(error_msg)
        return False, error_msg


def get_export_path(
    storage_path: str,
    job_id: int,
    format: str,
    quality: str
) -> str:
    """
    Generate the export file path for a job.

    Args:
        storage_path: Base storage path for videos
        job_id: Job ID
        format: Output format
        quality: Quality preset

    Returns:
        Full path to the export file
    """
    exports_dir = Path(storage_path) / "exports" / str(job_id)
    exports_dir.mkdir(parents=True, exist_ok=True)

    filename = f"{format}_{quality}.{FORMAT_CONFIGS[format]['ext']}"
    return str(exports_dir / filename)


def cleanup_old_exports(storage_path: str, job_id: int) -> None:
    """
    Clean up old export files for a job.

    Args:
        storage_path: Base storage path for videos
        job_id: Job ID
    """
    exports_dir = Path(storage_path) / "exports" / str(job_id)

    if exports_dir.exists():
        for file in exports_dir.iterdir():
            if file.is_file():
                try:
                    file.unlink()
                    logger.info(f"Deleted old export: {file}")
                except Exception as e:
                    logger.warning(f"Failed to delete {file}: {e}")
</file>

<file path="backend/services/video_renderer.py">
"""
Video Rendering Background Task.

This module handles the background task for rendering the final video from an approved
storyboard using the Replicate API. It orchestrates:
1. Extracting image URLs from storyboard
2. Generating video from images using Replicate
3. Downloading and saving the video
4. Progress tracking and cost calculation
5. Error handling with retry logic
"""

import logging
import time
import requests
from pathlib import Path
from typing import Optional, Dict, Any
from datetime import datetime

from ..models.video_generation import VideoStatus, VideoProgress
from ..services.replicate_client import ReplicateClient
from ..database import (
    get_job,
    update_job_progress,
    mark_job_failed,
    increment_retry_count,
    get_db
)

# Configure logging
logger = logging.getLogger(__name__)

# Configuration constants
MAX_RETRIES = 2  # Max 2 retries for video rendering
TIMEOUT = 600  # 10 minutes timeout for video rendering
EXPONENTIAL_BACKOFF_BASE = 30  # 30s, 90s for retries
COST_VARIANCE_THRESHOLD = 1.2  # Log if actual cost > estimated * 1.2


def render_video_task(job_id: int) -> None:
    """
    Main background task to render video from approved storyboard.

    This function orchestrates the complete video rendering workflow:
    1. Fetches job from database
    2. Validates storyboard is approved
    3. Extracts image URLs from storyboard
    4. Calls Replicate API to generate video
    5. Polls for video completion
    6. Downloads and saves video locally
    7. Updates job with video URL
    8. Calculates and stores actual cost
    9. Updates status to 'completed'

    Args:
        job_id: The video generation job ID

    Error Handling:
        - Marks job as failed on critical errors
        - Implements retry logic with exponential backoff (30s, 90s)
        - Tracks retry attempts
        - Updates progress throughout the process
    """
    logger.info(f"Starting video rendering for job {job_id}")

    try:
        # 1. Fetch job from database
        job = get_job(job_id)
        if not job:
            logger.error(f"Job {job_id} not found")
            return

        # 2. Validate storyboard is approved
        if not job.get("approved"):
            logger.error(f"Job {job_id}: Storyboard not approved, cannot render video")
            mark_job_failed(job_id, "Storyboard must be approved before rendering video")
            return

        # 3. Validate storyboard_data exists and has images
        storyboard_data = job.get("storyboard_data")
        if not storyboard_data:
            logger.error(f"Job {job_id}: No storyboard data found")
            mark_job_failed(job_id, "No storyboard data available")
            return

        # Parse storyboard data (it's stored as JSON string)
        import json
        if isinstance(storyboard_data, str):
            storyboard_data = json.loads(storyboard_data)

        if not isinstance(storyboard_data, list) or len(storyboard_data) == 0:
            logger.error(f"Job {job_id}: Storyboard data is empty or invalid")
            mark_job_failed(job_id, "Storyboard data is empty or invalid")
            return

        # 4. Extract image URLs from storyboard
        logger.info(f"Job {job_id}: Extracting image URLs from storyboard")
        image_urls = []
        for idx, entry in enumerate(storyboard_data):
            image_url = entry.get("image_url")
            if not image_url:
                logger.error(f"Job {job_id}: Scene {idx + 1} is missing image_url")
                mark_job_failed(job_id, f"Scene {idx + 1} is missing generated image")
                return
            image_urls.append(image_url)

        logger.info(f"Job {job_id}: Extracted {len(image_urls)} image URLs")

        # 5. Update status to 'rendering'
        _update_status(job_id, VideoStatus.RENDERING, "Rendering video from images...")

        # 6. Generate video with retry logic
        video_result = _render_video_with_retry(
            job_id,
            image_urls,
            max_retries=MAX_RETRIES
        )

        if not video_result.get("success"):
            error_msg = video_result.get("error", "Video rendering failed")
            logger.error(f"Job {job_id}: Video rendering failed - {error_msg}")
            mark_job_failed(job_id, error_msg)
            return

        video_url = video_result.get("video_url")
        duration_seconds = video_result.get("duration_seconds", 0)

        logger.info(f"Job {job_id}: Video rendered successfully - {video_url}")

        # 7. Download and save video
        _update_progress(
            job_id,
            current_stage=VideoStatus.RENDERING,
            message="Downloading video..."
        )

        try:
            local_video_path = download_video(video_url, job_id)
            logger.info(f"Job {job_id}: Video downloaded to {local_video_path}")
        except Exception as e:
            logger.error(f"Job {job_id}: Failed to download video - {e}")
            mark_job_failed(job_id, f"Failed to download video: {str(e)}")
            return

        # 8. Calculate actual cost
        actual_cost = _calculate_actual_cost(len(image_urls), duration_seconds)
        estimated_cost = job.get("estimated_cost", 0.0)

        # Log variance if actual cost significantly exceeds estimate
        if actual_cost > estimated_cost * COST_VARIANCE_THRESHOLD:
            variance_pct = ((actual_cost - estimated_cost) / estimated_cost) * 100
            logger.warning(
                f"Job {job_id}: Actual cost ${actual_cost:.2f} exceeds estimate "
                f"${estimated_cost:.2f} by {variance_pct:.1f}%"
            )

        logger.info(
            f"Job {job_id}: Cost - Estimated: ${estimated_cost:.2f}, "
            f"Actual: ${actual_cost:.2f}"
        )

        # 9. Update job with video URL and cost
        _update_status(job_id, VideoStatus.RENDERING, "Finalizing...")

        with get_db() as conn:
            conn.execute(
                """
                UPDATE generated_videos
                SET video_url = ?, actual_cost = ?, status = 'completed', updated_at = CURRENT_TIMESTAMP
                WHERE id = ?
                """,
                (local_video_path, actual_cost, job_id)
            )
            conn.commit()

        # 10. Update final progress
        _update_progress(
            job_id,
            current_stage=VideoStatus.COMPLETED,
            message="Video rendering complete!"
        )

        logger.info(f"Job {job_id}: Video rendering task completed successfully")

    except Exception as e:
        logger.exception(f"Job {job_id}: Unexpected error in video rendering")
        mark_job_failed(job_id, f"Unexpected error: {str(e)}")


def _render_video_with_retry(
    job_id: int,
    image_urls: list[str],
    max_retries: int = MAX_RETRIES
) -> Dict[str, Any]:
    """
    Generate video with retry logic and exponential backoff.

    Args:
        job_id: Job ID for logging and retry tracking
        image_urls: List of image URLs to stitch into video
        max_retries: Maximum number of retry attempts

    Returns:
        Dict with 'success', 'video_url', 'duration_seconds', and 'error' keys
    """
    replicate_client = ReplicateClient()

    for attempt in range(max_retries + 1):
        try:
            logger.info(
                f"Job {job_id}: Video rendering attempt {attempt + 1}/{max_retries + 1}"
            )

            # Update progress
            _update_progress(
                job_id,
                current_stage=VideoStatus.RENDERING,
                message=f"Rendering video (attempt {attempt + 1})..."
            )

            # Call Replicate API
            result = replicate_client.generate_video(image_urls)

            if result.get("success"):
                logger.info(f"Job {job_id}: Video rendering succeeded")
                return result
            else:
                error = result.get("error", "Unknown error")
                logger.warning(
                    f"Job {job_id}: Rendering attempt {attempt + 1} failed - {error}"
                )

                # Check if we should retry
                if attempt < max_retries:
                    # Exponential backoff: 30s, 90s
                    backoff_delay = EXPONENTIAL_BACKOFF_BASE * (3 ** attempt)
                    logger.info(
                        f"Job {job_id}: Retrying in {backoff_delay}s..."
                    )

                    # Track retry count in database
                    increment_retry_count(job_id)

                    # Update progress with retry info
                    _update_progress(
                        job_id,
                        current_stage=VideoStatus.RENDERING,
                        message=f"Retrying in {backoff_delay}s (attempt {attempt + 2}/{max_retries + 1})..."
                    )

                    time.sleep(backoff_delay)
                else:
                    # All retries exhausted
                    logger.error(f"Job {job_id}: All rendering retries exhausted")
                    return result

        except Exception as e:
            logger.error(
                f"Job {job_id}: Rendering attempt {attempt + 1} exception - {e}"
            )

            if attempt < max_retries:
                backoff_delay = EXPONENTIAL_BACKOFF_BASE * (3 ** attempt)
                logger.info(
                    f"Job {job_id}: Retrying after exception in {backoff_delay}s..."
                )

                increment_retry_count(job_id)

                _update_progress(
                    job_id,
                    current_stage=VideoStatus.RENDERING,
                    message=f"Retrying after error in {backoff_delay}s..."
                )

                time.sleep(backoff_delay)
            else:
                return {
                    "success": False,
                    "video_url": None,
                    "error": f"Exception after {max_retries + 1} attempts: {str(e)}",
                    "duration_seconds": 0
                }

    # Should not reach here, but just in case
    return {
        "success": False,
        "video_url": None,
        "error": "Max retries exceeded",
        "duration_seconds": 0
    }


def download_video(video_url: str, job_id: int) -> str:
    """
    Download video from Replicate and save to local storage.

    This function:
    1. Creates storage directory: DATA/videos/{job_id}/
    2. Downloads video with streaming
    3. Validates video format using magic bytes
    4. Saves video as final.mp4
    5. Returns local file path

    Args:
        video_url: URL of the video to download
        job_id: Job ID for storage organization

    Returns:
        Local file path to the downloaded video

    Raises:
        ValueError: If video validation fails
        requests.RequestException: If download fails
    """
    logger.info(f"Job {job_id}: Downloading video from {video_url}")

    # Create job-specific video directory
    video_dir = Path(__file__).parent.parent / "DATA" / "videos" / str(job_id)
    video_dir.mkdir(parents=True, exist_ok=True)

    # Save as final.mp4
    video_path = video_dir / "final.mp4"

    try:
        # Download with timeout and streaming
        response = requests.get(video_url, stream=True, timeout=TIMEOUT)
        response.raise_for_status()

        # Write to temporary file first
        temp_path = video_path.with_suffix(".tmp")
        bytes_downloaded = 0

        with open(temp_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
                    bytes_downloaded += len(chunk)

        # Validate download
        if bytes_downloaded == 0:
            raise ValueError("Downloaded file is empty (0 bytes)")

        if bytes_downloaded < 1024:  # Less than 1KB is suspicious
            raise ValueError(f"Downloaded file is too small ({bytes_downloaded} bytes)")

        # Validate file is a video by checking magic bytes
        with open(temp_path, 'rb') as f:
            header = f.read(12)
            is_video = False

            # Check common video file signatures
            if header.startswith(b'\x00\x00\x00\x18ftypmp4') or \
               header.startswith(b'\x00\x00\x00\x1cftypisom') or \
               header.startswith(b'\x00\x00\x00\x14ftyp') or \
               header[4:8] == b'ftyp':  # Generic MP4/MOV
                is_video = True
            elif header.startswith(b'RIFF') and header[8:12] == b'AVI ':  # AVI
                is_video = True
            elif header.startswith(b'\x1a\x45\xdf\xa3'):  # WebM/MKV
                is_video = True

            if not is_video:
                raise ValueError(
                    f"Downloaded file does not appear to be a valid video "
                    f"(header: {header.hex()})"
                )

        # Move from temp to final location
        temp_path.replace(video_path)

        logger.info(
            f"Job {job_id}: Video downloaded successfully "
            f"({bytes_downloaded} bytes) to {video_path}"
        )

        # Return relative path from backend directory
        return f"/api/videos/{job_id}/data"

    except requests.exceptions.Timeout:
        logger.error(f"Job {job_id}: Video download timeout")
        raise ValueError(f"Video download timeout after {TIMEOUT} seconds")

    except requests.exceptions.RequestException as e:
        logger.error(f"Job {job_id}: Network error during video download - {e}")
        raise ValueError(f"Network error during video download: {str(e)}")

    except Exception as e:
        logger.error(f"Job {job_id}: Unexpected error during video download - {e}")
        # Clean up temp file if it exists
        if temp_path.exists():
            temp_path.unlink()
        raise


def _calculate_actual_cost(num_images: int, video_duration: int) -> float:
    """
    Calculate actual cost from Replicate API usage.

    Uses the same pricing as ReplicateClient:
    - Flux-Schnell: $0.003 per image
    - SkyReels-2: $0.10 per second of video

    Args:
        num_images: Number of images generated
        video_duration: Duration of video in seconds

    Returns:
        Total actual cost in USD
    """
    # Import pricing from ReplicateClient
    image_cost = num_images * ReplicateClient.FLUX_SCHNELL_PRICE_PER_IMAGE
    video_cost = video_duration * ReplicateClient.SKYREELS2_PRICE_PER_SECOND
    total_cost = image_cost + video_cost

    logger.debug(
        f"Cost calculation - Images: {num_images} x ${ReplicateClient.FLUX_SCHNELL_PRICE_PER_IMAGE} = ${image_cost:.3f}, "
        f"Video: {video_duration}s x ${ReplicateClient.SKYREELS2_PRICE_PER_SECOND} = ${video_cost:.2f}, "
        f"Total: ${total_cost:.2f}"
    )

    return round(total_cost, 2)


def _update_status(job_id: int, status: VideoStatus, message: str) -> None:
    """
    Update job status and progress message.

    Args:
        job_id: Job ID
        status: New VideoStatus
        message: Progress message
    """
    try:
        with get_db() as conn:
            conn.execute(
                "UPDATE generated_videos SET status = ? WHERE id = ?",
                (status.value, job_id)
            )
            conn.commit()

        # Update progress
        update_job_progress(job_id, {
            "current_stage": status.value,
            "message": message
        })

        logger.info(f"Job {job_id}: Status updated to {status.value}")
    except Exception as e:
        logger.error(f"Job {job_id}: Failed to update status - {e}")


def _update_progress(
    job_id: int,
    current_stage: VideoStatus,
    message: Optional[str] = None
) -> None:
    """
    Update job progress with detailed tracking information.

    Args:
        job_id: Job ID
        current_stage: Current VideoStatus
        message: Optional progress message
    """
    progress = VideoProgress(
        current_stage=current_stage,
        scenes_total=0,
        scenes_completed=0,
        current_scene=None,
        estimated_completion_seconds=None,
        message=message
    )

    try:
        update_job_progress(job_id, progress.model_dump())
    except Exception as e:
        logger.error(f"Job {job_id}: Failed to update progress - {e}")
</file>

<file path="backend/__init__.py">
# Backend package initialization
</file>

<file path="backend/add_team_users.py">
#!/usr/bin/env python3
"""
Script to add team users with API keys.
Creates: reuben, mike, harrison with password "bestvideoproject"
"""

import sys
from pathlib import Path

# Add backend directory to path
sys.path.insert(0, str(Path(__file__).parent))

from database import create_user, get_user_by_username, create_api_key as db_create_api_key
from auth import get_password_hash, generate_api_key, hash_api_key

# Team configuration
TEAM_USERS = [
    {
        "username": "reuben",
        "email": "reuben@bestvideoproject.com",
        "is_admin": True  # Reuben as admin
    },
    {
        "username": "mike",
        "email": "mike@bestvideoproject.com",
        "is_admin": False
    },
    {
        "username": "harrison",
        "email": "harrison@bestvideoproject.com",
        "is_admin": False
    }
]

PASSWORD = "bestvideoproject"

def main():
    print("=" * 60)
    print("Best Video Project - Team User Setup")
    print("=" * 60)
    print()

    created_users = []
    api_keys_generated = {}

    # Create users
    for user_config in TEAM_USERS:
        username = user_config["username"]
        email = user_config["email"]
        is_admin = user_config["is_admin"]

        # Check if user already exists
        existing_user = get_user_by_username(username)

        if existing_user:
            print(f"⚠️  User '{username}' already exists (ID: {existing_user['id']})")
            user_id = existing_user["id"]
        else:
            try:
                hashed_password = get_password_hash(PASSWORD)
                user_id = create_user(
                    username=username,
                    email=email,
                    hashed_password=hashed_password,
                    is_admin=is_admin
                )
                print(f"✓ Created user '{username}' (ID: {user_id})")
                if is_admin:
                    print(f"  → Admin privileges granted")
                created_users.append(username)
            except Exception as e:
                print(f"✗ Failed to create user '{username}': {e}")
                continue

        # Generate API key
        try:
            api_key = generate_api_key()
            key_hash = hash_api_key(api_key)

            key_id = db_create_api_key(
                key_hash=key_hash,
                name=f"{username}'s API Key",
                user_id=user_id,
                expires_at=None  # No expiration
            )

            api_keys_generated[username] = api_key
            print(f"✓ Generated API key for '{username}' (Key ID: {key_id})")
        except Exception as e:
            print(f"✗ Failed to create API key for '{username}': {e}")

    print()
    print("=" * 60)
    print("Setup Complete!")
    print("=" * 60)
    print()

    # Display summary
    print("Team Users:")
    for user_config in TEAM_USERS:
        username = user_config["username"]
        role = "Admin" if user_config["is_admin"] else "User"
        print(f"  • {username} ({role})")

    print()
    print("Credentials:")
    print(f"  Password (all users): {PASSWORD}")
    print()

    if api_keys_generated:
        print("=" * 60)
        print("API KEYS - SAVE THESE SECURELY!")
        print("=" * 60)
        print()
        for username, api_key in api_keys_generated.items():
            print(f"{username}:")
            print(f"  {api_key}")
            print()

        print("=" * 60)
        print()
        print("Usage:")
        print("  curl http://localhost:8000/api/videos \\")
        print("    -H 'X-API-Key: <key-from-above>'")
        print()
        print("Or login with username/password:")
        print("  curl -X POST http://localhost:8000/api/auth/login \\")
        print("    -H 'Content-Type: application/x-www-form-urlencoded' \\")
        print("    -d 'username=reuben&password=bestvideoproject'")
        print()

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"\n✗ Unexpected error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
</file>

<file path="backend/api_routes.py">
"""API routes for Clients and Campaigns management.

This module provides RESTful API endpoints for the ad-video-gen frontend:
- /api/clients - Client CRUD operations
- /api/clients/:id/stats - Client statistics
- /api/campaigns - Campaign CRUD operations
- /api/campaigns/:id/stats - Campaign statistics

Note: Asset management has been consolidated into /api/v2/upload-asset
Use that endpoint with clientId/campaignId parameters for asset uploads.
"""

from fastapi import APIRouter, HTTPException, Depends, File, UploadFile, Query
from pydantic import BaseModel, Field
from typing import Dict, Optional, List, Any
from datetime import datetime
import os
import uuid

from .auth import verify_auth
from .database_helpers import (
    # Client operations
    create_client,
    get_client_by_id,
    list_clients,
    update_client,
    delete_client,
    get_client_stats,
    # Campaign operations
    create_campaign,
    get_campaign_by_id,
    list_campaigns,
    update_campaign,
    delete_campaign,
    get_campaign_stats,
    # Video operations
    list_videos_by_campaign,
    update_video_metrics
)

# Create router
router = APIRouter()


# ============================================================================
# PYDANTIC MODELS
# ============================================================================

class BrandGuidelines(BaseModel):
    """Client brand guidelines structure."""
    colors: List[str] = Field(default_factory=list, description="Array of hex colors")
    fonts: List[str] = Field(default_factory=list, description="Array of font names")
    styleKeywords: List[str] = Field(default_factory=list, description="Style descriptors")
    documentUrls: Optional[List[str]] = Field(default=None, description="Brand guideline document URLs")


class CreateClientRequest(BaseModel):
    """Request model for creating a client."""
    name: str = Field(..., min_length=1, max_length=100, description="Client name")
    description: str = Field(default="", max_length=500, description="Client description")
    brandGuidelines: Optional[BrandGuidelines] = Field(default=None, description="Brand guidelines")


class UpdateClientRequest(BaseModel):
    """Request model for updating a client (all fields optional)."""
    name: Optional[str] = Field(None, min_length=1, max_length=100)
    description: Optional[str] = Field(None, max_length=500)
    brandGuidelines: Optional[BrandGuidelines] = None


class CampaignBrief(BaseModel):
    """Campaign creative brief structure."""
    objective: str = Field(..., description="Campaign objective")
    targetAudience: str = Field(..., description="Target audience description")
    keyMessages: List[str] = Field(..., description="Key messages array")


class CreateCampaignRequest(BaseModel):
    """Request model for creating a campaign."""
    clientId: str = Field(..., description="Client UUID")
    name: str = Field(..., min_length=1, max_length=100, description="Campaign name")
    goal: str = Field(..., min_length=1, max_length=500, description="Campaign goal")
    status: str = Field(default="draft", pattern="^(active|archived|draft)$", description="Campaign status")
    brief: Optional[CampaignBrief] = Field(default=None, description="Creative brief")


class UpdateCampaignRequest(BaseModel):
    """Request model for updating a campaign (all fields optional)."""
    name: Optional[str] = Field(None, min_length=1, max_length=100)
    goal: Optional[str] = Field(None, min_length=1, max_length=500)
    status: Optional[str] = Field(None, pattern="^(active|archived|draft)$")
    brief: Optional[CampaignBrief] = None


class UpdateVideoMetricsRequest(BaseModel):
    """Request model for updating video performance metrics."""
    views: Optional[int] = Field(None, ge=0)
    clicks: Optional[int] = Field(None, ge=0)
    ctr: Optional[float] = Field(None, ge=0.0, le=1.0)
    conversions: Optional[int] = Field(None, ge=0)


class ApiResponse(BaseModel):
    """Generic API response wrapper."""
    data: Any
    message: Optional[str] = None
    timestamp: str = Field(default_factory=lambda: datetime.utcnow().isoformat() + "Z")


# ============================================================================
# CLIENT ENDPOINTS
# ============================================================================

@router.get("/clients")
async def get_clients(
    current_user: Dict[str, Any] = Depends(verify_auth)
) -> ApiResponse:
    """
    Get all clients for the authenticated user.

    Returns:
        ApiResponse with array of Client objects
    """
    try:
        clients = list_clients(current_user["id"])
        return ApiResponse(data=clients)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to retrieve clients: {str(e)}")


@router.get("/clients/{client_id}")
async def get_client(
    client_id: str,
    current_user: Dict[str, Any] = Depends(verify_auth)
) -> ApiResponse:
    """
    Get a single client by ID.

    Args:
        client_id: Client UUID

    Returns:
        ApiResponse with Client object

    Raises:
        404: Client not found
    """
    client = get_client_by_id(client_id, current_user["id"])
    if not client:
        raise HTTPException(status_code=404, detail="Client not found")

    return ApiResponse(data=client)


@router.post("/clients", status_code=201)
async def create_new_client(
    request: CreateClientRequest,
    current_user: Dict[str, Any] = Depends(verify_auth)
) -> ApiResponse:
    """
    Create a new client.

    Args:
        request: CreateClientRequest with client data

    Returns:
        ApiResponse with created Client object (includes id, createdAt, updatedAt)
    """
    try:
        # Convert Pydantic model to dict
        brand_guidelines = request.brandGuidelines.dict() if request.brandGuidelines else None

        # Create client
        client_id = create_client(
            user_id=current_user["id"],
            name=request.name,
            description=request.description,
            brand_guidelines=brand_guidelines
        )

        # Retrieve and return the created client
        client = get_client_by_id(client_id, current_user["id"])
        return ApiResponse(data=client, message="Client created successfully")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to create client: {str(e)}")


@router.patch("/clients/{client_id}")
async def update_existing_client(
    client_id: str,
    request: UpdateClientRequest,
    current_user: Dict[str, Any] = Depends(verify_auth)
) -> ApiResponse:
    """
    Update an existing client (partial update).

    Args:
        client_id: Client UUID
        request: UpdateClientRequest with fields to update

    Returns:
        ApiResponse with updated Client object

    Raises:
        404: Client not found
    """
    # Convert Pydantic model to dict (only provided fields)
    brand_guidelines = request.brandGuidelines.dict() if request.brandGuidelines else None

    success = update_client(
        client_id=client_id,
        user_id=current_user["id"],
        name=request.name,
        description=request.description,
        brand_guidelines=brand_guidelines
    )

    if not success:
        raise HTTPException(status_code=404, detail="Client not found")

    # Retrieve and return the updated client
    client = get_client_by_id(client_id, current_user["id"])
    return ApiResponse(data=client, message="Client updated successfully")


@router.delete("/clients/{client_id}")
async def delete_existing_client(
    client_id: str,
    current_user: Dict[str, Any] = Depends(verify_auth)
) -> ApiResponse:
    """
    Delete a client (cascades to campaigns and videos).

    Args:
        client_id: Client UUID

    Returns:
        ApiResponse with success message

    Raises:
        404: Client not found
    """
    success = delete_client(client_id, current_user["id"])
    if not success:
        raise HTTPException(status_code=404, detail="Client not found")

    return ApiResponse(data=None, message="Client deleted successfully")


@router.get("/clients/{client_id}/stats")
async def get_client_statistics(
    client_id: str,
    current_user: Dict[str, Any] = Depends(verify_auth)
) -> ApiResponse:
    """
    Get statistics for a client.

    Args:
        client_id: Client UUID

    Returns:
        ApiResponse with ClientStats object

    Raises:
        404: Client not found
    """
    stats = get_client_stats(client_id, current_user["id"])
    if stats is None:
        raise HTTPException(status_code=404, detail="Client not found")

    return ApiResponse(data=stats)


# ============================================================================
# DEPRECATED: Use POST /api/v2/upload-asset with clientId parameter instead
# ============================================================================
# @router.post("/clients/{client_id}/assets", status_code=201)
# This endpoint has been deprecated in favor of the consolidated asset API.
# Use POST /api/v2/upload-asset with form-data parameter clientId={client_id}


# ============================================================================
# CAMPAIGN ENDPOINTS
# ============================================================================

@router.get("/campaigns")
async def get_campaigns(
    clientId: Optional[str] = Query(None, description="Filter by client ID"),
    current_user: Dict[str, Any] = Depends(verify_auth)
) -> ApiResponse:
    """
    Get all campaigns for the authenticated user, optionally filtered by client.

    Args:
        clientId: Optional client UUID to filter campaigns

    Returns:
        ApiResponse with array of Campaign objects
    """
    try:
        campaigns = list_campaigns(current_user["id"], client_id=clientId)
        return ApiResponse(data=campaigns)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to retrieve campaigns: {str(e)}")


@router.get("/campaigns/{campaign_id}")
async def get_campaign(
    campaign_id: str,
    current_user: Dict[str, Any] = Depends(verify_auth)
) -> ApiResponse:
    """
    Get a single campaign by ID.

    Args:
        campaign_id: Campaign UUID

    Returns:
        ApiResponse with Campaign object

    Raises:
        404: Campaign not found
    """
    campaign = get_campaign_by_id(campaign_id, current_user["id"])
    if not campaign:
        raise HTTPException(status_code=404, detail="Campaign not found")

    return ApiResponse(data=campaign)


@router.post("/campaigns", status_code=201)
async def create_new_campaign(
    request: CreateCampaignRequest,
    current_user: Dict[str, Any] = Depends(verify_auth)
) -> ApiResponse:
    """
    Create a new campaign.

    Args:
        request: CreateCampaignRequest with campaign data

    Returns:
        ApiResponse with created Campaign object (includes id, createdAt, updatedAt)
    """
    try:
        # Verify client exists
        client = get_client_by_id(request.clientId, current_user["id"])
        if not client:
            raise HTTPException(status_code=404, detail="Client not found")

        # Convert brief to dict
        brief = request.brief.dict() if request.brief else None

        # Create campaign
        campaign_id = create_campaign(
            user_id=current_user["id"],
            client_id=request.clientId,
            name=request.name,
            goal=request.goal,
            status=request.status,
            brief=brief
        )

        # Retrieve and return the created campaign
        campaign = get_campaign_by_id(campaign_id, current_user["id"])
        return ApiResponse(data=campaign, message="Campaign created successfully")
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to create campaign: {str(e)}")


@router.patch("/campaigns/{campaign_id}")
async def update_existing_campaign(
    campaign_id: str,
    request: UpdateCampaignRequest,
    current_user: Dict[str, Any] = Depends(verify_auth)
) -> ApiResponse:
    """
    Update an existing campaign (partial update).

    Args:
        campaign_id: Campaign UUID
        request: UpdateCampaignRequest with fields to update

    Returns:
        ApiResponse with updated Campaign object

    Raises:
        404: Campaign not found
    """
    # Convert brief to dict
    brief = request.brief.dict() if request.brief else None

    success = update_campaign(
        campaign_id=campaign_id,
        user_id=current_user["id"],
        name=request.name,
        goal=request.goal,
        status=request.status,
        brief=brief
    )

    if not success:
        raise HTTPException(status_code=404, detail="Campaign not found")

    # Retrieve and return the updated campaign
    campaign = get_campaign_by_id(campaign_id, current_user["id"])
    return ApiResponse(data=campaign, message="Campaign updated successfully")


@router.delete("/campaigns/{campaign_id}")
async def delete_existing_campaign(
    campaign_id: str,
    current_user: Dict[str, Any] = Depends(verify_auth)
) -> ApiResponse:
    """
    Delete a campaign (cascades to campaign assets).

    Args:
        campaign_id: Campaign UUID

    Returns:
        ApiResponse with success message

    Raises:
        404: Campaign not found
        409: Campaign has associated videos (cannot delete)
    """
    # Check if campaign has videos
    videos = list_videos_by_campaign(campaign_id, limit=1)
    if videos:
        raise HTTPException(
            status_code=409,
            detail="Cannot delete campaign with associated videos. Please delete videos first."
        )

    success = delete_campaign(campaign_id, current_user["id"])
    if not success:
        raise HTTPException(status_code=404, detail="Campaign not found")

    return ApiResponse(data=None, message="Campaign deleted successfully")


@router.get("/campaigns/{campaign_id}/stats")
async def get_campaign_statistics(
    campaign_id: str,
    current_user: Dict[str, Any] = Depends(verify_auth)
) -> ApiResponse:
    """
    Get statistics for a campaign.

    Args:
        campaign_id: Campaign UUID

    Returns:
        ApiResponse with CampaignStats object

    Raises:
        404: Campaign not found
    """
    stats = get_campaign_stats(campaign_id, current_user["id"])
    if stats is None:
        raise HTTPException(status_code=404, detail="Campaign not found")

    return ApiResponse(data=stats)


# ============================================================================
# DEPRECATED: Use POST /api/v2/upload-asset with campaignId parameter instead
# ============================================================================
# @router.post("/campaigns/{campaign_id}/assets", status_code=201)
# This endpoint has been deprecated in favor of the consolidated asset API.
# Use POST /api/v2/upload-asset with form-data parameter campaignId={campaign_id}


# ============================================================================
# VIDEO METRICS ENDPOINT (Enhancement)
# ============================================================================

@router.patch("/videos/{video_id}/metrics")
async def update_video_performance_metrics(
    video_id: int,
    request: UpdateVideoMetricsRequest,
    current_user: Dict[str, Any] = Depends(verify_auth)
) -> ApiResponse:
    """
    Update video performance metrics (views, clicks, CTR, conversions).

    Args:
        video_id: Video ID
        request: UpdateVideoMetricsRequest with metrics to update

    Returns:
        ApiResponse with success message

    Raises:
        404: Video not found
    """
    success = update_video_metrics(
        video_id=video_id,
        views=request.views,
        clicks=request.clicks,
        ctr=request.ctr,
        conversions=request.conversions
    )

    if not success:
        raise HTTPException(status_code=404, detail="Video not found")

    return ApiResponse(data=None, message="Video metrics updated successfully")
</file>

<file path="backend/asset_metadata.py">
"""
Asset metadata extraction utilities.

This module provides functions to extract metadata from various file types:
- Images: dimensions (width, height)
- Videos: dimensions, duration, thumbnail generation
- Audio: duration, waveform generation (placeholder)
- Documents: page count (PDFs)
"""

import os
import subprocess
import mimetypes
from pathlib import Path
from typing import Dict, Any, Optional, Tuple
from PIL import Image
import json


def get_file_format(file_path: str, mime_type: Optional[str] = None) -> str:
    """Get the file format/extension.

    Args:
        file_path: Path to the file
        mime_type: Optional MIME type

    Returns:
        File format (e.g., 'png', 'mp4', 'pdf')
    """
    # Get extension from file path
    ext = Path(file_path).suffix.lower().lstrip('.')

    # Map common extensions
    if ext:
        return ext

    # Fallback to mime type
    if mime_type:
        ext_from_mime = mimetypes.guess_extension(mime_type)
        if ext_from_mime:
            return ext_from_mime.lstrip('.')

    return 'unknown'


def determine_asset_type(mime_type: str, file_format: str) -> str:
    """Determine asset_type from MIME type and format.

    Args:
        mime_type: MIME type string
        file_format: File extension/format

    Returns:
        Asset type: 'image', 'video', 'audio', or 'document'
    """
    if mime_type.startswith('image/'):
        return 'image'
    elif mime_type.startswith('video/'):
        return 'video'
    elif mime_type.startswith('audio/'):
        return 'audio'
    elif mime_type in ['application/pdf', 'application/msword', 'application/vnd.openxmlformats-officedocument.wordprocessingml.document']:
        return 'document'
    elif file_format.lower() in ['pdf', 'doc', 'docx', 'txt']:
        return 'document'
    else:
        return 'document'  # Default to document for unknown types


def extract_image_metadata(file_path: str) -> Dict[str, Any]:
    """Extract metadata from an image file.

    Args:
        file_path: Path to image file

    Returns:
        Dict with width and height
    """
    try:
        with Image.open(file_path) as img:
            width, height = img.size
            return {
                'width': width,
                'height': height
            }
    except Exception as e:
        print(f"Error extracting image metadata: {e}")
        return {}


def extract_video_metadata(file_path: str) -> Dict[str, Any]:
    """Extract metadata from a video file using ffprobe.

    Args:
        file_path: Path to video file

    Returns:
        Dict with width, height, and duration
    """
    try:
        # Use ffprobe to get video metadata
        cmd = [
            'ffprobe',
            '-v', 'quiet',
            '-print_format', 'json',
            '-show_format',
            '-show_streams',
            file_path
        ]

        result = subprocess.run(cmd, capture_output=True, text=True, timeout=10)

        if result.returncode != 0:
            print(f"ffprobe error: {result.stderr}")
            return {}

        data = json.loads(result.stdout)

        # Find video stream
        video_stream = None
        for stream in data.get('streams', []):
            if stream.get('codec_type') == 'video':
                video_stream = stream
                break

        if not video_stream:
            return {}

        metadata = {}

        # Get dimensions
        if 'width' in video_stream:
            metadata['width'] = video_stream['width']
        if 'height' in video_stream:
            metadata['height'] = video_stream['height']

        # Get duration (prefer from format, fallback to stream)
        duration_str = data.get('format', {}).get('duration') or video_stream.get('duration')
        if duration_str:
            metadata['duration'] = int(float(duration_str))

        return metadata

    except subprocess.TimeoutExpired:
        print(f"ffprobe timed out for {file_path}")
        return {}
    except FileNotFoundError:
        print("ffprobe not found. Install ffmpeg to extract video metadata.")
        return {}
    except Exception as e:
        print(f"Error extracting video metadata: {e}")
        return {}


def generate_video_thumbnail(video_path: str, output_path: str, timestamp: float = 1.0) -> bool:
    """Generate a thumbnail from a video at a specific timestamp.

    Args:
        video_path: Path to video file
        output_path: Path where thumbnail should be saved
        timestamp: Time in seconds to extract frame (default: 1.0)

    Returns:
        True if successful, False otherwise
    """
    try:
        # Ensure output directory exists
        os.makedirs(os.path.dirname(output_path), exist_ok=True)

        # Use ffmpeg to extract a frame
        cmd = [
            'ffmpeg',
            '-i', video_path,
            '-ss', str(timestamp),
            '-vframes', '1',
            '-q:v', '2',  # Quality (2 is high quality)
            '-y',  # Overwrite output file
            output_path
        ]

        result = subprocess.run(cmd, capture_output=True, timeout=15)

        if result.returncode == 0 and os.path.exists(output_path):
            return True
        else:
            print(f"ffmpeg thumbnail generation failed: {result.stderr.decode()}")
            return False

    except subprocess.TimeoutExpired:
        print(f"ffmpeg timed out generating thumbnail for {video_path}")
        return False
    except FileNotFoundError:
        print("ffmpeg not found. Install ffmpeg to generate video thumbnails.")
        return False
    except Exception as e:
        print(f"Error generating video thumbnail: {e}")
        return False


def extract_audio_metadata(file_path: str) -> Dict[str, Any]:
    """Extract metadata from an audio file.

    Args:
        file_path: Path to audio file

    Returns:
        Dict with duration
    """
    try:
        # Use ffprobe to get audio metadata
        cmd = [
            'ffprobe',
            '-v', 'quiet',
            '-print_format', 'json',
            '-show_format',
            file_path
        ]

        result = subprocess.run(cmd, capture_output=True, text=True, timeout=10)

        if result.returncode != 0:
            return {}

        data = json.loads(result.stdout)

        metadata = {}

        # Get duration
        duration_str = data.get('format', {}).get('duration')
        if duration_str:
            metadata['duration'] = int(float(duration_str))

        return metadata

    except Exception as e:
        print(f"Error extracting audio metadata: {e}")
        return {}


def extract_document_metadata(file_path: str) -> Dict[str, Any]:
    """Extract metadata from a document (PDF).

    Args:
        file_path: Path to document file

    Returns:
        Dict with pageCount (for PDFs)
    """
    try:
        # For now, we'll skip PDF page counting unless PyPDF2 is installed
        # This can be added later if needed
        return {}
    except Exception as e:
        print(f"Error extracting document metadata: {e}")
        return {}


def extract_file_metadata(file_path: str, mime_type: str) -> Dict[str, Any]:
    """Extract all relevant metadata from a file based on its type.

    Args:
        file_path: Path to the file
        mime_type: MIME type of the file

    Returns:
        Dict containing all extracted metadata
    """
    file_format = get_file_format(file_path, mime_type)
    asset_type = determine_asset_type(mime_type, file_format)

    metadata = {
        'asset_type': asset_type,
        'format': file_format,
        'size': os.path.getsize(file_path) if os.path.exists(file_path) else None
    }

    # Extract type-specific metadata
    if asset_type == 'image':
        metadata.update(extract_image_metadata(file_path))

    elif asset_type == 'video':
        video_meta = extract_video_metadata(file_path)
        metadata.update(video_meta)

    elif asset_type == 'audio':
        metadata.update(extract_audio_metadata(file_path))

    elif asset_type == 'document':
        metadata.update(extract_document_metadata(file_path))

    return metadata


if __name__ == "__main__":
    # Test the metadata extraction
    import sys

    if len(sys.argv) > 1:
        test_file = sys.argv[1]
        mime_type = mimetypes.guess_type(test_file)[0] or 'application/octet-stream'
        print(f"File: {test_file}")
        print(f"MIME type: {mime_type}")
        print(f"Metadata: {extract_file_metadata(test_file, mime_type)}")
</file>

<file path="backend/genesis_renderer.py">
"""
Genesis Photorealistic Renderer

Main renderer that orchestrates:
1. LLM semantic augmentation
2. Scene conversion to Genesis
3. Ray-traced rendering
4. Video export
"""

import os
import time
from typing import Dict, List, Optional, Tuple
from pathlib import Path

import genesis as gs
from llm_interpreter import get_interpreter
from scene_converter import SceneConverter


class RenderQuality:
    """Predefined quality presets for rendering"""

    DRAFT = {
        "spp": 64,
        "description": "Fast preview (30 sec/frame)",
        "tracing_depth": 16
    }

    HIGH = {
        "spp": 256,
        "description": "Production quality (2 min/frame)",
        "tracing_depth": 32
    }

    ULTRA = {
        "spp": 512,
        "description": "Maximum quality (4 min/frame)",
        "tracing_depth": 48
    }


class GenesisRenderer:
    """Photorealistic renderer using Genesis ray-tracer"""

    def __init__(
        self,
        quality: str = "high",
        output_dir: str = "./backend/DATA/genesis_videos"
    ):
        """
        Initialize Genesis renderer

        Args:
            quality: "draft", "high", or "ultra"
            output_dir: Directory to save rendered videos
        """

        self.quality = self._get_quality_preset(quality)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        self.scene = None
        self.camera = None
        self.converter = None
        self.llm_interpreter = get_interpreter()
        self.using_raytracer = False  # Track which renderer we're using

    def _get_quality_preset(self, quality: str) -> Dict:
        """Get quality preset by name"""
        presets = {
            "draft": RenderQuality.DRAFT,
            "high": RenderQuality.HIGH,
            "ultra": RenderQuality.ULTRA
        }
        return presets.get(quality.lower(), RenderQuality.HIGH)

    async def render_scene(
        self,
        scene_data: Dict,
        duration: float = 5.0,
        fps: int = 60,
        resolution: Tuple[int, int] = (1920, 1080),
        camera_config: Optional[Dict] = None,
        scene_context: Optional[str] = None
    ) -> str:
        """
        Render a complete scene to video

        Args:
            scene_data: JSON scene data with objects
            duration: Video duration in seconds
            fps: Frames per second
            resolution: (width, height) tuple
            camera_config: Optional camera position/settings
            scene_context: Optional overall scene description for LLM context

        Returns:
            Path to rendered video file
        """

        print(f"🎬 Starting Genesis render (Quality: {self.quality['description']})")
        start_time = time.time()

        # Step 1: Augment objects with LLM
        print("🤖 Augmenting scene with LLM...")
        augmented_objects = await self.llm_interpreter.augment_scene(
            scene_data.get("objects", []),
            scene_context=scene_context
        )
        scene_data["objects"] = augmented_objects

        # Step 2: Create Genesis scene with ray-tracer
        print("🌍 Creating Genesis scene...")
        self._create_scene()

        # Step 3: Convert JSON to Genesis entities
        print("📦 Converting objects to Genesis entities...")
        self.converter = SceneConverter(self.scene)
        # TODO: Fix ground plane - gs.surfaces.Surface has NotImplementedError in Genesis 0.3.7
        # self.converter.add_ground_plane()
        self.converter.convert_scene(scene_data)

        # Step 4: Setup camera
        print("📸 Setting up camera...")
        self._setup_camera(resolution, camera_config)

        # Step 5: Build scene
        print("🔨 Building scene...")
        self.scene.build()

        # Step 6: Render frames
        print(f"🎥 Rendering {int(duration * fps)} frames...")
        output_path = await self._render_video(duration, fps)

        elapsed = time.time() - start_time
        print(f"✅ Rendering complete in {elapsed:.1f}s: {output_path}")

        return output_path

    def _create_scene(self):
        """Create Genesis scene with ray-tracer backend"""

        # Initialize Genesis if not already initialized
        # Try GPU backend for ray-tracing support, fall back to CPU
        try:
            if not hasattr(gs, '_initialized') or not gs._initialized:
                # Try GPU backend first (supports RayTracer)
                try:
                    gs.init(backend=gs.gpu)
                    print("✅ Genesis initialized with GPU backend")
                except Exception as gpu_err:
                    print(f"⚠️  GPU backend failed ({gpu_err}), trying CPU...")
                    gs.init(backend=gs.cpu)
                    print("✅ Genesis initialized with CPU backend")
        except:
            # If Genesis is already initialized, continue
            pass

        # Configure lighting (3-point lighting setup)
        lights = [
            {
                "pos": (10.0, 20.0, 10.0),
                "color": (1.0, 0.95, 0.9),  # Warm key light
                "intensity": 15.0,
                "radius": 6.0
            },
            {
                "pos": (-10.0, 10.0, -10.0),
                "color": (0.8, 0.9, 1.0),  # Cool fill light
                "intensity": 5.0,
                "radius": 4.0
            },
            {
                "pos": (0.0, 5.0, -15.0),
                "color": (1.0, 1.0, 1.0),  # Back light
                "intensity": 8.0,
                "radius": 3.0
            }
        ]

        # Create scene with renderer
        # Try RayTracer first, fall back to Rasterizer if LuisaRenderer unavailable
        print("🎨 Attempting to create scene with RayTracer...")
        try:
            renderer = gs.renderers.RayTracer(
                lights=lights,
                env_radius=1000.0,
                logging_level="warning"
            )

            # Try to create scene - this is where LuisaRenderer import happens
            self.scene = gs.Scene(
                renderer=renderer,
                show_viewer=False,
                sim_options=gs.options.SimOptions(
                    dt=1/60,
                    gravity=(0, -9.81, 0)
                )
            )
            self.using_raytracer = True
            print("✅ RayTracer scene created successfully")

        except Exception as e:
            # RayTracer failed - fall back to Rasterizer
            print(f"⚠️  RayTracer unavailable ({str(e)[:50]}...), using Rasterizer with PBR")

            # Rasterizer with default parameters
            renderer = gs.renderers.Rasterizer()

            self.scene = gs.Scene(
                renderer=renderer,
                show_viewer=False,
                sim_options=gs.options.SimOptions(
                    dt=1/60,
                    gravity=(0, -9.81, 0)
                )
            )
            self.using_raytracer = False
            print("✅ Rasterizer scene created successfully (PBR materials enabled)")

    def _setup_camera(
        self,
        resolution: Tuple[int, int],
        camera_config: Optional[Dict] = None
    ):
        """Setup camera with photorealistic settings"""

        # Default camera config
        config = camera_config or {}

        pos = config.get("position", (8, 6, 8))
        lookat = config.get("lookat", (0, 2, 0))
        fov = config.get("fov", 40)

        # Check if we're using RayTracer or Rasterizer
        if self.using_raytracer:
            # RayTracer supports advanced camera features
            aperture = config.get("aperture", 2.8)
            self.camera = self.scene.add_camera(
                model="thinlens",  # Enable depth-of-field
                spp=self.quality["spp"],
                aperture=aperture,
                focus_dist=None,  # Auto-compute from pos/lookat
                denoise=True,  # Enable AI denoising
                res=resolution,
                fov=fov,
                pos=pos,
                lookat=lookat
            )
        else:
            # Rasterizer - simpler camera
            self.camera = self.scene.add_camera(
                res=resolution,
                fov=fov,
                pos=pos,
                lookat=lookat
            )

    async def _render_video(
        self,
        duration: float,
        fps: int
    ) -> str:
        """Render simulation to video"""

        # Generate unique output filename
        timestamp = int(time.time())
        output_filename = f"genesis_render_{timestamp}.mp4"
        output_path = str(self.output_dir / output_filename)

        # Start recording
        self.camera.start_recording()

        # Simulate and render frames
        num_frames = int(duration * fps)
        for frame_idx in range(num_frames):
            # Progress indicator
            if frame_idx % 10 == 0:
                progress = (frame_idx / num_frames) * 100
                print(f"  Progress: {progress:.1f}% ({frame_idx}/{num_frames} frames)")

            # Step physics simulation
            self.scene.step()

            # Optional: Update camera pose for dynamic shots
            # self.camera.set_pose(pos=..., lookat=...)

            # Render frame (automatically captured by recorder)
            self.camera.render(
                rgb=True,
                antialiasing=True
            )

        # Stop recording and export video
        self.camera.stop_recording(
            save_to_filename=output_path,
            fps=fps
        )

        return output_path

    def cleanup(self):
        """Clean up Genesis resources"""
        if self.scene:
            # Genesis handles cleanup automatically, but we can reset references
            self.scene = None
            self.camera = None
            self.converter = None


# Factory function for easy creation
def create_renderer(
    quality: str = "high",
    output_dir: str = "./backend/DATA/genesis_videos"
) -> GenesisRenderer:
    """
    Create a Genesis renderer with specified quality

    Args:
        quality: "draft", "high", or "ultra"
        output_dir: Output directory for videos

    Returns:
        GenesisRenderer instance
    """
    return GenesisRenderer(quality=quality, output_dir=output_dir)
</file>

<file path="backend/llm_interpreter.py">
"""
LLM Interpreter for Semantic Scene Augmentation

Takes simple geometry + text descriptions and uses an LLM to generate
detailed Genesis properties (materials, colors, scales, etc.)
"""

import os
import json
from typing import Dict, List, Optional
from openai import OpenAI
from pydantic import BaseModel


class GenesisProperties(BaseModel):
    """Enhanced properties for Genesis rendering"""
    # Visual properties
    color: tuple[float, float, float]  # RGB 0-1
    metallic: float  # 0-1
    roughness: float  # 0-1
    opacity: float = 1.0
    emissive: tuple[float, float, float] = (0.0, 0.0, 0.0)

    # Geometry adjustments
    scale_multiplier: tuple[float, float, float] = (1.0, 1.0, 1.0)
    suggested_dimensions: Optional[Dict[str, float]] = None  # Real-world dimensions

    # Additional details
    add_details: List[str] = []  # e.g., ["wheels", "windows", "headlights"]
    material_type: str = "generic"  # "metal", "plastic", "glass", "wood", etc.

    # Contextual info
    object_category: str = "unknown"  # "vehicle", "furniture", "building", etc.
    reasoning: str = ""  # LLM's reasoning for choices


class LLMInterpreter:
    """Interprets text descriptions and generates Genesis properties"""

    def __init__(self):
        self.client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        self.model = "gpt-4o"  # or gpt-4-turbo, gpt-4, gpt-3.5-turbo

    async def augment_object(
        self,
        shape: str,
        base_dimensions: Dict[str, float],
        description: str,
        context: Optional[str] = None
    ) -> GenesisProperties:
        """
        Augment a simple shape with semantic properties based on description

        Args:
            shape: Base shape type ("Box", "Sphere", "Cylinder", "Capsule")
            base_dimensions: Current dimensions (e.g., {"x": 2.0, "y": 1.0, "z": 4.0})
            description: User's text description (e.g., "blue corvette")
            context: Optional scene context for better interpretation

        Returns:
            GenesisProperties with enhanced rendering properties
        """

        prompt = self._build_augmentation_prompt(
            shape, base_dimensions, description, context
        )

        response = self.client.chat.completions.create(
            model=self.model,
            max_tokens=2000,
            temperature=0.3,  # Lower for consistency
            messages=[{"role": "user", "content": prompt}]
        )

        # Parse LLM response
        response_text = response.choices[0].message.content
        properties = self._parse_llm_response(response_text)

        return properties

    def _build_augmentation_prompt(
        self,
        shape: str,
        base_dimensions: Dict[str, float],
        description: str,
        context: Optional[str] = None
    ) -> str:
        """Build the prompt for the LLM"""

        return f"""You are helping create a photorealistic 3D scene. A user has placed a simple {shape} shape and wants it rendered as: "{description}"

Base shape dimensions:
{json.dumps(base_dimensions, indent=2)}

Your task: Generate PBR (Physically Based Rendering) properties to make this shape look like the described object.

Respond with a JSON object containing:
{{
  "color": [R, G, B],  // RGB values 0.0-1.0
  "metallic": 0.0-1.0,  // 0=non-metallic, 1=fully metallic
  "roughness": 0.0-1.0,  // 0=mirror smooth, 1=rough/matte
  "opacity": 0.0-1.0,    // 1=opaque, 0=transparent
  "emissive": [R, G, B], // Self-illumination (usually [0,0,0])
  "scale_multiplier": [x, y, z],  // Adjust proportions (1.0 = no change)
  "suggested_dimensions": {{"length": X, "width": Y, "height": Z}},  // Real-world meters
  "add_details": ["detail1", "detail2"],  // Visual details to emphasize
  "material_type": "metal|plastic|glass|wood|fabric|concrete|ceramic",
  "object_category": "vehicle|furniture|building|nature|electronics|sports",
  "reasoning": "Brief explanation of your choices"
}}

Examples for reference:

"blue corvette" on a Box:
{{
  "color": [0.0, 0.27, 0.67],  // Deep blue
  "metallic": 0.9,  // Car paint is metallic
  "roughness": 0.2,  // Glossy finish
  "scale_multiplier": [1.0, 0.65, 2.25],  // Car proportions (wider, lower, longer)
  "suggested_dimensions": {{"length": 4.5, "width": 1.8, "height": 1.3}},
  "add_details": ["wheels", "windows", "headlights", "spoiler"],
  "material_type": "metal",
  "object_category": "vehicle",
  "reasoning": "Corvette is a sports car with metallic blue paint, low profile, and distinctive aerodynamic shape"
}}

"light pole" on a Cylinder:
{{
  "color": [0.5, 0.5, 0.52],  // Galvanized steel gray
  "metallic": 0.7,  // Metal pole
  "roughness": 0.6,  // Weathered metal
  "scale_multiplier": [1.0, 6.0, 1.0],  // Tall and thin
  "suggested_dimensions": {{"diameter": 0.25, "height": 8.0}},
  "add_details": ["light_bulb", "base_plate", "electrical_box"],
  "material_type": "metal",
  "object_category": "building",
  "reasoning": "Street light poles are typically 8m tall, galvanized steel, with weathered finish"
}}

"wooden coffee table" on a Box:
{{
  "color": [0.55, 0.35, 0.2],  // Walnut brown
  "metallic": 0.0,  // Wood is non-metallic
  "roughness": 0.4,  // Polished wood
  "scale_multiplier": [1.2, 0.4, 0.8],  // Table proportions
  "suggested_dimensions": {{"length": 1.2, "width": 0.6, "height": 0.45}},
  "add_details": ["wood_grain", "table_legs", "surface_reflection"],
  "material_type": "wood",
  "object_category": "furniture",
  "reasoning": "Coffee tables are low, wide, with polished wood finish showing natural grain"
}}

Now generate properties for: "{description}"
Shape: {shape}
Current dimensions: {json.dumps(base_dimensions)}
{f"Scene context: {context}" if context else ""}

Respond with ONLY the JSON object, no other text.
"""

    def _parse_llm_response(self, response: str) -> GenesisProperties:
        """Parse LLM JSON response into GenesisProperties"""

        try:
            # Extract JSON from response (handle markdown code blocks)
            if "```json" in response:
                json_str = response.split("```json")[1].split("```")[0].strip()
            elif "```" in response:
                json_str = response.split("```")[1].split("```")[0].strip()
            else:
                json_str = response.strip()

            data = json.loads(json_str)

            # Convert to GenesisProperties
            return GenesisProperties(
                color=tuple(data["color"]),
                metallic=data["metallic"],
                roughness=data["roughness"],
                opacity=data.get("opacity", 1.0),
                emissive=tuple(data.get("emissive", [0.0, 0.0, 0.0])),
                scale_multiplier=tuple(data.get("scale_multiplier", [1.0, 1.0, 1.0])),
                suggested_dimensions=data.get("suggested_dimensions"),
                add_details=data.get("add_details", []),
                material_type=data.get("material_type", "generic"),
                object_category=data.get("object_category", "unknown"),
                reasoning=data.get("reasoning", "")
            )

        except Exception as e:
            print(f"Error parsing LLM response: {e}")
            print(f"Response was: {response}")

            # Return default properties on error
            return GenesisProperties(
                color=(0.7, 0.7, 0.7),
                metallic=0.2,
                roughness=0.7,
                reasoning=f"Failed to parse LLM response: {e}"
            )

    async def augment_scene(
        self,
        scene_objects,
        scene_context: Optional[str] = None
    ):
        """
        Augment all objects in a scene

        Args:
            scene_objects: Dictionary or list of objects with shape, dimensions, and description
            scene_context: Overall scene description for context

        Returns:
            Objects with added 'genesis_properties' field (same type as input)
        """

        # Handle both dict and list inputs
        is_dict = isinstance(scene_objects, dict)
        objects_to_process = scene_objects.values() if is_dict else scene_objects
        augmented_objects = {} if is_dict else []

        for obj in objects_to_process:
            # Skip if no description provided
            if not obj.get("description"):
                if is_dict:
                    augmented_objects[obj["id"]] = obj
                else:
                    augmented_objects.append(obj)
                continue

            # Extract dimensions from object
            shape = obj.get("visualProperties", {}).get("shape", "Box")
            scale = obj.get("transform", {}).get("scale", {"x": 1, "y": 1, "z": 1})

            # Get augmented properties
            properties = await self.augment_object(
                shape=shape,
                base_dimensions=scale,
                description=obj["description"],
                context=scene_context
            )

            # Add to object
            obj["genesis_properties"] = properties.model_dump()

            if is_dict:
                augmented_objects[obj["id"]] = obj
            else:
                augmented_objects.append(obj)

        return augmented_objects


# Singleton instance
_interpreter = None

def get_interpreter() -> LLMInterpreter:
    """Get or create the LLM interpreter singleton"""
    global _interpreter
    if _interpreter is None:
        _interpreter = LLMInterpreter()
    return _interpreter
</file>

<file path="backend/scene_converter.py">
"""
Scene Converter: JSON Scene Data → Genesis Entities

Converts the frontend JSON scene format to Genesis scene objects
with LLM-augmented properties
"""

import genesis as gs
from typing import Dict, List, Optional, Tuple


class SceneConverter:
    """Converts JSON scene data to Genesis entities"""

    def __init__(self, scene):
        self.scene = scene
        self.entities = []

    def convert_scene(self, scene_data: Dict) -> List:
        """
        Convert full scene data to Genesis entities

        Args:
            scene_data: JSON scene data with objects (list or dict)

        Returns:
            List of created Genesis entities
        """

        objects = scene_data.get("objects", [])
        # Handle both list and dict formats
        if isinstance(objects, dict):
            objects = list(objects.values())

        for obj_data in objects:
            entity = self.convert_object(obj_data)
            if entity:
                self.entities.append(entity)

        return self.entities

    def convert_object(self, obj_data: Dict):
        """
        Convert a single object to a Genesis entity

        Args:
            obj_data: Object data with transform, physics, visual, and genesis properties

        Returns:
            Genesis Entity or None if conversion fails
        """

        try:
            # Extract base properties
            transform = obj_data.get("transform", {})
            physics = obj_data.get("physicsProperties", {})
            visual = obj_data.get("visualProperties", {})
            genesis_props = obj_data.get("genesis_properties", {})

            # Create morph (geometry)
            morph = self._create_morph(visual, genesis_props)
            if not morph:
                return None

            # Create material (physics)
            material = self._create_material(physics)

            # Create surface (visual/PBR)
            # TODO: Fix - gs.surfaces.Surface has NotImplementedError in Genesis 0.3.7
            # surface = self._create_surface(visual, genesis_props)

            # Get position and rotation
            position = self._get_position(transform)
            rotation = self._get_rotation(transform)

            # Add entity to scene (without surface for now)
            entity = self.scene.add_entity(
                morph=morph,
                material=material,
                # surface=surface,  # Disabled - NotImplementedError
                pos=position,
                quat=rotation
            )

            return entity

        except Exception as e:
            print(f"Error converting object: {e}")
            print(f"Object data: {obj_data}")
            return None

    def _create_morph(
        self,
        visual: Dict,
        genesis_props: Dict
    ):
        """Create Genesis morph (geometry) from visual properties"""

        shape = visual.get("shape", "Box")

        # Get scale (use LLM-suggested dimensions if available, else use base scale)
        if genesis_props and genesis_props.get("suggested_dimensions"):
            dims = genesis_props["suggested_dimensions"]

            if shape == "Box":
                size = [
                    dims.get("length", dims.get("width", 1.0)),
                    dims.get("width", dims.get("length", 1.0)),
                    dims.get("height", 1.0)
                ]
            elif shape == "Sphere":
                size = dims.get("radius", dims.get("diameter", 1.0) / 2)
            elif shape == "Cylinder":
                size = {
                    "radius": dims.get("radius", dims.get("diameter", 1.0) / 2),
                    "height": dims.get("height", 1.0)
                }
            elif shape == "Capsule":
                size = {
                    "radius": dims.get("radius", dims.get("diameter", 1.0) / 2),
                    "length": dims.get("length", dims.get("height", 1.0))
                }
        else:
            # Use base scale with optional multiplier
            scale = visual.get("scale", {"x": 1, "y": 1, "z": 1})
            multiplier = genesis_props.get("scale_multiplier", [1.0, 1.0, 1.0]) if genesis_props else [1.0, 1.0, 1.0]

            if shape == "Box":
                size = [
                    scale.get("x", 1.0) * multiplier[0],
                    scale.get("y", 1.0) * multiplier[1],
                    scale.get("z", 1.0) * multiplier[2]
                ]
            elif shape == "Sphere":
                size = scale.get("x", 1.0) * multiplier[0] / 2  # radius
            elif shape == "Cylinder":
                size = {
                    "radius": scale.get("x", 1.0) * multiplier[0] / 2,
                    "height": scale.get("y", 1.0) * multiplier[1]
                }
            elif shape == "Capsule":
                size = {
                    "radius": scale.get("x", 1.0) * multiplier[0] / 2,
                    "length": scale.get("y", 1.0) * multiplier[1]
                }

        # Create appropriate morph
        if shape == "Box":
            return gs.morphs.Box(size=size)
        elif shape == "Sphere":
            return gs.morphs.Sphere(radius=size)
        elif shape == "Cylinder":
            return gs.morphs.Cylinder(radius=size["radius"], height=size["height"])
        elif shape == "Capsule":
            return gs.morphs.Capsule(radius=size["radius"], length=size["length"])
        else:
            print(f"Unsupported shape: {shape}, defaulting to Box")
            return gs.morphs.Box(size=[1.0, 1.0, 1.0])

    def _create_material(self, physics: Dict):
        """Create Genesis material (physics properties) from physics data"""

        return gs.materials.Rigid(
            rho=physics.get("mass", 1.0) * 1000,  # Convert to kg/m³
            friction=physics.get("friction", 0.5),
            # Note: Genesis 0.3.7 uses 'coup_restitution' instead of 'restitution'
            coup_restitution=physics.get("restitution", 0.3)
        )

    def _create_surface(
        self,
        visual: Dict,
        genesis_props: Dict
    ):
        """Create Genesis surface (PBR visual properties)"""

        # If we have LLM-augmented properties, use them
        if genesis_props:
            color = tuple(genesis_props.get("color", [0.7, 0.7, 0.7]))
            metallic = genesis_props.get("metallic", 0.2)
            roughness = genesis_props.get("roughness", 0.7)
            opacity = genesis_props.get("opacity", 1.0)
            emissive = tuple(genesis_props.get("emissive", [0.0, 0.0, 0.0]))
        else:
            # Fall back to basic visual properties
            color = self._hex_to_rgb(visual.get("color", "#B0B0B0"))
            metallic = 0.2
            roughness = 0.7
            opacity = 1.0
            emissive = (0.0, 0.0, 0.0)

        return gs.surfaces.Surface(
            color=color,
            metallic=metallic,
            roughness=roughness,
            opacity=opacity,
            emissive=emissive,
            smooth=True,  # Enable smooth shading
            double_sided=False
        )

    def _get_position(self, transform: Dict) -> Tuple[float, float, float]:
        """Extract position from transform"""
        pos = transform.get("position", {"x": 0, "y": 0, "z": 0})
        return (pos.get("x", 0), pos.get("y", 0), pos.get("z", 0))

    def _get_rotation(self, transform: Dict) -> Optional[Tuple[float, float, float, float]]:
        """Extract rotation quaternion from transform"""
        rot = transform.get("rotation")
        if rot and all(k in rot for k in ["x", "y", "z", "w"]):
            return (rot["x"], rot["y"], rot["z"], rot["w"])
        return None  # Use default rotation

    def _hex_to_rgb(self, hex_color: str) -> Tuple[float, float, float]:
        """Convert hex color to RGB tuple (0-1 range)"""
        hex_color = hex_color.lstrip('#')

        if len(hex_color) == 6:
            r = int(hex_color[0:2], 16) / 255.0
            g = int(hex_color[2:4], 16) / 255.0
            b = int(hex_color[4:6], 16) / 255.0
            return (r, g, b)
        else:
            return (0.7, 0.7, 0.7)  # Default gray

    def add_ground_plane(
        self,
        size: float = 50.0,
        height: float = 0.0,
        color: Tuple[float, float, float] = (0.3, 0.3, 0.3)
    ):
        """Add a ground plane to the scene"""

        ground = self.scene.add_entity(
            morph=gs.morphs.Plane(),
            material=gs.materials.Rigid(rho=1000, friction=0.8),
            surface=gs.surfaces.Surface(
                color=color,
                roughness=0.9,
                metallic=0.0
            ),
            pos=(0, height, 0)
        )

        self.entities.append(ground)
        return ground
</file>

<file path="backend/setup_auth.py">
#!/usr/bin/env python3
"""
Setup script for creating initial admin user and API key.
Run this once to set up authentication for your API.

Usage:
    python setup_auth.py
"""

import sys
import getpass
from pathlib import Path

# Add backend directory to path
sys.path.insert(0, str(Path(__file__).parent))

from database import create_user, get_user_by_username, create_api_key as db_create_api_key
from auth import get_password_hash, generate_api_key, hash_api_key


def main():
    print("=" * 60)
    print("Physics Simulator API - Authentication Setup")
    print("=" * 60)
    print()

    # Check if admin user already exists
    existing_admin = get_user_by_username("admin")
    if existing_admin:
        print("⚠️  Admin user already exists!")
        response = input("Do you want to create a new API key for the existing admin? (y/n): ")
        if response.lower() != 'y':
            print("Setup cancelled.")
            return

        user_id = existing_admin["id"]
        username = existing_admin["username"]
        print(f"\n✓ Using existing admin user: {username}")

    else:
        print("Creating a new admin user...")
        print()

        # Get admin username
        username = input("Enter admin username (default: admin): ").strip()
        if not username:
            username = "admin"

        # Get admin email
        email = input("Enter admin email: ").strip()
        while not email or "@" not in email:
            print("Please enter a valid email address.")
            email = input("Enter admin email: ").strip()

        # Get admin password
        password = getpass.getpass("Enter admin password: ")
        while len(password) < 8:
            print("Password must be at least 8 characters long.")
            password = getpass.getpass("Enter admin password: ")

        password_confirm = getpass.getpass("Confirm admin password: ")
        while password != password_confirm:
            print("Passwords do not match!")
            password = getpass.getpass("Enter admin password: ")
            password_confirm = getpass.getpass("Confirm admin password: ")

        # Create admin user
        try:
            hashed_password = get_password_hash(password)
            user_id = create_user(
                username=username,
                email=email,
                hashed_password=hashed_password,
                is_admin=True
            )
            print(f"\n✓ Admin user created successfully! (ID: {user_id})")
        except Exception as e:
            print(f"\n✗ Failed to create admin user: {e}")
            return

    # Ask if user wants to create an API key
    print()
    create_key = input("Do you want to create an API key? (y/n): ")
    if create_key.lower() == 'y':
        key_name = input("Enter a name for this API key (e.g., 'Production Key'): ").strip()
        if not key_name:
            key_name = "Default API Key"

        expires_input = input("Should this key expire? Enter days (or press Enter for no expiration): ").strip()
        expires_at = None
        if expires_input:
            try:
                expires_days = int(expires_input)
                from datetime import datetime, timedelta
                expires_at = (datetime.utcnow() + timedelta(days=expires_days)).isoformat()
                print(f"Key will expire in {expires_days} days")
            except ValueError:
                print("Invalid number of days. Key will not expire.")

        # Generate API key
        api_key = generate_api_key()
        key_hash = hash_api_key(api_key)

        try:
            key_id = db_create_api_key(
                key_hash=key_hash,
                name=key_name,
                user_id=user_id,
                expires_at=expires_at
            )
            print()
            print("=" * 60)
            print("✓ API Key created successfully!")
            print("=" * 60)
            print()
            print("IMPORTANT: Save this API key somewhere safe!")
            print("You will NOT be able to see it again.")
            print()
            print(f"API Key: {api_key}")
            print()
            print("=" * 60)
            print()
            print("Usage:")
            print("  - Add to requests as header: X-API-Key: <your-key>")
            print("  - Or use Bearer token authentication via /api/auth/login")
            print()
        except Exception as e:
            print(f"\n✗ Failed to create API key: {e}")
            return

    print()
    print("=" * 60)
    print("Setup Complete!")
    print("=" * 60)
    print()
    print(f"Admin username: {username}")
    if not existing_admin:
        print(f"Admin email: {email}")
    print()
    print("Next steps:")
    print("  1. Start the server: python main.py")
    print("  2. Test authentication:")
    print()
    print("     # Login to get JWT token")
    print(f"     curl -X POST http://localhost:8000/api/auth/login \\")
    print(f"       -H 'Content-Type: application/x-www-form-urlencoded' \\")
    print(f"       -d 'username={username}&password=YOUR_PASSWORD'")
    print()
    print("     # Or use API key")
    print("     curl http://localhost:8000/api/videos \\")
    print("       -H 'X-API-Key: YOUR_API_KEY'")
    print()


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nSetup cancelled by user.")
        sys.exit(1)
    except Exception as e:
        print(f"\n\n✗ Unexpected error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
</file>

<file path="backend/test_add_entity.py">
import genesis as gs

# Initialize
gs.init(backend=gs.cpu)

# Create scene
scene = gs.Scene(show_viewer=False)

# Try to see what the add_entity signature is
import inspect
sig = inspect.signature(scene.add_entity)
print("add_entity signature:")
print(sig)

# Try adding a simple entity
try:
    # Check if morphs exists
    print("\nChecking for morphs...")
    if hasattr(gs, 'morphs'):
        print("gs.morphs exists!")
        print(dir(gs.morphs))
    else:
        print("gs.morphs does NOT exist")

    # Check for materials
    print("\nChecking for materials...")
    if hasattr(gs, 'materials'):
        print("gs.materials exists!")
    else:
        print("gs.materials does NOT exist")

except Exception as e:
    print(f"Error: {e}")
    import traceback
    traceback.print_exc()
</file>

<file path="backend/test_genesis_api.py">
import genesis as gs

# Initialize Genesis
gs.init(backend=gs.cpu)

# Create scene
scene = gs.Scene(show_viewer=False)

# Print available methods
print("Scene methods:")
print([m for m in dir(scene) if not m.startswith('_')])

# Try to see what objects/morphs are available
print("\n\nGenesis namespace:")
print([m for m in dir(gs) if not m.startswith('_') and m[0].isupper()])
</file>

<file path="backend/test_video_models.py">
#!/usr/bin/env python3
"""Test script for video generation Pydantic models."""

from backend.models.video_generation import (
    VideoStatus, Scene, StoryboardEntry,
    GenerationRequest, VideoProgress, JobResponse
)
from datetime import datetime

def test_models():
    """Test all video generation models."""

    # Test VideoStatus enum
    print('Testing VideoStatus enum...')
    print(f'  PENDING: {VideoStatus.PENDING}')
    print(f'  COMPLETED: {VideoStatus.COMPLETED}')

    # Test Scene model
    print('\nTesting Scene model...')
    scene = Scene(
        scene_number=1,
        description='A beautiful sunset over mountains',
        duration=5.5,
        image_prompt='Cinematic shot of golden sunset over snow-capped mountains'
    )
    print(f'  Created scene: {scene.scene_number}, duration: {scene.duration}s')

    # Test GenerationRequest
    print('\nTesting GenerationRequest...')
    request = GenerationRequest(
        prompt='Create a promotional video about AI technology',
        duration=30,
        style='cinematic',
        aspect_ratio='16:9'
    )
    print(f'  Request prompt: {request.prompt[:50]}...')
    print(f'  Duration: {request.duration}s, Style: {request.style}')

    # Test VideoProgress
    print('\nTesting VideoProgress...')
    progress = VideoProgress(
        current_stage=VideoStatus.GENERATING_STORYBOARD,
        scenes_total=6,
        scenes_completed=3,
        current_scene=4,
        estimated_completion_seconds=120,
        message='Generating scene 4 of 6'
    )
    print(f'  Stage: {progress.current_stage}')
    print(f'  Progress: {progress.scenes_completed}/{progress.scenes_total}')

    # Test StoryboardEntry
    print('\nTesting StoryboardEntry...')
    entry = StoryboardEntry(
        scene=scene,
        image_url='https://example.com/scene1.jpg',
        generation_status='completed'
    )
    print(f'  Storyboard entry for scene {entry.scene.scene_number}: {entry.generation_status}')

    # Test JobResponse
    print('\nTesting JobResponse...')
    job = JobResponse(
        job_id=12345,
        status=VideoStatus.STORYBOARD_READY,
        progress=progress,
        storyboard=[entry],
        estimated_cost=15.50,
        created_at=datetime.now(),
        updated_at=datetime.now(),
        approved=False
    )
    print(f'  Job ID: {job.job_id}')
    print(f'  Status: {job.status}')
    print(f'  Estimated cost: ${job.estimated_cost}')

    # Test JSON serialization
    print('\nTesting JSON serialization...')
    json_data = job.model_dump_json(indent=2)
    print(f'  JSON output length: {len(json_data)} characters')
    print('  First 200 chars:', json_data[:200])

    # Test validation errors
    print('\nTesting validation...')
    try:
        invalid_scene = Scene(
            scene_number=0,  # Invalid: must be >= 1
            description='Test',
            duration=5.0,
            image_prompt='Test prompt'
        )
    except Exception as e:
        print(f'  ✓ Caught expected validation error for scene_number: {type(e).__name__}')

    try:
        invalid_request = GenerationRequest(
            prompt='Too short',  # Invalid: must be >= 10 chars
            duration=30
        )
    except Exception as e:
        print(f'  ✓ Caught expected validation error for prompt: {type(e).__name__}')

    try:
        invalid_ratio = GenerationRequest(
            prompt='A valid prompt that is long enough',
            duration=30,
            aspect_ratio='21:9'  # Invalid ratio
        )
    except Exception as e:
        print(f'  ✓ Caught expected validation error for aspect_ratio: {type(e).__name__}')

    print('\n✓ All models validated successfully!')

if __name__ == '__main__':
    test_models()
</file>

<file path="backend/api/v3/__init__.py">
"""
V3 API Package

This package contains the v3 API implementation for frontend alignment.
All endpoints return data in the standardized API envelope format.
"""
</file>

<file path="backend/migrations/add_luigi_state_table.py">
"""
Migration: Add Luigi task state tracking table.

This migration creates the necessary table for tracking Luigi task states
in the existing database.
"""

import sqlite3
import logging
from pathlib import Path

logger = logging.getLogger(__name__)


def migrate():
    """Create the luigi_task_state table."""
    from ..database import get_db

    logger.info("Running migration: add_luigi_state_table")

    with get_db() as conn:
        # Check if table already exists
        cursor = conn.execute(
            """
            SELECT name FROM sqlite_master
            WHERE type='table' AND name='luigi_task_state'
            """
        )

        if cursor.fetchone():
            logger.info("Table luigi_task_state already exists, skipping")
            return

        # Create the table
        conn.execute(
            """
            CREATE TABLE luigi_task_state (
                task_id VARCHAR(255) PRIMARY KEY,
                job_id INTEGER NOT NULL,
                task_name VARCHAR(100) NOT NULL,
                status VARCHAR(20) NOT NULL,
                output_data TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                completed_at TIMESTAMP,
                FOREIGN KEY (job_id) REFERENCES generated_videos(id)
            )
            """
        )

        # Create indexes for faster queries
        conn.execute(
            """
            CREATE INDEX idx_luigi_task_job_id
            ON luigi_task_state(job_id)
            """
        )

        conn.execute(
            """
            CREATE INDEX idx_luigi_task_status
            ON luigi_task_state(status)
            """
        )

        conn.execute(
            """
            CREATE INDEX idx_luigi_task_created
            ON luigi_task_state(created_at)
            """
        )

        conn.commit()

        logger.info("Created luigi_task_state table with indexes")


if __name__ == "__main__":
    # Run migration directly
    logging.basicConfig(level=logging.INFO)
    migrate()
    print("Migration completed successfully!")
</file>

<file path="backend/migrations/add_source_url.py">
"""
Migration: Add source_url column to assets table
Allows storing the original URL where assets were downloaded from
"""

import sqlite3
from pathlib import Path


def up(conn: sqlite3.Connection):
    """Add source_url column to assets table"""
    cursor = conn.cursor()

    try:
        # Add source_url column for storing original download URLs
        cursor.execute("""
            ALTER TABLE assets
            ADD COLUMN source_url TEXT
        """)

        conn.commit()
        print("✓ Added source_url column to assets table")

    except sqlite3.OperationalError as e:
        if "duplicate column name" in str(e).lower():
            print("⚠ source_url column already exists, skipping")
        else:
            raise

    cursor.close()


def down(conn: sqlite3.Connection):
    """
    Remove source_url column from assets table
    Note: SQLite doesn't support DROP COLUMN directly, would need table recreation
    """
    cursor = conn.cursor()

    # SQLite limitation: Can't drop columns easily
    # Would need to:
    # 1. Create new table without source_url
    # 2. Copy data
    # 3. Drop old table
    # 4. Rename new table

    print("⚠ Downgrade not implemented for SQLite (DROP COLUMN not supported)")
    print("  To remove source_url column, manually recreate the table")

    cursor.close()


def run_migration(db_path: str = "backend/DATA/scenes.db"):
    """Run the migration"""
    conn = sqlite3.connect(db_path)
    try:
        up(conn)
    finally:
        conn.close()


if __name__ == "__main__":
    # Run migration if executed directly
    run_migration()
</file>

<file path="backend/migrations/add_thumbnail_blob_id.py">
"""
Migration: Add thumbnail_blob_id column to assets table
Allows storing thumbnail references in the asset_blobs table
"""

import sqlite3
from pathlib import Path


def up(conn: sqlite3.Connection):
    """Add thumbnail_blob_id column to assets table"""
    cursor = conn.cursor()

    try:
        # Add thumbnail_blob_id column for storing thumbnail blob references
        cursor.execute("""
            ALTER TABLE assets
            ADD COLUMN thumbnail_blob_id TEXT
        """)

        conn.commit()
        print("✓ Added thumbnail_blob_id column to assets table")

    except sqlite3.OperationalError as e:
        if "duplicate column name" in str(e).lower():
            print("⚠ thumbnail_blob_id column already exists, skipping")
        else:
            raise

    cursor.close()


def down(conn: sqlite3.Connection):
    """
    Remove thumbnail_blob_id column from assets table
    Note: SQLite doesn't support DROP COLUMN directly, would need table recreation
    """
    cursor = conn.cursor()

    # SQLite limitation: Can't drop columns easily
    # Would need to:
    # 1. Create new table without thumbnail_blob_id
    # 2. Copy data
    # 3. Drop old table
    # 4. Rename new table

    print("⚠ Downgrade not implemented for SQLite (DROP COLUMN not supported)")
    print("  To remove thumbnail_blob_id column, manually recreate the table")

    cursor.close()


def run_migration(db_path: str = "backend/DATA/scenes.db"):
    """Run the migration"""
    conn = sqlite3.connect(db_path)
    try:
        up(conn)
    finally:
        conn.close()


if __name__ == "__main__":
    # Run migration if executed directly
    run_migration()
</file>

<file path="backend/migrations/consolidate_assets_table.py">
"""
Database migration: Consolidate all asset tables into single 'assets' table

This migration:
1. Drops old tables: uploaded_assets, client_assets, campaign_assets
2. Creates new consolidated 'assets' table with discriminated union support
3. Adds indexes for efficient querying

Run this migration to implement the new asset model.
"""

import sqlite3
from pathlib import Path

def run_migration(db_path: str = None):
    """Execute the asset consolidation migration"""
    if db_path is None:
        # Use the same database path as database_helpers.py
        from pathlib import Path
        import os
        DATA_DIR = Path(os.getenv("DATA", "./DATA"))
        db_path = str(DATA_DIR / "scenes.db")
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()

    try:
        print("Starting asset consolidation migration...")

        # Step 1: Drop old asset tables
        print("Dropping old asset tables...")
        cursor.execute("DROP TABLE IF EXISTS uploaded_assets")
        cursor.execute("DROP TABLE IF EXISTS client_assets")
        cursor.execute("DROP TABLE IF EXISTS campaign_assets")

        # Step 2: Create new consolidated assets table
        print("Creating new consolidated assets table...")
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS assets (
                id TEXT PRIMARY KEY,
                user_id INTEGER,
                client_id TEXT NOT NULL,
                campaign_id TEXT,
                name TEXT NOT NULL,
                asset_type TEXT NOT NULL,
                url TEXT NOT NULL,
                size INTEGER,
                uploaded_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
                format TEXT NOT NULL,
                tags TEXT,
                width INTEGER,
                height INTEGER,
                duration INTEGER,
                thumbnail_url TEXT,
                waveform_url TEXT,
                page_count INTEGER,
                FOREIGN KEY (user_id) REFERENCES users(id) ON DELETE CASCADE,
                FOREIGN KEY (client_id) REFERENCES clients(id) ON DELETE CASCADE,
                FOREIGN KEY (campaign_id) REFERENCES campaigns(id) ON DELETE CASCADE
            )
        """)

        # Step 3: Create indexes for efficient querying
        print("Creating indexes...")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_assets_user_id ON assets(user_id)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_assets_client_id ON assets(client_id)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_assets_campaign_id ON assets(campaign_id)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_assets_type ON assets(asset_type)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_assets_uploaded_at ON assets(uploaded_at)")

        # Commit changes
        conn.commit()
        print("✅ Asset consolidation migration completed successfully!")

    except Exception as e:
        conn.rollback()
        print(f"❌ Migration failed: {e}")
        raise
    finally:
        conn.close()

if __name__ == "__main__":
    # Run migration
    run_migration()
</file>

<file path="backend/services/musicgen_client.py">
"""
MusicGen client for generating scene-appropriate music with continuation.

Uses Meta's MusicGen model via Replicate to create progressive audio
that builds across multiple scenes using the continuation feature.
"""

import logging
import requests
import time
from typing import Dict, Any, Optional, List
from pathlib import Path
import tempfile
import os

logger = logging.getLogger(__name__)


class MusicGenClient:
    """Client for generating music with Meta's MusicGen model."""

    # MusicGen model on Replicate
    MUSICGEN_MODEL = "meta/musicgen"

    def __init__(self, api_key: Optional[str] = None):
        """
        Initialize MusicGen client.

        Args:
            api_key: Replicate API key (defaults to env var REPLICATE_API_KEY)
        """
        self.api_key = api_key or os.getenv("REPLICATE_API_KEY")
        if not self.api_key:
            raise ValueError("REPLICATE_API_KEY not found in environment or parameters")

        self.base_url = "https://api.replicate.com/v1"
        self.session = requests.Session()
        self.session.headers.update({
            "Authorization": f"Token {self.api_key}",
            "Content-Type": "application/json",
        })

        logger.info("MusicGen client initialized")

    def generate_initial_scene_audio(
        self,
        prompt: str,
        duration: int = 4,
        model_version: str = "stereo-large",
        temperature: float = 1.0,
        top_k: int = 250,
        top_p: float = 0.0,
    ) -> Dict[str, Any]:
        """
        Generate the first audio clip for Scene 1.

        Args:
            prompt: Music generation prompt (mood, style, instruments)
            duration: Duration in seconds (default: 4)
            model_version: MusicGen model variant (stereo-large, stereo-melody-large, etc.)
            temperature: Sampling temperature (0.0-1.5)
            top_k: Top-k sampling parameter
            top_p: Top-p sampling parameter

        Returns:
            dict: {
                "success": bool,
                "audio_url": str,
                "duration": int,
                "prediction_id": str,
                "error": str or None
            }
        """
        logger.info(f"Generating initial scene audio: '{prompt}' ({duration}s)")

        try:
            input_params = {
                "prompt": prompt,
                "duration": duration,
                "model_version": model_version,
                "temperature": temperature,
                "top_k": top_k,
                "top_p": top_p,
                "output_format": "mp3",
                "normalization_strategy": "peak",
            }

            # Create prediction
            response = self.session.post(
                f"{self.base_url}/predictions",
                json={
                    "version": self._get_model_version(),
                    "input": input_params,
                },
                timeout=30,
            )
            response.raise_for_status()

            prediction_data = response.json()
            prediction_id = prediction_data.get("id")

            if not prediction_id:
                return {
                    "success": False,
                    "audio_url": None,
                    "duration": 0,
                    "prediction_id": None,
                    "error": "No prediction ID returned from MusicGen API",
                }

            logger.info(f"MusicGen prediction started: {prediction_id}")

            # Poll for completion
            result = self._poll_prediction(prediction_id, timeout=300)

            if result["status"] == "succeeded":
                audio_url = result.get("output")
                logger.info(f"Initial audio generated successfully: {audio_url}")
                return {
                    "success": True,
                    "audio_url": audio_url,
                    "duration": duration,
                    "prediction_id": prediction_id,
                    "error": None,
                }
            else:
                error_msg = result.get("error", f"Generation failed with status: {result['status']}")
                logger.error(f"MusicGen generation failed: {error_msg}")
                return {
                    "success": False,
                    "audio_url": None,
                    "duration": 0,
                    "prediction_id": prediction_id,
                    "error": error_msg,
                }

        except Exception as e:
            logger.error(f"MusicGen initial generation error: {e}", exc_info=True)
            return {
                "success": False,
                "audio_url": None,
                "duration": 0,
                "prediction_id": None,
                "error": str(e),
            }

    def continue_scene_audio(
        self,
        prompt: str,
        input_audio_url: str,
        duration: int = 4,
        model_version: str = "stereo-large",
        temperature: float = 1.0,
        top_k: int = 250,
        top_p: float = 0.0,
    ) -> Dict[str, Any]:
        """
        Generate continuation audio for subsequent scenes.

        This takes the previous scene's audio and extends it with new music
        matching the next scene's mood/style.

        Args:
            prompt: Music generation prompt for this scene
            input_audio_url: URL of previous scene's audio to continue from
            duration: Duration to add (default: 4)
            model_version: MusicGen model variant
            temperature: Sampling temperature
            top_k: Top-k sampling parameter
            top_p: Top-p sampling parameter

        Returns:
            dict: {
                "success": bool,
                "audio_url": str (combined previous + new audio),
                "duration": int (total duration),
                "prediction_id": str,
                "error": str or None
            }
        """
        logger.info(f"Generating continuation audio: '{prompt}' ({duration}s)")
        logger.info(f"Input audio: {input_audio_url}")

        try:
            input_params = {
                "prompt": prompt,
                "input_audio": input_audio_url,  # Previous scene's audio
                "duration": duration,  # Additional duration to add
                "model_version": model_version,
                "temperature": temperature,
                "top_k": top_k,
                "top_p": top_p,
                "output_format": "mp3",
                "normalization_strategy": "peak",
                "continuation": True,  # Enable continuation mode
                "continuation_start": 0,  # Continue from end of input
            }

            # Create prediction
            response = self.session.post(
                f"{self.base_url}/predictions",
                json={
                    "version": self._get_model_version(),
                    "input": input_params,
                },
                timeout=30,
            )
            response.raise_for_status()

            prediction_data = response.json()
            prediction_id = prediction_data.get("id")

            if not prediction_id:
                return {
                    "success": False,
                    "audio_url": None,
                    "duration": 0,
                    "prediction_id": None,
                    "error": "No prediction ID returned from MusicGen API",
                }

            logger.info(f"MusicGen continuation started: {prediction_id}")

            # Poll for completion
            result = self._poll_prediction(prediction_id, timeout=300)

            if result["status"] == "succeeded":
                audio_url = result.get("output")
                logger.info(f"Continuation audio generated: {audio_url}")
                return {
                    "success": True,
                    "audio_url": audio_url,
                    "duration": duration,  # Just the added duration
                    "prediction_id": prediction_id,
                    "error": None,
                }
            else:
                error_msg = result.get("error", f"Continuation failed with status: {result['status']}")
                logger.error(f"MusicGen continuation failed: {error_msg}")
                return {
                    "success": False,
                    "audio_url": None,
                    "duration": 0,
                    "prediction_id": prediction_id,
                    "error": error_msg,
                }

        except Exception as e:
            logger.error(f"MusicGen continuation error: {e}", exc_info=True)
            return {
                "success": False,
                "audio_url": None,
                "duration": 0,
                "prediction_id": None,
                "error": str(e),
            }

    def generate_progressive_audio(
        self,
        scene_prompts: List[Dict[str, Any]],
        duration_per_scene: int = 4,
    ) -> Dict[str, Any]:
        """
        Generate progressive audio across all scenes using continuation.

        This creates a seamless audio track that evolves through all scenes.

        Args:
            scene_prompts: List of scene dictionaries with 'music_prompt' field
            duration_per_scene: Seconds of audio per scene (default: 4)

        Returns:
            dict: {
                "success": bool,
                "final_audio_url": str (full audio for all scenes),
                "total_duration": int,
                "scene_audio_urls": List[str] (audio after each scene),
                "error": str or None
            }

        Example scene_prompts:
            [
                {"scene_number": 1, "music_prompt": "Cinematic ambient, gentle water"},
                {"scene_number": 2, "music_prompt": "Soft luxurious bedroom ambiance"},
                ...
            ]
        """
        logger.info(f"Generating progressive audio for {len(scene_prompts)} scenes")

        scene_audio_urls = []
        current_audio_url = None

        try:
            for i, scene in enumerate(scene_prompts, 1):
                music_prompt = scene.get("music_prompt", "Cinematic background music")

                logger.info(f"Scene {i}/{len(scene_prompts)}: {music_prompt}")

                if i == 1:
                    # First scene: generate initial audio
                    result = self.generate_initial_scene_audio(
                        prompt=music_prompt,
                        duration=duration_per_scene,
                    )
                else:
                    # Subsequent scenes: continue from previous audio
                    result = self.continue_scene_audio(
                        prompt=music_prompt,
                        input_audio_url=current_audio_url,
                        duration=duration_per_scene,
                    )

                if not result["success"]:
                    logger.error(f"Failed to generate audio for scene {i}: {result['error']}")
                    return {
                        "success": False,
                        "final_audio_url": None,
                        "total_duration": 0,
                        "scene_audio_urls": scene_audio_urls,
                        "error": f"Scene {i} audio generation failed: {result['error']}",
                    }

                current_audio_url = result["audio_url"]
                scene_audio_urls.append(current_audio_url)

                logger.info(f"Scene {i} audio complete. Cumulative URL: {current_audio_url}")

            # Final audio URL contains all scenes
            total_duration = len(scene_prompts) * duration_per_scene

            logger.info(f"Progressive audio generation complete. Total: {total_duration}s")
            return {
                "success": True,
                "final_audio_url": current_audio_url,  # Last continuation has all scenes
                "total_duration": total_duration,
                "scene_audio_urls": scene_audio_urls,
                "error": None,
            }

        except Exception as e:
            logger.error(f"Progressive audio generation failed: {e}", exc_info=True)
            return {
                "success": False,
                "final_audio_url": None,
                "total_duration": 0,
                "scene_audio_urls": scene_audio_urls,
                "error": str(e),
            }

    def _get_model_version(self) -> str:
        """Get the MusicGen model version hash from Replicate."""
        # This would need to be updated periodically or fetched from API
        # For now, using a placeholder
        return "b05b1dff1d8c6dc63d14b0cdb42135378dcb87f6373b0d3d341ede46e59e2b38"

    def _poll_prediction(
        self,
        prediction_id: str,
        timeout: int = 300,
        poll_interval: int = 2,
    ) -> Dict[str, Any]:
        """
        Poll prediction until completion or timeout.

        Args:
            prediction_id: Replicate prediction ID
            timeout: Max seconds to wait
            poll_interval: Seconds between polls

        Returns:
            dict: Final prediction data with status and output
        """
        start_time = time.time()

        while time.time() - start_time < timeout:
            response = self.session.get(
                f"{self.base_url}/predictions/{prediction_id}",
                timeout=10,
            )
            response.raise_for_status()

            data = response.json()
            status = data.get("status")

            if status in ["succeeded", "failed", "canceled"]:
                return data

            logger.debug(f"Prediction {prediction_id} status: {status}")
            time.sleep(poll_interval)

        # Timeout
        logger.error(f"Prediction {prediction_id} timed out after {timeout}s")
        return {
            "status": "timeout",
            "error": f"Prediction timed out after {timeout} seconds",
        }
</file>

<file path="backend/services/property_photo_selector.py">
"""
Property Photo Selection Service for Luxury Lodging Video Generation.

This service uses xAI Grok to intelligently select optimal image pairs
from crawled property photos based on 7 predefined scene types for
luxury hospitality marketing videos.
"""

import logging
from typing import List, Dict, Any, Optional, Tuple
from .xai_client import XAIClient

logger = logging.getLogger(__name__)

# Scene type definitions for luxury lodging videos
LUXURY_LODGING_SCENE_TYPES = [
    {
        "scene_number": 1,
        "scene_type": "Grand Arrival",
        "duration": 5,
        "purpose": "Establish property exterior, architectural style, sense of arrival",
        "visual_priority": "Wide establishing shots, impressive architecture, welcoming entrance",
        "mood": "Aspirational, luxurious, inviting",
        "ideal_tags": ["exterior", "architecture", "entrance", "facade", "driveway", "landscape"],
        "first_image_guidance": "Distant/approach view showing full property context",
        "last_image_guidance": "Closer view of entrance or architectural detail",
        "transition_goal": "Viewer feels they're 'arriving' at the property"
    },
    {
        "scene_number": 2,
        "scene_type": "Refined Interiors",
        "duration": 5,
        "purpose": "Showcase lobby/common areas, interior design excellence",
        "visual_priority": "Sophisticated design, attention to detail, spatial luxury",
        "mood": "Elegant, refined, comfortable",
        "ideal_tags": ["lobby", "interior", "lounge", "reception", "design", "furniture", "chandelier"],
        "first_image_guidance": "Wide interior shot showing overall ambiance",
        "last_image_guidance": "Design detail or inviting seating area",
        "transition_goal": "Smooth flow through interior spaces"
    },
    {
        "scene_number": 3,
        "scene_type": "Guest Room Sanctuary",
        "duration": 5,
        "purpose": "Highlight guest room luxury, comfort, amenities",
        "visual_priority": "Bed quality, room spaciousness, view from room, premium finishes",
        "mood": "Serene, comfortable, intimate luxury",
        "ideal_tags": ["bedroom", "suite", "bed", "room_view", "balcony", "bathroom", "amenities"],
        "first_image_guidance": "Room overview showing bed and overall layout",
        "last_image_guidance": "View from room or bathroom luxury detail",
        "transition_goal": "Convey comfort and private retreat feeling"
    },
    {
        "scene_number": 4,
        "scene_type": "Culinary Excellence",
        "duration": 5,
        "purpose": "Feature dining experiences, food quality, restaurant ambiance",
        "visual_priority": "Plated dishes, dining atmosphere, chef craftsmanship",
        "mood": "Sophisticated, appetizing, experiential",
        "ideal_tags": ["restaurant", "dining", "food", "plating", "bar", "wine", "chef", "cuisine"],
        "first_image_guidance": "Restaurant ambiance or beautifully plated dish",
        "last_image_guidance": "Dining detail or culinary presentation",
        "transition_goal": "Evoke desire and culinary anticipation"
    },
    {
        "scene_number": 5,
        "scene_type": "Wellness & Recreation",
        "duration": 5,
        "purpose": "Showcase amenities (pool, spa, fitness, activities)",
        "visual_priority": "Premium facilities, relaxation spaces, activity options",
        "mood": "Rejuvenating, active, leisurely",
        "ideal_tags": ["pool", "spa", "fitness", "yoga", "massage", "wellness", "recreation", "activities"],
        "first_image_guidance": "Wide shot of pool/spa area",
        "last_image_guidance": "Close-up of wellness detail or activity",
        "transition_goal": "Communicate relaxation and vitality"
    },
    {
        "scene_number": 6,
        "scene_type": "Unique Experiences",
        "duration": 5,
        "purpose": "Highlight distinctive property features, local culture, special offerings",
        "visual_priority": "Differentiators, authentic experiences, memorable moments",
        "mood": "Distinctive, experiential, authentic",
        "ideal_tags": ["experience", "unique", "culture", "local", "activity", "sunset", "beach", "nature"],
        "first_image_guidance": "Signature experience or unique property feature",
        "last_image_guidance": "Guest enjoying experience or stunning natural context",
        "transition_goal": "Showcase what makes this property special"
    },
    {
        "scene_number": 7,
        "scene_type": "Lasting Impression",
        "duration": 5,
        "purpose": "Close with memorable hero shot, emotional resonance",
        "visual_priority": "Stunning vista, iconic property angle, sunset/golden hour",
        "mood": "Inspirational, memorable, desire-to-return",
        "ideal_tags": ["sunset", "view", "landscape", "ocean", "mountains", "hero_shot", "panorama"],
        "first_image_guidance": "Beautiful wide shot of property in context",
        "last_image_guidance": "Ultimate 'hero shot' - most stunning property moment",
        "transition_goal": "Leave viewer inspired and wanting to book"
    }
]


class PropertyPhotoSelector:
    """Selects optimal image pairs from property photos using AI."""

    def __init__(self, xai_api_key: Optional[str] = None):
        """
        Initialize the property photo selector.

        Args:
            xai_api_key: Optional xAI API key. If not provided, uses env variable.
        """
        self.xai_client = XAIClient(api_key=xai_api_key)

    def select_scene_image_pairs(
        self,
        property_info: Dict[str, Any],
        photos: List[Dict[str, Any]],
        scene_types: Optional[List[Dict[str, Any]]] = None
    ) -> Dict[str, Any]:
        """
        Select optimal image pairs for each scene type from property photos.

        Args:
            property_info: Dict with keys:
                - name: Property name
                - location: Property location
                - property_type: Type of property (e.g., "boutique hotel")
                - positioning: Brand positioning (e.g., "eco-luxury")
            photos: List of photo dicts with keys:
                - id: Unique photo identifier
                - filename: Photo filename
                - url: URL to photo
                - tags: List of tags (optional)
                - dominant_colors: List of colors (optional)
                - detected_objects: List of objects (optional)
                - composition: Composition style (optional)
                - lighting: Lighting conditions (optional)
                - resolution: Resolution (optional)
                - aspect_ratio: Aspect ratio (optional)
            scene_types: Optional custom scene types. Defaults to luxury lodging scenes.

        Returns:
            Dict with structure:
            {
                "property_name": str,
                "selection_metadata": {...},
                "scene_pairs": [
                    {
                        "scene_number": int,
                        "scene_type": str,
                        "first_image": {...},
                        "last_image": {...},
                        "transition_analysis": {...}
                    }
                ],
                "recommendations": {...}
            }
        """
        if not photos or len(photos) < 14:
            raise ValueError(f"Need at least 14 photos to select 7 pairs, got {len(photos)}")

        scene_types = scene_types or LUXURY_LODGING_SCENE_TYPES

        if len(scene_types) != 7:
            raise ValueError(f"Expected 7 scene types, got {len(scene_types)}")

        logger.info(
            f"Selecting image pairs for property '{property_info.get('name')}' "
            f"from {len(photos)} photos across {len(scene_types)} scenes"
        )

        # Call Grok with custom prompt
        try:
            result = self.xai_client.select_property_scene_pairs(
                property_info=property_info,
                photos=photos,
                scene_types=scene_types
            )

            # Validate result
            if not result.get("scene_pairs"):
                raise ValueError("Grok returned no scene pairs")

            if len(result["scene_pairs"]) != 7:
                logger.warning(
                    f"Expected 7 scene pairs, got {len(result['scene_pairs'])}. "
                    "Filling missing scenes with best available photos."
                )

            # Ensure we have exactly 7 pairs
            result["scene_pairs"] = self._ensure_seven_pairs(
                result["scene_pairs"],
                photos,
                scene_types
            )

            # Validate no duplicate images
            self._validate_no_duplicates(result["scene_pairs"])

            logger.info(
                f"Successfully selected {len(result['scene_pairs'])} scene pairs "
                f"with confidence {result.get('selection_metadata', {}).get('selection_confidence', 'unknown')}"
            )

            return result

        except Exception as e:
            logger.error(f"Failed to select property scene pairs: {e}", exc_info=True)
            raise

    def _ensure_seven_pairs(
        self,
        scene_pairs: List[Dict[str, Any]],
        photos: List[Dict[str, Any]],
        scene_types: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        Ensure exactly 7 scene pairs exist, filling gaps with fallback selections.

        Args:
            scene_pairs: Existing scene pairs from Grok
            photos: Available photos
            scene_types: Scene type definitions

        Returns:
            List of exactly 7 scene pairs
        """
        if len(scene_pairs) == 7:
            return scene_pairs

        # Get scene numbers that are missing
        existing_scene_nums = {pair["scene_number"] for pair in scene_pairs}
        missing_scene_nums = set(range(1, 8)) - existing_scene_nums

        # Get IDs of already used photos
        used_photo_ids = set()
        for pair in scene_pairs:
            used_photo_ids.add(pair["first_image"]["id"])
            used_photo_ids.add(pair["last_image"]["id"])

        # Fill missing scenes with available photos
        available_photos = [p for p in photos if p["id"] not in used_photo_ids]

        for scene_num in sorted(missing_scene_nums):
            if len(available_photos) < 2:
                logger.warning(f"Not enough photos to fill scene {scene_num}")
                break

            scene_type = scene_types[scene_num - 1]

            # Simple fallback: take first two available photos
            first_photo = available_photos.pop(0)
            last_photo = available_photos.pop(0)

            scene_pairs.append({
                "scene_number": scene_num,
                "scene_type": scene_type["scene_type"],
                "first_image": {
                    "id": first_photo["id"],
                    "filename": first_photo.get("filename", ""),
                    "reasoning": "Fallback selection - insufficient Grok coverage",
                    "quality_score": 7.0,
                    "tag_match_score": 7.0
                },
                "last_image": {
                    "id": last_photo["id"],
                    "filename": last_photo.get("filename", ""),
                    "reasoning": "Fallback selection - insufficient Grok coverage",
                    "quality_score": 7.0,
                    "tag_match_score": 7.0
                },
                "transition_analysis": {
                    "color_compatibility": "unknown",
                    "lighting_consistency": "unknown",
                    "compositional_flow": "fallback",
                    "interpolation_confidence": 7.0
                }
            })

            used_photo_ids.add(first_photo["id"])
            used_photo_ids.add(last_photo["id"])

        # Sort by scene number
        scene_pairs.sort(key=lambda x: x["scene_number"])

        return scene_pairs

    def _validate_no_duplicates(self, scene_pairs: List[Dict[str, Any]]) -> None:
        """
        Validate that no image is used more than once across all scene pairs.

        Args:
            scene_pairs: List of scene pair dicts

        Raises:
            ValueError: If duplicate images are found
        """
        used_images = set()

        for pair in scene_pairs:
            first_id = pair["first_image"]["id"]
            last_id = pair["last_image"]["id"]

            if first_id in used_images:
                raise ValueError(f"Duplicate image detected: {first_id} used in multiple scenes")
            if last_id in used_images:
                raise ValueError(f"Duplicate image detected: {last_id} used in multiple scenes")

            used_images.add(first_id)
            used_images.add(last_id)

        logger.debug(f"Validated {len(used_images)} unique images across {len(scene_pairs)} scene pairs")

    def convert_to_video_generation_format(
        self,
        selection_result: Dict[str, Any]
    ) -> List[Tuple[str, str, float, str]]:
        """
        Convert Grok's selection result to video generation format.

        Args:
            selection_result: Result from select_scene_image_pairs()

        Returns:
            List of tuples: (image1_id, image2_id, score, reasoning)
            Ordered by scene number (1-7)
        """
        pairs = []

        for scene_pair in selection_result["scene_pairs"]:
            image1_id = scene_pair["first_image"]["id"]
            image2_id = scene_pair["last_image"]["id"]

            # Use interpolation confidence as score
            score = scene_pair.get("transition_analysis", {}).get("interpolation_confidence", 8.0) / 10.0

            # Combine reasoning from both images and transition
            reasoning = (
                f"Scene {scene_pair['scene_number']}: {scene_pair['scene_type']}. "
                f"{scene_pair['first_image'].get('reasoning', '')} → "
                f"{scene_pair['last_image'].get('reasoning', '')} "
                f"[{scene_pair.get('transition_analysis', {}).get('compositional_flow', 'unknown')}]"
            )

            pairs.append((image1_id, image2_id, score, reasoning))

        return pairs
</file>

<file path="backend/services/scene_audio_generator.py">
"""
Scene Audio Generator Service.

This module handles generating continuous audio tracks from scene prompts
using MusicGen with continuation features. Supports building cohesive
music tracks that match video scene sequences.
"""

import logging
import asyncio
import tempfile
import os
import base64
from typing import List, Optional, Dict, Any, Tuple
import requests

from ..config import get_settings
from ..database import save_generated_audio

logger = logging.getLogger(__name__)
settings = get_settings()


class SceneAudioGenerator:
    """Generates continuous audio tracks from scene prompts using MusicGen continuation"""

    def __init__(self, api_key: Optional[str] = None):
        self.api_key = api_key or getattr(settings, "REPLICATE_API_TOKEN", None)
        if not self.api_key:
            raise ValueError("REPLICATE_API_TOKEN must be set")

    async def generate_scene_audio(
        self,
        scenes: List[Dict[str, Any]],
        default_duration: float = 4.0,
        model_id: str = "meta/musicgen",
    ) -> bytes:
        """
        Generate continuous audio track from scene prompts using continuation.

        Args:
            scenes: List of scene dictionaries with 'scene_number', 'prompt', 'duration'
            default_duration: Default duration per scene in seconds
            model_id: MusicGen model to use

        Returns:
            Combined audio data as bytes
        """
        if not scenes:
            raise ValueError("No scenes provided")

        logger.info(f"Generating audio for {len(scenes)} scenes using {model_id}")

        combined_audio = None
        total_duration = 0

        for i, scene in enumerate(scenes):
            scene_num = scene.get("scene_number", i + 1)
            prompt = scene.get("prompt", "")
            duration = scene.get("duration", default_duration) or default_duration

            logger.info(f"Processing scene {scene_num}: {prompt[:50]}... ({duration}s)")

            if i == 0:
                # Generate initial clip
                audio_data = await self._generate_initial_clip(
                    prompt, duration, model_id
                )
            else:
                # Continue from previous audio
                if combined_audio is None:
                    raise ValueError(
                        "Cannot continue audio without previous audio data"
                    )
                audio_data = await self._continue_audio(
                    combined_audio, prompt, duration, total_duration, model_id
                )

            # Combine with previous audio
            if combined_audio:
                combined_audio = self._concatenate_audio(combined_audio, audio_data)
            else:
                combined_audio = audio_data

            total_duration += duration
            logger.info(
                f"Scene {scene_num} processed. Total duration: {total_duration}s"
            )

        logger.info(f"Audio generation complete. Final duration: {total_duration}s")
        return combined_audio

    async def _generate_initial_clip(
        self, prompt: str, duration: float, model_id: str
    ) -> bytes:
        """Generate the first audio clip without continuation"""
        payload = {
            "version": "671ac645ce5e552cc63a54a2bbff63fcf798043055d2dac5fc9e36a837eedcfb",
            "input": {
                "model_version": "stereo-melody-large",
                "prompt": prompt,
                "duration": duration,
                "output_format": "mp3",
            },
        }

        return await self._call_replicate_api(payload)

    async def _continue_audio(
        self,
        input_audio: bytes,
        prompt: str,
        duration: float,
        continuation_start: float,
        model_id: str,
    ) -> bytes:
        """Continue audio generation from existing clip"""
        # Encode input audio as base64
        input_audio_b64 = base64.b64encode(input_audio).decode("utf-8")

        payload = {
            "version": "671ac645ce5e552cc63a54a2bbff63fcf798043055d2dac5fc9e36a837eedcfb",
            "input": {
                "model_version": "stereo-melody-large",
                "prompt": prompt,
                "duration": duration,
                "input_audio": input_audio_b64,
                "continuation": True,
                "continuation_start": continuation_start,
                "continuation_end": continuation_start + duration,
                "output_format": "mp3",
            },
        }

        return await self._call_replicate_api(payload)

    async def _call_replicate_api(self, payload: Dict[str, Any]) -> bytes:
        """Call Replicate API and return audio data"""
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }

        try:
            # Create prediction
            response = requests.post(
                "https://api.replicate.com/v1/predictions",
                json=payload,
                headers=headers,
                timeout=30,
            )
            response.raise_for_status()
            prediction = response.json()

            prediction_id = prediction["id"]
            logger.info(f"Created prediction {prediction_id}")

            # Poll for completion
            audio_url = await self._poll_prediction(prediction_id)

            # Download audio
            audio_response = requests.get(audio_url, timeout=60)
            audio_response.raise_for_status()

            logger.info(f"Downloaded audio: {len(audio_response.content)} bytes")
            return audio_response.content

        except Exception as e:
            logger.error(f"Replicate API call failed: {e}")
            raise

    async def _poll_prediction(self, prediction_id: str, max_attempts: int = 60) -> str:
        """Poll prediction until complete and return audio URL"""
        headers = {"Authorization": f"Bearer {self.api_key}"}

        for attempt in range(max_attempts):
            try:
                response = requests.get(
                    f"https://api.replicate.com/v1/predictions/{prediction_id}",
                    headers=headers,
                    timeout=10,
                )
                response.raise_for_status()
                prediction = response.json()

                status = prediction.get("status")

                if status == "succeeded":
                    # Extract audio URL from output
                    output = prediction.get("output")
                    if isinstance(output, str):
                        return output
                    elif isinstance(output, list) and output:
                        return output[0]
                    else:
                        raise ValueError(f"Unexpected output format: {output}")

                elif status == "failed":
                    error = prediction.get("error", "Unknown error")
                    raise Exception(f"Prediction failed: {error}")

                elif status == "canceled":
                    raise Exception("Prediction was canceled")

                # Still processing, wait and retry
                await asyncio.sleep(2)

            except Exception as e:
                logger.warning(f"Polling attempt {attempt + 1} failed: {e}")
                await asyncio.sleep(2)

        raise Exception(
            f"Prediction {prediction_id} did not complete within {max_attempts * 2} seconds"
        )

    def _concatenate_audio(self, audio1: bytes, audio2: bytes) -> bytes:
        """Concatenate two MP3 files using ffmpeg"""
        import subprocess

        with (
            tempfile.NamedTemporaryFile(suffix=".mp3", delete=False) as f1,
            tempfile.NamedTemporaryFile(suffix=".mp3", delete=False) as f2,
            tempfile.NamedTemporaryFile(suffix=".mp3", delete=False) as out,
        ):
            try:
                # Write input files
                f1.write(audio1)
                f2.write(audio2)
                f1.flush()
                f2.flush()

                # Use ffmpeg to concatenate
                cmd = [
                    "ffmpeg",
                    "-y",
                    "-i",
                    f1.name,
                    "-i",
                    f2.name,
                    "-filter_complex",
                    "[0:0][1:0]concat=n=2:v=0:a=1[outa]",
                    "-map",
                    "[outa]",
                    "-c:a",
                    "libmp3lame",
                    "-q:a",
                    "2",  # High quality
                    out.name,
                ]

                result = subprocess.run(cmd, capture_output=True, text=True, timeout=60)

                if result.returncode != 0:
                    logger.error(f"ffmpeg concatenation failed: {result.stderr}")
                    raise Exception(f"Audio concatenation failed: {result.stderr}")

                # Read output
                with open(out.name, "rb") as f:
                    return f.read()

            finally:
                # Clean up temp files
                for f in [f1, f2, out]:
                    try:
                        os.unlink(f.name)
                    except:
                        pass


async def generate_scene_audio_track(
    scenes: List[Dict[str, Any]],
    default_duration: float = 4.0,
    model_id: str = "meta/musicgen",
    user_id: Optional[int] = None,
) -> Dict[str, Any]:
    """
    High-level function to generate scene-based audio track.

    Returns dict with audio_id, audio_url, etc.
    """
    generator = SceneAudioGenerator()
    audio_data = await generator.generate_scene_audio(
        scenes, default_duration, model_id
    )

    # Save to database
    total_duration = sum(
        s.get("duration", default_duration) or default_duration for s in scenes
    )
    audio_id = save_generated_audio(
        prompt=f"Scene-based audio: {len(scenes)} scenes",
        audio_url="",  # Will be served from blob
        model_id=model_id,
        parameters={"scenes": len(scenes), "default_duration": default_duration},
        duration=total_duration,
    )

    total_duration = sum(
        s.get("duration", default_duration) or default_duration for s in scenes
    )

    return {
        "audio_id": audio_id,
        "audio_url": f"/api/audio/{audio_id}/data",
        "total_duration": total_duration,
        "scenes_processed": len(scenes),
        "model_used": model_id,
    }
</file>

<file path="backend/services/scene_generator.py">
"""
Scene Generation Service.

This module handles AI-powered scene generation for video ad creation.
Uses LLM to generate scene descriptions, scripts, and shot suggestions.
"""

import logging
import json
import os
from typing import List, Dict, Any, Optional
from openai import OpenAI

# Configure logging
logger = logging.getLogger(__name__)

# Configuration from environment
AI_PROVIDER = os.getenv("AI_PROVIDER", "openai")  # openai, anthropic, etc.
AI_API_KEY = os.getenv("OPENAI_API_KEY")  # Default to OpenAI key
AI_MODEL = os.getenv("AI_MODEL", "gpt-4o-mini")  # Default model
SCENES_PER_VIDEO_MIN = int(os.getenv("SCENES_PER_VIDEO_MIN", "3"))
SCENES_PER_VIDEO_MAX = int(os.getenv("SCENES_PER_VIDEO_MAX", "7"))
DEFAULT_VIDEO_DURATION = float(os.getenv("DEFAULT_VIDEO_DURATION", "30.0"))


class SceneGenerationError(Exception):
    """Exception raised when scene generation fails."""
    pass


def generate_scenes(
    ad_basics: Dict[str, Any],
    creative_direction: Dict[str, Any],
    assets: Optional[List[str]] = None,
    duration: Optional[float] = None,
    num_scenes: Optional[int] = None
) -> List[Dict[str, Any]]:
    """
    Generate scenes for a video ad using AI/LLM.

    Args:
        ad_basics: Ad basics containing product, target audience, key message, CTA
        creative_direction: Creative direction with style, tone, visual elements
        assets: Optional list of asset IDs to incorporate
        duration: Total video duration in seconds (default: 30.0)
        num_scenes: Number of scenes to generate (default: auto-determine)

    Returns:
        List of scene dictionaries with structure:
        {
            "sceneNumber": int,
            "duration": float,
            "description": str,
            "script": str,
            "shotType": str,
            "transition": str,
            "assets": List[str],
            "metadata": dict
        }

    Raises:
        SceneGenerationError: If generation fails
    """
    logger.info("Generating scenes with AI/LLM")

    # Validate inputs
    if not ad_basics:
        raise SceneGenerationError("ad_basics is required")

    # Set defaults
    video_duration = duration or DEFAULT_VIDEO_DURATION
    target_num_scenes = num_scenes or _calculate_optimal_scenes(video_duration)

    # Ensure target within bounds
    target_num_scenes = max(SCENES_PER_VIDEO_MIN, min(target_num_scenes, SCENES_PER_VIDEO_MAX))

    # Build prompt for LLM
    prompt = _build_scene_generation_prompt(
        ad_basics=ad_basics,
        creative_direction=creative_direction,
        assets=assets,
        video_duration=video_duration,
        num_scenes=target_num_scenes
    )

    # Call AI provider
    try:
        if AI_PROVIDER == "openai":
            scenes = _generate_scenes_openai(prompt, target_num_scenes)
        else:
            raise SceneGenerationError(f"Unsupported AI provider: {AI_PROVIDER}")

        # Post-process and validate scenes
        scenes = _post_process_scenes(scenes, video_duration, assets or [])

        logger.info(f"Successfully generated {len(scenes)} scenes")
        return scenes

    except Exception as e:
        logger.error(f"Scene generation failed: {e}")
        raise SceneGenerationError(f"Failed to generate scenes: {str(e)}")


def regenerate_scene(
    scene_number: int,
    original_scene: Dict[str, Any],
    all_scenes: List[Dict[str, Any]],
    ad_basics: Dict[str, Any],
    creative_direction: Dict[str, Any],
    feedback: Optional[str] = None,
    constraints: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """
    Regenerate a single scene with optional user feedback.

    Args:
        scene_number: The scene number to regenerate (1-indexed)
        original_scene: The original scene data
        all_scenes: All scenes for context
        ad_basics: Ad basics for context
        creative_direction: Creative direction for context
        feedback: Optional user feedback (e.g., "make it more energetic")
        constraints: Optional constraints (e.g., {"duration": 10.0})

    Returns:
        Updated scene dictionary

    Raises:
        SceneGenerationError: If regeneration fails
    """
    logger.info(f"Regenerating scene {scene_number} with feedback: {feedback}")

    # Build regeneration prompt
    prompt = _build_scene_regeneration_prompt(
        scene_number=scene_number,
        original_scene=original_scene,
        all_scenes=all_scenes,
        ad_basics=ad_basics,
        creative_direction=creative_direction,
        feedback=feedback,
        constraints=constraints
    )

    try:
        if AI_PROVIDER == "openai":
            new_scene = _regenerate_scene_openai(prompt, original_scene)
        else:
            raise SceneGenerationError(f"Unsupported AI provider: {AI_PROVIDER}")

        # Apply constraints
        if constraints:
            if "duration" in constraints:
                new_scene["duration"] = constraints["duration"]

        logger.info(f"Successfully regenerated scene {scene_number}")
        return new_scene

    except Exception as e:
        logger.error(f"Scene regeneration failed: {e}")
        raise SceneGenerationError(f"Failed to regenerate scene: {str(e)}")


def _calculate_optimal_scenes(duration: float) -> int:
    """Calculate optimal number of scenes based on video duration."""
    if duration <= 15:
        return 3
    elif duration <= 30:
        return 4
    elif duration <= 45:
        return 5
    elif duration <= 60:
        return 6
    else:
        return 7


def _build_scene_generation_prompt(
    ad_basics: Dict[str, Any],
    creative_direction: Dict[str, Any],
    assets: Optional[List[str]],
    video_duration: float,
    num_scenes: int
) -> str:
    """Build the prompt for initial scene generation."""
    prompt = f"""Generate a {num_scenes}-scene storyboard for a {video_duration}-second video advertisement.

**Ad Basics:**
- Product: {ad_basics.get('product', 'N/A')}
- Target Audience: {ad_basics.get('targetAudience', 'N/A')}
- Key Message: {ad_basics.get('keyMessage', 'N/A')}
- Call to Action: {ad_basics.get('callToAction', 'N/A')}

**Creative Direction:**
- Style: {creative_direction.get('style', 'Modern')}
- Tone: {creative_direction.get('tone', 'Professional')}
- Visual Elements: {', '.join(creative_direction.get('visualElements', [])) if creative_direction.get('visualElements') else 'N/A'}
- Music Style: {creative_direction.get('musicStyle', 'Upbeat')}

**Available Assets:**
{len(assets) if assets else 0} assets provided for use

**Requirements:**
- Each scene should have a clear description
- Include suggested script/voiceover text
- Specify shot type (wide, medium, close-up, product, etc.)
- Specify transition (cut, fade, dissolve, etc.)
- Scenes should flow naturally and tell a cohesive story
- Total duration should sum to approximately {video_duration} seconds
- Each scene should be 4-10 seconds

Return ONLY a valid JSON array with this exact structure (no markdown, no explanation):
[
  {{
    "sceneNumber": 1,
    "duration": 6.0,
    "description": "detailed scene description",
    "script": "voiceover text",
    "shotType": "wide|medium|close-up|product",
    "transition": "cut|fade|dissolve",
    "metadata": {{
      "setting": "description",
      "mood": "description"
    }}
  }}
]"""
    return prompt


def _build_scene_regeneration_prompt(
    scene_number: int,
    original_scene: Dict[str, Any],
    all_scenes: List[Dict[str, Any]],
    ad_basics: Dict[str, Any],
    creative_direction: Dict[str, Any],
    feedback: Optional[str],
    constraints: Optional[Dict[str, Any]]
) -> str:
    """Build the prompt for scene regeneration."""
    context_before = [s for s in all_scenes if s["sceneNumber"] < scene_number]
    context_after = [s for s in all_scenes if s["sceneNumber"] > scene_number]

    prompt = f"""Regenerate scene {scene_number} for a video advertisement.

**Original Scene:**
```json
{json.dumps(original_scene, indent=2)}
```

**Context (Previous Scenes):**
{json.dumps(context_before[-2:], indent=2) if context_before else 'N/A'}

**Context (Following Scenes):**
{json.dumps(context_after[:2], indent=2) if context_after else 'N/A'}

**Ad Basics:**
- Product: {ad_basics.get('product')}
- Key Message: {ad_basics.get('keyMessage')}

**Creative Direction:**
- Style: {creative_direction.get('style')}
- Tone: {creative_direction.get('tone')}

**User Feedback:**
{feedback or 'No specific feedback - just provide fresh variation'}

**Constraints:**
{json.dumps(constraints, indent=2) if constraints else 'Maintain similar duration and flow'}

**Requirements:**
- Maintain continuity with surrounding scenes
- Apply user feedback if provided
- Keep the same scene structure
- Ensure natural flow with adjacent scenes

Return ONLY a valid JSON object with this exact structure (no markdown, no explanation):
{{
  "sceneNumber": {scene_number},
  "duration": 6.0,
  "description": "updated scene description",
  "script": "updated voiceover text",
  "shotType": "wide|medium|close-up|product",
  "transition": "cut|fade|dissolve",
  "metadata": {{
    "setting": "description",
    "mood": "description"
  }}
}}"""
    return prompt


def _generate_scenes_openai(prompt: str, num_scenes: int) -> List[Dict[str, Any]]:
    """Generate scenes using OpenAI API."""
    if not AI_API_KEY:
        raise SceneGenerationError("OPENAI_API_KEY not configured")

    try:
        client = OpenAI(api_key=AI_API_KEY)

        response = client.chat.completions.create(
            model=AI_MODEL,
            messages=[
                {
                    "role": "system",
                    "content": "You are an expert video storyboard creator. Return ONLY valid JSON arrays/objects with no markdown formatting or explanations."
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            temperature=0.7,
            max_tokens=2000,
            response_format={"type": "json_object"} if "gpt-4" in AI_MODEL or "gpt-3.5" in AI_MODEL else None
        )

        content = response.choices[0].message.content
        logger.debug(f"OpenAI response: {content[:200]}...")

        # Parse JSON response
        try:
            # Try direct parse first
            scenes = json.loads(content)

            # If response is wrapped in an object, extract the array
            if isinstance(scenes, dict):
                # Look for array keys
                for key in ["scenes", "storyboard", "data", "result"]:
                    if key in scenes and isinstance(scenes[key], list):
                        scenes = scenes[key]
                        break

            if not isinstance(scenes, list):
                raise ValueError("Response is not a list")

        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse OpenAI response as JSON: {e}")
            logger.error(f"Response content: {content}")
            raise SceneGenerationError("AI response was not valid JSON")

        return scenes

    except Exception as e:
        logger.error(f"OpenAI API error: {e}")
        raise SceneGenerationError(f"OpenAI API error: {str(e)}")


def _regenerate_scene_openai(prompt: str, original_scene: Dict[str, Any]) -> Dict[str, Any]:
    """Regenerate a single scene using OpenAI API."""
    if not AI_API_KEY:
        raise SceneGenerationError("OPENAI_API_KEY not configured")

    try:
        client = OpenAI(api_key=AI_API_KEY)

        response = client.chat.completions.create(
            model=AI_MODEL,
            messages=[
                {
                    "role": "system",
                    "content": "You are an expert video storyboard creator. Return ONLY valid JSON objects with no markdown formatting or explanations."
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            temperature=0.8,  # Higher temperature for more variation
            max_tokens=1000
        )

        content = response.choices[0].message.content
        logger.debug(f"OpenAI regeneration response: {content[:200]}...")

        # Parse JSON response
        scene = json.loads(content)

        # Ensure scene has required fields
        if not isinstance(scene, dict):
            raise ValueError("Response is not a dictionary")

        return scene

    except Exception as e:
        logger.error(f"OpenAI API error during regeneration: {e}")
        raise SceneGenerationError(f"OpenAI API error: {str(e)}")


def _post_process_scenes(
    scenes: List[Dict[str, Any]],
    target_duration: float,
    available_assets: List[str]
) -> List[Dict[str, Any]]:
    """Post-process and validate generated scenes."""
    processed_scenes = []

    # Calculate total duration
    total_duration = sum(s.get("duration", 0) for s in scenes)

    for i, scene in enumerate(scenes):
        # Ensure scene number
        scene["sceneNumber"] = i + 1

        # Adjust duration proportionally if needed
        if total_duration > 0:
            duration_ratio = target_duration / total_duration
            scene["duration"] = round(scene.get("duration", 5.0) * duration_ratio, 1)
        else:
            scene["duration"] = round(target_duration / len(scenes), 1)

        # Ensure required fields
        scene.setdefault("description", f"Scene {i + 1}")
        scene.setdefault("script", "")
        scene.setdefault("shotType", "medium")
        scene.setdefault("transition", "cut" if i > 0 else "fade")
        scene.setdefault("assets", [])
        scene.setdefault("metadata", {})

        # Assign assets if available (simple distribution for now)
        if available_assets and not scene["assets"]:
            asset_index = i % len(available_assets)
            scene["assets"] = [available_assets[asset_index]]

        processed_scenes.append(scene)

    return processed_scenes
</file>

<file path="backend/tests/__init__.py">
"""Tests for video ad generation backend."""
</file>

<file path="backend/tests/test_scene_endpoints.py">
"""
Integration tests for scene management API endpoints.

Tests the V3 scene management endpoints including:
- List scenes for a job
- Get individual scene details
- Update scene properties
- Regenerate scene with AI
- Delete scene
"""

import pytest
import json
from fastapi.testclient import TestClient
from unittest.mock import patch, MagicMock
from backend.main import app
from backend.database_helpers import (
    create_job_scene,
    delete_scenes_by_job,
    get_scenes_by_job
)


client = TestClient(app)


class TestSceneEndpoints:
    """Integration tests for scene management endpoints."""

    @pytest.fixture
    def auth_headers(self):
        """Mock authentication headers."""
        return {"Authorization": "Bearer test-token"}

    @pytest.fixture
    def sample_job_id(self):
        """Sample job ID for testing."""
        return "123"

    @pytest.fixture
    def sample_scenes(self, sample_job_id):
        """Create sample scenes in database for testing."""
        # Clean up any existing scenes
        delete_scenes_by_job(int(sample_job_id))

        # Create test scenes
        scene_ids = []
        scenes_data = [
            {
                "job_id": int(sample_job_id),
                "scene_number": 1,
                "duration": 7.0,
                "description": "Opening shot of pristine nature",
                "script": "Imagine a world without plastic waste",
                "shot_type": "wide",
                "transition": "fade",
                "assets": ["asset-001"],
                "metadata": {"mood": "inspiring"}
            },
            {
                "job_id": int(sample_job_id),
                "scene_number": 2,
                "duration": 10.0,
                "description": "Product showcase",
                "script": "Meet EcoBottle",
                "shot_type": "close-up",
                "transition": "cut",
                "assets": ["asset-002", "asset-003"],
                "metadata": {"mood": "energetic"}
            },
            {
                "job_id": int(sample_job_id),
                "scene_number": 3,
                "duration": 13.0,
                "description": "Lifestyle integration",
                "script": "Stay hydrated, stay sustainable",
                "shot_type": "medium",
                "transition": "dissolve",
                "assets": ["asset-004"],
                "metadata": {"mood": "uplifting"}
            }
        ]

        for scene_data in scenes_data:
            scene_id = create_job_scene(**scene_data)
            scene_ids.append(scene_id)

        yield scene_ids

        # Cleanup
        delete_scenes_by_job(int(sample_job_id))

    @patch('backend.api.v3.router.verify_auth')
    def test_list_scenes_success(self, mock_auth, auth_headers, sample_job_id, sample_scenes):
        """Test listing all scenes for a job."""
        mock_auth.return_value = {"id": 1, "email": "test@example.com"}

        response = client.get(
            f"/api/v3/jobs/{sample_job_id}/scenes",
            headers=auth_headers
        )

        assert response.status_code == 200
        data = response.json()
        assert data["success"] is True
        assert "scenes" in data["data"]
        assert len(data["data"]["scenes"]) == 3
        assert data["data"]["scenes"][0]["sceneNumber"] == 1
        assert data["data"]["scenes"][0]["duration"] == 7.0

    @patch('backend.api.v3.router.verify_auth')
    def test_list_scenes_empty_job(self, mock_auth, auth_headers):
        """Test listing scenes for job with no scenes."""
        mock_auth.return_value = {"id": 1, "email": "test@example.com"}

        response = client.get(
            "/api/v3/jobs/999/scenes",
            headers=auth_headers
        )

        assert response.status_code == 200
        data = response.json()
        assert data["success"] is True
        assert len(data["data"]["scenes"]) == 0

    @patch('backend.api.v3.router.verify_auth')
    def test_get_scene_success(self, mock_auth, auth_headers, sample_job_id, sample_scenes):
        """Test getting a specific scene by ID."""
        mock_auth.return_value = {"id": 1, "email": "test@example.com"}
        scene_id = sample_scenes[0]

        response = client.get(
            f"/api/v3/jobs/{sample_job_id}/scenes/{scene_id}",
            headers=auth_headers
        )

        assert response.status_code == 200
        data = response.json()
        assert data["success"] is True
        assert data["data"]["id"] == scene_id
        assert data["data"]["sceneNumber"] == 1
        assert data["data"]["description"] == "Opening shot of pristine nature"

    @patch('backend.api.v3.router.verify_auth')
    def test_get_scene_not_found(self, mock_auth, auth_headers, sample_job_id):
        """Test getting non-existent scene."""
        mock_auth.return_value = {"id": 1, "email": "test@example.com"}

        response = client.get(
            f"/api/v3/jobs/{sample_job_id}/scenes/non-existent-id",
            headers=auth_headers
        )

        assert response.status_code == 200
        data = response.json()
        assert data["success"] is False
        assert "not found" in data["error"].lower()

    @patch('backend.api.v3.router.verify_auth')
    def test_get_scene_wrong_job(self, mock_auth, auth_headers, sample_scenes):
        """Test getting scene with mismatched job ID."""
        mock_auth.return_value = {"id": 1, "email": "test@example.com"}
        scene_id = sample_scenes[0]

        response = client.get(
            f"/api/v3/jobs/999/scenes/{scene_id}",
            headers=auth_headers
        )

        assert response.status_code == 200
        data = response.json()
        assert data["success"] is False
        assert "does not belong" in data["error"].lower()

    @patch('backend.api.v3.router.verify_auth')
    def test_update_scene_success(self, mock_auth, auth_headers, sample_job_id, sample_scenes):
        """Test updating scene properties."""
        mock_auth.return_value = {"id": 1, "email": "test@example.com"}
        scene_id = sample_scenes[1]

        update_data = {
            "description": "Updated product showcase with emotional appeal",
            "script": "Meet EcoBottle - your partner in sustainability",
            "duration": 12.0,
            "shotType": "medium"
        }

        response = client.put(
            f"/api/v3/jobs/{sample_job_id}/scenes/{scene_id}",
            headers=auth_headers,
            json=update_data
        )

        assert response.status_code == 200
        data = response.json()
        assert data["success"] is True
        assert data["data"]["description"] == update_data["description"]
        assert data["data"]["script"] == update_data["script"]
        assert data["data"]["duration"] == update_data["duration"]
        assert data["data"]["shotType"] == update_data["shotType"]

    @patch('backend.api.v3.router.verify_auth')
    def test_update_scene_partial(self, mock_auth, auth_headers, sample_job_id, sample_scenes):
        """Test partial update of scene (only some fields)."""
        mock_auth.return_value = {"id": 1, "email": "test@example.com"}
        scene_id = sample_scenes[0]

        update_data = {
            "script": "Updated script only"
        }

        response = client.put(
            f"/api/v3/jobs/{sample_job_id}/scenes/{scene_id}",
            headers=auth_headers,
            json=update_data
        )

        assert response.status_code == 200
        data = response.json()
        assert data["success"] is True
        assert data["data"]["script"] == "Updated script only"
        # Original fields should remain unchanged
        assert data["data"]["description"] == "Opening shot of pristine nature"

    @patch('backend.api.v3.router.regenerate_scene')
    @patch('backend.api.v3.router.get_job')
    @patch('backend.api.v3.router.verify_auth')
    def test_regenerate_scene_success(self, mock_auth, mock_get_job, mock_regenerate, auth_headers, sample_job_id, sample_scenes):
        """Test regenerating a scene with AI."""
        mock_auth.return_value = {"id": 1, "email": "test@example.com"}
        scene_id = sample_scenes[1]

        # Mock job data
        mock_get_job.return_value = {
            "id": int(sample_job_id),
            "parameters": json.dumps({
                "ad_basics": {
                    "product": "EcoBottle",
                    "targetAudience": "Millennials",
                    "keyMessage": "Save the planet",
                    "callToAction": "Shop Now"
                },
                "creative": {
                    "direction": {
                        "style": "modern",
                        "tone": "inspiring"
                    }
                }
            })
        }

        # Mock regenerated scene
        mock_regenerate.return_value = {
            "sceneNumber": 2,
            "duration": 10.0,
            "description": "Enhanced product showcase",
            "script": "Regenerated script with more emotional impact",
            "shotType": "medium",
            "transition": "dissolve",
            "assets": ["asset-002"]
        }

        regenerate_request = {
            "feedback": "Make it more emotional and impactful",
            "constraints": {"duration": 10.0}
        }

        response = client.post(
            f"/api/v3/jobs/{sample_job_id}/scenes/{scene_id}/regenerate",
            headers=auth_headers,
            json=regenerate_request
        )

        assert response.status_code == 200
        data = response.json()
        assert data["success"] is True
        assert "Enhanced" in data["data"]["description"]
        mock_regenerate.assert_called_once()

    @patch('backend.api.v3.router.verify_auth')
    def test_delete_scene_success(self, mock_auth, auth_headers, sample_job_id, sample_scenes):
        """Test deleting a scene."""
        mock_auth.return_value = {"id": 1, "email": "test@example.com"}
        scene_id = sample_scenes[2]

        response = client.delete(
            f"/api/v3/jobs/{sample_job_id}/scenes/{scene_id}",
            headers=auth_headers
        )

        assert response.status_code == 200
        data = response.json()
        assert data["success"] is True
        assert "deleted successfully" in data["data"]["message"].lower()

        # Verify scene was deleted
        scenes = get_scenes_by_job(int(sample_job_id))
        assert len(scenes) == 2

    @patch('backend.api.v3.router.verify_auth')
    def test_delete_scene_not_found(self, mock_auth, auth_headers, sample_job_id):
        """Test deleting non-existent scene."""
        mock_auth.return_value = {"id": 1, "email": "test@example.com"}

        response = client.delete(
            f"/api/v3/jobs/{sample_job_id}/scenes/non-existent-id",
            headers=auth_headers
        )

        assert response.status_code == 200
        data = response.json()
        assert data["success"] is False
        assert "not found" in data["error"].lower()

    @patch('backend.api.v3.router.verify_auth')
    def test_scene_operations_require_auth(self, mock_auth, sample_job_id, sample_scenes):
        """Test that scene endpoints require authentication."""
        mock_auth.side_effect = Exception("Unauthorized")

        # Test all endpoints without auth
        endpoints = [
            ("GET", f"/api/v3/jobs/{sample_job_id}/scenes"),
            ("GET", f"/api/v3/jobs/{sample_job_id}/scenes/{sample_scenes[0]}"),
            ("PUT", f"/api/v3/jobs/{sample_job_id}/scenes/{sample_scenes[0]}"),
            ("POST", f"/api/v3/jobs/{sample_job_id}/scenes/{sample_scenes[0]}/regenerate"),
            ("DELETE", f"/api/v3/jobs/{sample_job_id}/scenes/{sample_scenes[0]}")
        ]

        for method, endpoint in endpoints:
            if method == "GET":
                response = client.get(endpoint)
            elif method == "PUT":
                response = client.put(endpoint, json={})
            elif method == "POST":
                response = client.post(endpoint, json={})
            elif method == "DELETE":
                response = client.delete(endpoint)

            # Should fail due to auth error
            assert response.status_code in [401, 500]  # Depending on how auth is handled

    @patch('backend.api.v3.router.verify_auth')
    def test_update_scene_with_metadata(self, mock_auth, auth_headers, sample_job_id, sample_scenes):
        """Test updating scene with custom metadata."""
        mock_auth.return_value = {"id": 1, "email": "test@example.com"}
        scene_id = sample_scenes[0]

        update_data = {
            "metadata": {
                "mood": "dramatic",
                "color_palette": "warm",
                "music_cue": "uplifting-strings"
            }
        }

        response = client.put(
            f"/api/v3/jobs/{sample_job_id}/scenes/{scene_id}",
            headers=auth_headers,
            json=update_data
        )

        assert response.status_code == 200
        data = response.json()
        assert data["success"] is True
        assert data["data"]["metadata"]["mood"] == "dramatic"
        assert "color_palette" in data["data"]["metadata"]


class TestJobStatusWithScenes:
    """Test job status endpoint includes scenes."""

    @pytest.fixture
    def sample_job_with_scenes(self, sample_job_id, sample_scenes):
        """Job with scenes for testing."""
        return sample_job_id

    @patch('backend.api.v3.router.get_job')
    @patch('backend.api.v3.router.verify_auth')
    def test_job_status_includes_scenes(self, mock_auth, mock_get_job, auth_headers):
        """Test that GET /api/v3/jobs/{id} includes scenes."""
        mock_auth.return_value = {"id": 1, "email": "test@example.com"}

        job_id = "123"
        mock_get_job.return_value = {
            "id": int(job_id),
            "status": "storyboard_ready",
            "progress": None,
            "video_url": None,
            "error_message": None,
            "estimated_cost": 5.0,
            "actual_cost": None,
            "created_at": "2025-11-19T22:00:00Z",
            "updated_at": "2025-11-19T22:05:00Z",
            "storyboard_data": None,
            "parameters": "{}"
        }

        # Create test scenes
        delete_scenes_by_job(int(job_id))
        create_job_scene(
            job_id=int(job_id),
            scene_number=1,
            duration=15.0,
            description="Test scene",
            script="Test script"
        )

        response = client.get(
            f"/api/v3/jobs/{job_id}",
            headers=auth_headers
        )

        assert response.status_code == 200
        data = response.json()
        assert data["success"] is True
        assert "scenes" in data["data"]
        assert len(data["data"]["scenes"]) == 1
        assert data["data"]["scenes"][0]["sceneNumber"] == 1

        # Cleanup
        delete_scenes_by_job(int(job_id))


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
</file>

<file path="backend/tests/test_scene_generation.py">
"""
Unit tests for scene generation service.

Tests the AI-powered scene generation functionality including:
- Scene generation from ad basics and creative direction
- Scene count calculation based on duration
- Scene regeneration with feedback
- Error handling and validation
"""

import pytest
import json
from unittest.mock import Mock, patch, MagicMock
from backend.services.scene_generator import (
    generate_scenes,
    regenerate_scene,
    SceneGenerationError,
    _calculate_optimal_scenes,
    _build_scene_generation_prompt,
    _post_process_scenes
)


class TestSceneGeneration:
    """Test suite for scene generation functionality."""

    @pytest.fixture
    def sample_ad_basics(self):
        """Sample ad basics data for testing."""
        return {
            "product": "EcoBottle - Sustainable Water Bottle",
            "targetAudience": "Environmentally conscious millennials",
            "keyMessage": "Stay hydrated, save the planet",
            "callToAction": "Shop Now at ecobottle.com"
        }

    @pytest.fixture
    def sample_creative_direction(self):
        """Sample creative direction data for testing."""
        return {
            "style": "modern",
            "tone": "inspiring",
            "visualElements": ["nature", "lifestyle", "product shots"]
        }

    @pytest.fixture
    def sample_assets(self):
        """Sample asset IDs for testing."""
        return [
            "asset-001-product-shot",
            "asset-002-lifestyle",
            "asset-003-nature"
        ]

    def test_calculate_optimal_scenes_30_seconds(self):
        """Test optimal scene calculation for 30-second video."""
        result = _calculate_optimal_scenes(30.0)
        assert result >= 3
        assert result <= 7
        assert isinstance(result, int)

    def test_calculate_optimal_scenes_15_seconds(self):
        """Test optimal scene calculation for 15-second video."""
        result = _calculate_optimal_scenes(15.0)
        assert result >= 3
        assert result <= 5

    def test_calculate_optimal_scenes_60_seconds(self):
        """Test optimal scene calculation for 60-second video."""
        result = _calculate_optimal_scenes(60.0)
        assert result >= 5
        assert result <= 7

    @patch('backend.services.scene_generator.OpenAI')
    def test_generate_scenes_success(self, mock_openai, sample_ad_basics, sample_creative_direction, sample_assets):
        """Test successful scene generation."""
        # Mock OpenAI response
        mock_client = MagicMock()
        mock_openai.return_value = mock_client

        mock_response = MagicMock()
        mock_response.choices = [MagicMock()]
        mock_response.choices[0].message.content = json.dumps({
            "scenes": [
                {
                    "sceneNumber": 1,
                    "duration": 5.0,
                    "description": "Opening shot of pristine nature",
                    "script": "Imagine a world without plastic waste",
                    "shotType": "wide",
                    "transition": "fade",
                    "assets": ["asset-003-nature"]
                },
                {
                    "sceneNumber": 2,
                    "duration": 8.0,
                    "description": "Product showcase in natural setting",
                    "script": "Meet EcoBottle, your sustainable companion",
                    "shotType": "close-up",
                    "transition": "cut",
                    "assets": ["asset-001-product-shot"]
                },
                {
                    "sceneNumber": 3,
                    "duration": 7.0,
                    "description": "Lifestyle shot with product",
                    "script": "Stay hydrated, stay sustainable",
                    "shotType": "medium",
                    "transition": "dissolve",
                    "assets": ["asset-002-lifestyle"]
                },
                {
                    "sceneNumber": 4,
                    "duration": 6.0,
                    "description": "Environmental impact message",
                    "script": "Every bottle saves 100 plastic bottles from landfills",
                    "shotType": "wide",
                    "transition": "fade",
                    "assets": ["asset-003-nature"]
                },
                {
                    "sceneNumber": 5,
                    "duration": 4.0,
                    "description": "Call to action",
                    "script": "Shop Now at ecobottle.com",
                    "shotType": "close-up",
                    "transition": "cut",
                    "assets": ["asset-001-product-shot"]
                }
            ]
        })
        mock_client.chat.completions.create.return_value = mock_response

        # Generate scenes
        scenes = generate_scenes(
            ad_basics=sample_ad_basics,
            creative_direction=sample_creative_direction,
            assets=sample_assets,
            duration=30.0
        )

        # Assertions
        assert len(scenes) == 5
        assert all(isinstance(s, dict) for s in scenes)
        assert all('sceneNumber' in s for s in scenes)
        assert all('duration' in s for s in scenes)
        assert all('description' in s for s in scenes)
        assert scenes[0]['sceneNumber'] == 1
        assert sum(s['duration'] for s in scenes) == 30.0

    @patch('backend.services.scene_generator.OpenAI')
    def test_generate_scenes_with_auto_scene_count(self, mock_openai, sample_ad_basics, sample_creative_direction):
        """Test scene generation with automatic scene count calculation."""
        mock_client = MagicMock()
        mock_openai.return_value = mock_client

        mock_response = MagicMock()
        mock_response.choices = [MagicMock()]
        mock_response.choices[0].message.content = json.dumps({
            "scenes": [
                {"sceneNumber": i, "duration": 10.0, "description": f"Scene {i}",
                 "script": f"Script {i}", "shotType": "medium", "transition": "cut"}
                for i in range(1, 4)
            ]
        })
        mock_client.chat.completions.create.return_value = mock_response

        scenes = generate_scenes(
            ad_basics=sample_ad_basics,
            creative_direction=sample_creative_direction,
            duration=30.0,
            num_scenes=None  # Auto-calculate
        )

        assert len(scenes) >= 3
        mock_client.chat.completions.create.assert_called_once()

    def test_generate_scenes_missing_ad_basics(self):
        """Test that missing ad_basics raises error."""
        with pytest.raises(SceneGenerationError, match="ad_basics is required"):
            generate_scenes(
                ad_basics=None,
                creative_direction={},
                duration=30.0
            )

    @patch('backend.services.scene_generator.OpenAI')
    def test_regenerate_scene_success(self, mock_openai, sample_ad_basics, sample_creative_direction):
        """Test successful scene regeneration with feedback."""
        mock_client = MagicMock()
        mock_openai.return_value = mock_client

        original_scene = {
            "sceneNumber": 2,
            "duration": 8.0,
            "description": "Product showcase",
            "script": "Original script",
            "shotType": "close-up",
            "transition": "cut"
        }

        all_scenes = [
            {"sceneNumber": 1, "duration": 7.0, "description": "Scene 1"},
            original_scene,
            {"sceneNumber": 3, "duration": 15.0, "description": "Scene 3"}
        ]

        mock_response = MagicMock()
        mock_response.choices = [MagicMock()]
        mock_response.choices[0].message.content = json.dumps({
            "sceneNumber": 2,
            "duration": 8.0,
            "description": "Enhanced product showcase with emotional appeal",
            "script": "Regenerated script with more impact",
            "shotType": "medium",
            "transition": "dissolve",
            "assets": []
        })
        mock_client.chat.completions.create.return_value = mock_response

        regenerated = regenerate_scene(
            scene_number=2,
            original_scene=original_scene,
            all_scenes=all_scenes,
            ad_basics=sample_ad_basics,
            creative_direction=sample_creative_direction,
            feedback="Make it more emotional and impactful",
            constraints={"duration": 8.0}
        )

        assert regenerated['sceneNumber'] == 2
        assert regenerated['duration'] == 8.0
        assert 'Enhanced' in regenerated['description']
        assert regenerated['script'] != original_scene['script']
        mock_client.chat.completions.create.assert_called_once()

    def test_post_process_scenes_duration_adjustment(self):
        """Test that post-processing adjusts scene durations to match total."""
        scenes = [
            {"sceneNumber": 1, "duration": 10.0, "description": "Scene 1", "assets": []},
            {"sceneNumber": 2, "duration": 10.0, "description": "Scene 2", "assets": []},
            {"sceneNumber": 3, "duration": 10.0, "description": "Scene 3", "assets": []}
        ]

        processed = _post_process_scenes(scenes, target_duration=25.0, available_assets=[])

        # Total duration should be close to target
        total_duration = sum(s['duration'] for s in processed)
        assert abs(total_duration - 25.0) < 0.5  # Allow small rounding differences
        assert len(processed) == 3

    def test_post_process_scenes_asset_distribution(self):
        """Test that post-processing distributes assets if scenes have none."""
        scenes = [
            {"sceneNumber": 1, "duration": 10.0, "description": "Scene 1", "assets": []},
            {"sceneNumber": 2, "duration": 10.0, "description": "Scene 2", "assets": []},
            {"sceneNumber": 3, "duration": 10.0, "description": "Scene 3", "assets": []}
        ]

        available_assets = ["asset-001", "asset-002", "asset-003"]
        processed = _post_process_scenes(scenes, target_duration=30.0, available_assets=available_assets)

        # Check that at least some assets were distributed
        total_assets = sum(len(s.get('assets', [])) for s in processed)
        assert total_assets > 0

    def test_build_scene_generation_prompt(self, sample_ad_basics, sample_creative_direction, sample_assets):
        """Test that prompt building includes all necessary information."""
        prompt = _build_scene_generation_prompt(
            ad_basics=sample_ad_basics,
            creative_direction=sample_creative_direction,
            assets=sample_assets,
            video_duration=30.0,
            num_scenes=5
        )

        # Check that prompt contains key information
        assert sample_ad_basics['product'] in prompt
        assert sample_ad_basics['targetAudience'] in prompt
        assert sample_ad_basics['keyMessage'] in prompt
        assert str(30.0) in prompt or '30' in prompt
        assert '5' in prompt
        assert sample_creative_direction['style'] in prompt

    @patch('backend.services.scene_generator.OpenAI')
    def test_generate_scenes_openai_error(self, mock_openai, sample_ad_basics, sample_creative_direction):
        """Test handling of OpenAI API errors."""
        mock_client = MagicMock()
        mock_openai.return_value = mock_client
        mock_client.chat.completions.create.side_effect = Exception("API Error")

        with pytest.raises(SceneGenerationError):
            generate_scenes(
                ad_basics=sample_ad_basics,
                creative_direction=sample_creative_direction,
                duration=30.0
            )

    @patch('backend.services.scene_generator.OpenAI')
    def test_generate_scenes_invalid_json_response(self, mock_openai, sample_ad_basics, sample_creative_direction):
        """Test handling of invalid JSON in OpenAI response."""
        mock_client = MagicMock()
        mock_openai.return_value = mock_client

        mock_response = MagicMock()
        mock_response.choices = [MagicMock()]
        mock_response.choices[0].message.content = "Invalid JSON"
        mock_client.chat.completions.create.return_value = mock_response

        with pytest.raises(SceneGenerationError):
            generate_scenes(
                ad_basics=sample_ad_basics,
                creative_direction=sample_creative_direction,
                duration=30.0
            )

    def test_scene_number_validation(self):
        """Test that scene numbers are sequential."""
        scenes = [
            {"sceneNumber": 1, "duration": 10.0, "description": "Scene 1", "assets": []},
            {"sceneNumber": 2, "duration": 10.0, "description": "Scene 2", "assets": []},
            {"sceneNumber": 3, "duration": 10.0, "description": "Scene 3", "assets": []}
        ]

        processed = _post_process_scenes(scenes, target_duration=30.0, available_assets=[])

        for i, scene in enumerate(processed, 1):
            assert scene['sceneNumber'] == i


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
</file>

<file path="backend/workflows/__init__.py">
"""
Luigi workflow orchestration for V3 campaign pipeline.

This package provides Luigi task definitions for managing the end-to-end
video generation pipeline with dependency management and checkpointing.
"""

from .campaign_pipeline import CampaignPipelineWorkflow
from .base import CampaignPipelineTask
from .runner import run_pipeline_async, get_pipeline_status

__all__ = [
    "CampaignPipelineWorkflow",
    "CampaignPipelineTask",
    "run_pipeline_async",
    "get_pipeline_status",
]
</file>

<file path="backend/workflows/fastapi_integration.py">
"""
FastAPI integration for Luigi workflows.

This module provides example endpoints showing how to integrate Luigi
workflows with the existing V3 API.
"""

from fastapi import APIRouter, Depends, HTTPException, BackgroundTasks
from typing import Dict, Any, Optional
import logging

from .runner import (
    run_pipeline_async,
    get_pipeline_status,
    get_all_pipeline_statuses,
    cancel_pipeline,
    retry_failed_pipeline,
)

logger = logging.getLogger(__name__)

# Create router for Luigi workflow endpoints
luigi_router = APIRouter(prefix="/api/v3/luigi", tags=["luigi-workflows"])


@luigi_router.post("/jobs/from-image-pairs")
async def create_job_with_luigi(
    request: Dict[str, Any],
    background_tasks: BackgroundTasks,
) -> Dict[str, Any]:
    """
    Create a new job using Luigi workflow orchestration.

    This is an alternative to the existing /api/v3/jobs/from-image-pairs
    endpoint that uses Luigi for workflow management instead of manual
    asyncio orchestration.

    Request body:
    {
        "campaignId": "campaign-uuid",
        "clientId": "client-uuid" (optional),
        "clipDuration": 5.0 (optional),
        "numPairs": 10 (optional)
    }

    Returns:
    {
        "jobId": 123,
        "status": "pipeline_started",
        "message": "Luigi pipeline initiated",
        "luigiDashboard": "http://localhost:8082/..."
    }
    """
    from ..database import create_video_job

    try:
        campaign_id = request.get("campaignId")
        client_id = request.get("clientId")
        clip_duration = request.get("clipDuration")
        num_pairs = request.get("numPairs")

        if not campaign_id:
            raise HTTPException(status_code=400, detail="campaignId is required")

        # Create job record
        job_id = create_video_job(
            prompt=f"Luigi pipeline for campaign {campaign_id}",
            model_id="luigi-workflow",
            parameters={
                "campaign_id": campaign_id,
                "client_id": client_id,
                "clip_duration": clip_duration,
                "num_pairs": num_pairs,
                "workflow_type": "luigi",
            },
            estimated_cost=0.0,
            client_id=client_id,
            status="pipeline_starting",
        )

        logger.info(f"Created job {job_id} for Luigi pipeline")

        # Launch Luigi workflow in background
        background_tasks.add_task(
            run_pipeline_async,
            job_id,
            campaign_id,
            clip_duration,
            num_pairs,
            workers=10,
            use_local_scheduler=False,  # Use central scheduler
        )

        return {
            "jobId": job_id,
            "status": "pipeline_started",
            "message": "Luigi workflow initiated. Use /luigi/jobs/{job_id} to monitor progress.",
            "luigiDashboard": f"http://localhost:8082/static/visualiser/index.html",
        }

    except Exception as e:
        logger.error(f"Error creating Luigi job: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@luigi_router.get("/jobs/{job_id}")
async def get_luigi_job_status(job_id: int) -> Dict[str, Any]:
    """
    Get Luigi pipeline status for a job.

    Returns detailed task-level status information.

    Returns:
    {
        "jobId": 123,
        "status": "running",
        "progress": 45.5,
        "tasks": [
            {
                "name": "AssetCollectionTask",
                "status": "completed",
                "created_at": "2025-01-22T10:00:00Z",
                "completed_at": "2025-01-22T10:01:00Z"
            },
            ...
        ],
        "completed_tasks": 5,
        "total_tasks": 11,
        "luigiDashboard": "http://localhost:8082/..."
    }
    """
    try:
        status = get_pipeline_status(job_id)

        if not status or status.get("status") == "not_started":
            raise HTTPException(status_code=404, detail="Job not found")

        # Add Luigi dashboard link
        status["luigiDashboard"] = (
            f"http://localhost:8082/static/visualiser/index.html#task/job_{job_id}"
        )

        return status

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting job status: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@luigi_router.get("/jobs")
async def list_luigi_jobs(
    limit: int = 20,
    offset: int = 0,
) -> Dict[str, Any]:
    """
    List all Luigi pipeline jobs.

    Query parameters:
    - limit: Number of jobs to return (default: 20)
    - offset: Pagination offset (default: 0)

    Returns:
    {
        "jobs": [...],
        "total": 42,
        "limit": 20,
        "offset": 0
    }
    """
    try:
        jobs = get_all_pipeline_statuses(limit=limit, offset=offset)

        return {
            "jobs": jobs,
            "total": len(jobs),
            "limit": limit,
            "offset": offset,
        }

    except Exception as e:
        logger.error(f"Error listing jobs: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@luigi_router.post("/jobs/{job_id}/cancel")
async def cancel_luigi_job(job_id: int) -> Dict[str, Any]:
    """
    Cancel a running Luigi pipeline.

    This marks all pending/running tasks as cancelled.
    """
    try:
        result = cancel_pipeline(job_id)

        if not result.get("success"):
            raise HTTPException(
                status_code=500,
                detail=result.get("error", "Failed to cancel pipeline")
            )

        return result

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error cancelling job: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@luigi_router.post("/jobs/{job_id}/retry")
async def retry_luigi_job(
    job_id: int,
    background_tasks: BackgroundTasks,
) -> Dict[str, Any]:
    """
    Retry a failed Luigi pipeline from the last checkpoint.

    This resets failed tasks to pending and re-runs the pipeline.
    """
    try:
        # Reset failed tasks
        result = retry_failed_pipeline(job_id)

        if not result.get("success"):
            raise HTTPException(
                status_code=500,
                detail=result.get("error", "Failed to reset pipeline")
            )

        # Get job details
        from ..database import get_job

        job = get_job(job_id)
        if not job:
            raise HTTPException(status_code=404, detail="Job not found")

        # Extract campaign ID from parameters
        import json

        params = (
            json.loads(job["parameters"])
            if isinstance(job["parameters"], str)
            else job["parameters"]
        )

        campaign_id = params.get("campaign_id")
        if not campaign_id:
            raise HTTPException(
                status_code=400,
                detail="Cannot retry: campaign_id not found in job parameters"
            )

        clip_duration = params.get("clip_duration")
        num_pairs = params.get("num_pairs")

        # Re-run the pipeline in background
        background_tasks.add_task(
            run_pipeline_async,
            job_id,
            campaign_id,
            clip_duration,
            num_pairs,
            workers=10,
            use_local_scheduler=False,
        )

        return {
            "success": True,
            "job_id": job_id,
            "message": "Pipeline retry initiated",
            "reset_tasks": result.get("reset_tasks", 0),
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error retrying job: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# Export router
__all__ = ["luigi_router"]
</file>

<file path="backend/test_unified_upload.py">
#!/usr/bin/env python3
"""
Test script for unified asset upload functionality
Tests basic functionality without complex imports
"""

import sys
from pathlib import Path

# Add backend to path
backend_path = Path(__file__).parent
sys.path.insert(0, str(backend_path))


def test_imports():
    """Test that all our modified modules can be imported"""
    print("Testing imports...")

    try:
        from api.v3.models import UnifiedAssetUploadInput
        from schemas.assets import AssetDB, BaseAsset

        print("✓ All imports successful")
        return True
    except Exception as e:
        print(f"✗ Import failed: {e}")
        return False


def test_model_validation():
    """Test that our new models work correctly"""
    print("Testing model validation...")

    try:
        from api.v3.models import UnifiedAssetUploadInput

        # Test file upload input
        file_input = UnifiedAssetUploadInput(
            uploadType="file",
            name="test-image.jpg",
            type="image",
            clientId="test-client",
            generateThumbnail=True,
        )
        print("✓ File upload input validation passed")

        # Test URL upload input
        url_input = UnifiedAssetUploadInput(
            uploadType="url",
            name="test-image.jpg",
            type="image",
            sourceUrl="https://example.com/image.jpg",
            clientId="test-client",
            generateThumbnail=True,
        )
        print("✓ URL upload input validation passed")

        return True
    except Exception as e:
        print(f"✗ Model validation failed: {e}")
        return False


def test_database_schema():
    """Test that database schema includes our new fields"""
    print("Testing database schema...")

    try:
        import sqlite3

        conn = sqlite3.connect("backend/DATA/scenes.db")
        cursor = conn.cursor()

        # Check if thumbnail_blob_id column exists
        cursor.execute("PRAGMA table_info(assets)")
        columns = cursor.fetchall()
        column_names = [col[1] for col in columns]

        required_columns = [
            "id",
            "name",
            "asset_type",
            "url",
            "thumbnail_url",
            "thumbnail_blob_id",
            "source_url",
        ]
        missing_columns = [col for col in required_columns if col not in column_names]

        if missing_columns:
            print(f"✗ Missing columns: {missing_columns}")
            return False

        print("✓ All required columns present in assets table")
        conn.close()
        return True

    except Exception as e:
        print(f"✗ Database schema test failed: {e}")
        return False


def main():
    """Run all tests"""
    print("Running unified asset upload tests...\n")

    tests = [
        test_imports,
        test_model_validation,
        test_database_schema,
    ]

    passed = 0
    total = len(tests)

    for test in tests:
        try:
            if test():
                passed += 1
            print()
        except Exception as e:
            print(f"✗ Test {test.__name__} crashed: {e}\n")

    print(f"Results: {passed}/{total} tests passed")

    if passed == total:
        print("🎉 All tests passed!")
        return 0
    else:
        print("❌ Some tests failed")
        return 1


if __name__ == "__main__":
    sys.exit(main())
</file>

<file path="backend/services/scene_prompts.py">
"""
Scene-specific prompts for luxury property video generation.

Provides professional cinematography prompts for different property scenes.
"""

from typing import Dict, List, Any

# Professional luxury real estate scene templates
LUXURY_PROPERTY_SCENES = [
    {
        "scene_number": 1,
        "name": "The Hook (Exterior Feature)",
        "duration": 1.5,
        "ideal_tags": ["exterior", "pool", "courtyard", "terrace", "water"],
        "motion_goal": "Immediate visual interest with subtle water or surface movement + gentle forward motion",
        "prompt": (
            "Cinematic wide shot, low angle. Clear water or reflective surface gently rippling. "
            "Subtle, smooth camera push-in (dolly forward). Bright natural lighting with glistening "
            "highlights on the water/surface. Photorealistic 4K, high fidelity, sharp focus on the edge detail."
        ),
        "music_prompt": (
            "Cinematic ambient opening, gentle water sounds, soft synthesizer pads, "
            "subtle orchestral strings, uplifting and inviting, 120 BPM"
        )
    },
    {
        "scene_number": 2,
        "name": "The Hero Bedroom (Parallax)",
        "duration": 1.0,
        "ideal_tags": ["bedroom", "master bedroom", "interior", "windows"],
        "motion_goal": "Lateral movement to reveal depth between foreground and background",
        "prompt": (
            "Smooth sideways camera truck (left or right – choose direction that creates natural parallax). "
            "Luxurious bedroom with large windows or glass walls. Parallax effect: bed and foreground elements "
            "move slightly faster than the background view. Soft natural light, no zoom, pure linear sideways motion."
        ),
        "music_prompt": (
            "Soft luxurious ambiance, gentle piano melody, warm pad atmosphere, "
            "elegant and serene, subtle string harmonies"
        )
    },
    {
        "scene_number": 3,
        "name": "Bathroom Vanity (Symmetry)",
        "duration": 1.0,
        "ideal_tags": ["bathroom", "vanity", "interior", "mirror"],
        "motion_goal": "Smooth sliding movement (opposite direction to Scene 2 for flow)",
        "prompt": (
            "Cinematic sideways truck (left or right – opposite of Scene 2). Modern bathroom vanity with mirror. "
            "Reflections shift naturally as camera moves. Clean, bright lighting, sharp focus on surfaces and fixtures."
        ),
        "music_prompt": (
            "Clean spa-like ambiance, subtle chimes, flowing water undertones, "
            "peaceful and modern, minimal percussion"
        )
    },
    {
        "scene_number": 4,
        "name": "The Feature Tub / Shower (Depth)",
        "duration": 1.0,
        "ideal_tags": ["bathroom", "tub", "shower", "spa"],
        "motion_goal": "Intimate push-in to emphasize the luxury fixture",
        "prompt": (
            "Slow, smooth dolly forward toward the centerpiece tub or shower. Background view through window "
            "or opening remains steady. Serene spa-like atmosphere, soft balanced lighting, high detail on "
            "textures and materials."
        ),
        "music_prompt": (
            "Serene spa atmosphere, gentle harp arpeggios, soft ambient pads, "
            "tranquil and meditative, nature sounds"
        )
    },
    {
        "scene_number": 5,
        "name": "Living Room (The Sweep)",
        "duration": 1.0,
        "ideal_tags": ["living room", "interior", "seating", "lounge"],
        "motion_goal": "Sweeping movement to reveal scale and flow of the space",
        "prompt": (
            "Wide shot, smooth sideways pan or truck (choose direction that follows the natural lines of "
            "furniture/layout). Spacious living room with prominent seating. Natural light streaming in, "
            "subtle atmospheric particles in the air. Fluid, steady camera motion."
        ),
        "music_prompt": (
            "Spacious orchestral sweep, warm string section, elegant movement, "
            "building energy, sophisticated and grand"
        )
    },
    {
        "scene_number": 6,
        "name": "Lifestyle / Dining Area (Atmosphere)",
        "duration": 1.0,
        "ideal_tags": ["dining", "kitchen", "lifestyle", "indoor-outdoor"],
        "motion_goal": "Near-static shot that lets the lighting and ambiance breathe",
        "prompt": (
            "Almost static tripod shot with very subtle handheld float or gentle drift. Elegant dining or "
            "lifestyle area. Warm, inviting lighting. Minimal natural movement (candles, slight breeze, "
            "or soft fabric sway if present)."
        ),
        "music_prompt": (
            "Warm inviting atmosphere, acoustic guitar fingerpicking, soft jazz undertones, "
            "intimate and cozy, evening ambiance"
        )
    },
    {
        "scene_number": 7,
        "name": "The Outro (Establishing Wide)",
        "duration": 3.5,  # Adjusted to fit 10s total (1.5 + 1.0*5 + 3.5 = 10.0)
        "ideal_tags": ["exterior", "deck", "entrance", "view", "establishing"],
        "motion_goal": "Gentle pull-back to leave the viewer with a lasting impression",
        "prompt": (
            "Smooth dolly outward or subtle drone-style pull-back. Establishing shot of the property at its "
            "most inviting time of day. Warm interior glow visible through windows (if applicable). "
            "Calm, cinematic, and peaceful closing moment."
        ),
        "music_prompt": (
            "Grand cinematic finale, full orchestral swell, inspiring and aspirational, "
            "peaceful resolution, warm golden hour feeling, uplifting conclusion"
        )
    }
]


def get_scene_prompt(scene_number: int) -> Dict[str, Any]:
    """
    Get the prompt template for a specific scene number.

    Args:
        scene_number: Scene number (1-7)

    Returns:
        Dictionary containing scene metadata and prompt
    """
    if 1 <= scene_number <= len(LUXURY_PROPERTY_SCENES):
        return LUXURY_PROPERTY_SCENES[scene_number - 1]
    else:
        # Fallback to generic transition
        return {
            "scene_number": scene_number,
            "name": "Generic Transition",
            "duration": 1.0,
            "ideal_tags": [],
            "motion_goal": "Smooth transition",
            "prompt": "Smooth cinematic transition between images"
        }


def get_all_scenes() -> List[Dict[str, Any]]:
    """
    Get all luxury property scene templates.

    Returns:
        List of all scene dictionaries
    """
    return LUXURY_PROPERTY_SCENES


def match_scene_to_tags(tags: List[str]) -> int:
    """
    Match asset tags to the most appropriate scene number.

    Args:
        tags: List of asset tags

    Returns:
        Best matching scene number (1-7)
    """
    if not tags:
        return 1  # Default to Scene 1

    tag_set = set(tag.lower() for tag in tags)

    best_match = 1
    best_score = 0

    for scene in LUXURY_PROPERTY_SCENES:
        # Calculate overlap score
        ideal_tags_set = set(tag.lower() for tag in scene["ideal_tags"])
        overlap = len(tag_set & ideal_tags_set)

        if overlap > best_score:
            best_score = overlap
            best_match = scene["scene_number"]

    return best_match


def get_total_duration() -> float:
    """
    Get total duration of all scenes.

    Returns:
        Total duration in seconds
    """
    return sum(scene["duration"] for scene in LUXURY_PROPERTY_SCENES)
</file>

<file path="backend/workflows/base.py">
"""
Base classes for Luigi campaign pipeline tasks.

Provides common functionality for all pipeline tasks including:
- State management in database
- Async operation support
- Error handling and logging
"""

import luigi
import logging
import json
from typing import Dict, Any, Optional
from datetime import datetime
from pathlib import Path

logger = logging.getLogger(__name__)


class CampaignStateTarget(luigi.Target):
    """
    Custom Target that stores task completion state in database.

    This allows Luigi to track which tasks have completed and supports
    resuming from failures.
    """

    def __init__(self, job_id: int, task_name: str):
        self.job_id = job_id
        self.task_name = task_name

    def exists(self) -> bool:
        """Check if this task has already completed."""
        from ..database import get_db

        with get_db() as conn:
            row = conn.execute(
                """
                SELECT 1 FROM luigi_task_state
                WHERE job_id = ? AND task_name = ?
                AND status = 'completed'
                """,
                (self.job_id, self.task_name),
            ).fetchone()

            return row is not None

    def mark_complete(self, output_data: Optional[Dict[str, Any]] = None):
        """Mark this task as completed."""
        from ..database import get_db

        with get_db() as conn:
            conn.execute(
                """
                INSERT OR REPLACE INTO luigi_task_state
                (task_id, job_id, task_name, status, output_data, completed_at)
                VALUES (?, ?, ?, ?, ?, ?)
                """,
                (
                    f"{self.job_id}_{self.task_name}",
                    self.job_id,
                    self.task_name,
                    "completed",
                    json.dumps(output_data) if output_data else None,
                    datetime.utcnow().isoformat(),
                ),
            )
            conn.commit()

    def get_output_data(self) -> Optional[Dict[str, Any]]:
        """Retrieve output data from completed task."""
        from ..database import get_db

        with get_db() as conn:
            row = conn.execute(
                """
                SELECT output_data FROM luigi_task_state
                WHERE job_id = ? AND task_name = ?
                AND status = 'completed'
                """,
                (self.job_id, self.task_name),
            ).fetchone()

            if row and row["output_data"]:
                return json.loads(row["output_data"])
            return None


class CampaignPipelineTask(luigi.Task):
    """
    Base class for all campaign pipeline tasks.

    Provides common parameters and helper methods for pipeline tasks.
    """

    job_id = luigi.IntParameter(description="Parent job ID")
    campaign_id = luigi.Parameter(description="Campaign ID")

    def output(self):
        """
        Return the Target that marks this task's completion.

        Override this in subclasses if you need custom Target behavior.
        """
        # Use Luigi's task_id which includes all parameters for uniqueness
        return CampaignStateTarget(self.job_id, self.task_id)

    def get_output_data(self) -> Optional[Dict[str, Any]]:
        """Get output data from this task if it's already completed."""
        target = self.output()
        if isinstance(target, CampaignStateTarget):
            return target.get_output_data()
        return None

    def mark_complete(self, output_data: Optional[Dict[str, Any]] = None):
        """Mark this task as completed with optional output data."""
        target = self.output()
        if isinstance(target, CampaignStateTarget):
            target.mark_complete(output_data)

    def update_job_status(self, status: str, **metadata):
        """Update the main job status in database."""
        from ..database import update_video_status

        update_video_status(self.job_id, status, metadata=metadata)
        logger.info(f"Updated job {self.job_id} status to: {status}")

    def log_task_start(self):
        """Log when task starts."""
        from ..database import get_db

        # Include Luigi's unique task_id which includes all parameters
        task_id = self.task_id

        with get_db() as conn:
            conn.execute(
                """
                INSERT OR REPLACE INTO luigi_task_state
                (task_id, job_id, task_name, status, created_at)
                VALUES (?, ?, ?, ?, ?)
                """,
                (
                    task_id,
                    self.job_id,
                    self.__class__.__name__,
                    "running",
                    datetime.utcnow().isoformat(),
                ),
            )
            conn.commit()

        logger.info(f"Started task: {self.__class__.__name__} for job {self.job_id}")

    def log_task_failure(self, error: Exception):
        """Log when task fails."""
        from ..database import get_db

        # Use Luigi's unique task_id which includes all parameters
        task_id = self.task_id

        with get_db() as conn:
            conn.execute(
                """
                UPDATE luigi_task_state
                SET status = ?, output_data = ?
                WHERE task_id = ?
                """,
                (
                    "failed",
                    json.dumps({"error": str(error)}),
                    task_id,
                ),
            )
            conn.commit()

        logger.error(
            f"Task {self.__class__.__name__} failed for job {self.job_id}: {error}"
        )

    def on_failure(self, exception):
        """Called when task fails."""
        self.log_task_failure(exception)
        super().on_failure(exception)


class AsyncCampaignTask(CampaignPipelineTask):
    """
    Base class for tasks that need to run async operations.

    Wraps async operations to work with Luigi's synchronous execution model.
    """

    def run(self):
        """
        Luigi's synchronous run method.

        This calls the async_run method and waits for it to complete.
        """
        import asyncio

        self.log_task_start()

        try:
            # Run the async operation
            result = asyncio.run(self.async_run())

            # Mark as complete
            self.mark_complete(result)

        except Exception as e:
            self.log_task_failure(e)
            raise

    async def async_run(self) -> Optional[Dict[str, Any]]:
        """
        Override this method in subclasses to implement async logic.

        Returns:
            Optional dict with output data to be stored
        """
        raise NotImplementedError("Subclasses must implement async_run()")
</file>

<file path="backend/workflows/campaign_pipeline.py">
"""
Main campaign pipeline workflow.

This module defines the complete end-to-end workflow for campaign video generation.
"""

import luigi
import logging
from typing import Optional

from .tasks import (
    AssetCollectionTask,
    AssetGroupingTask,
    GroupSelectionTask,
    ImagePairSelectionTask,
    SubJobCreationTask,
    ParallelVideoGenerationTask,
    AudioGenerationTask,
    VideoCombinationTask,
    AudioMergingTask,
    VideoStorageTask,
)

logger = logging.getLogger(__name__)


class CampaignPipelineWorkflow(luigi.WrapperTask):
    """
    Complete campaign video generation workflow.

    This is the entry point for the entire pipeline. It orchestrates all
    tasks from asset collection through final video storage.

    Usage:
        # From command line
        luigi --module workflows.campaign_pipeline CampaignPipelineWorkflow \
            --job-id 123 \
            --campaign-id camp-abc \
            --local-scheduler

        # From Python code
        luigi.build([
            CampaignPipelineWorkflow(
                job_id=123,
                campaign_id="camp-abc"
            )
        ], workers=10)
    """

    job_id = luigi.IntParameter(description="Parent job ID")
    campaign_id = luigi.Parameter(description="Campaign ID")
    clip_duration = luigi.FloatParameter(
        default=6.0,
        description="Duration for each video clip in seconds"
    )
    num_pairs = luigi.IntParameter(
        default=10,
        description="Target number of image pairs to select"
    )

    def requires(self):
        """
        The final task of the pipeline.

        Luigi automatically resolves all dependencies starting from this task.
        """
        return VideoStorageTask(
            job_id=self.job_id,
            campaign_id=self.campaign_id
        )

    def complete(self):
        """
        Check if the entire workflow is complete.

        Returns True if the final task (VideoStorageTask) is complete.
        """
        return self.requires().complete()


class CampaignPipelineWorkflowWithParams(CampaignPipelineWorkflow):
    """
    Variant of the workflow that accepts additional parameters.

    Use this when you want more control over the pipeline configuration.
    """

    skip_audio = luigi.BoolParameter(
        default=False,
        description="Skip audio generation and merging"
    )

    def requires(self):
        """
        Conditionally skip audio-related tasks.
        """
        if self.skip_audio:
            # Skip audio tasks, go straight to storage after combination
            return VideoCombinationTask(
                job_id=self.job_id,
                campaign_id=self.campaign_id
            )
        else:
            # Normal flow with audio
            return super().requires()


class PropertyVideoPipelineWorkflow(luigi.WrapperTask):
    """
    Specialized workflow for property video generation.

    This workflow is optimized for luxury lodging properties with
    predefined scene types.
    """

    job_id = luigi.IntParameter()
    campaign_id = luigi.Parameter()
    property_name = luigi.Parameter()
    clip_duration = luigi.FloatParameter(default=6.0)

    def requires(self):
        """
        Property videos use the same pipeline but with different parameters.
        """
        return VideoStorageTask(
            job_id=self.job_id,
            campaign_id=self.campaign_id
        )

    def complete(self):
        """Check if the workflow is complete."""
        return self.requires().complete()


# Export all workflow classes
__all__ = [
    "CampaignPipelineWorkflow",
    "CampaignPipelineWorkflowWithParams",
    "PropertyVideoPipelineWorkflow",
]
</file>

<file path="backend/workflows/tasks.py">
"""
Individual Luigi tasks for the campaign pipeline.

Each task represents a discrete step in the video generation workflow.
"""

import luigi
import logging
import asyncio
from typing import Dict, Any, List, Tuple, Optional

from .base import AsyncCampaignTask, CampaignPipelineTask

logger = logging.getLogger(__name__)


class AssetCollectionTask(AsyncCampaignTask):
    """
    Task 1: Collect all image assets for the campaign.

    Fetches all image assets associated with the campaign from the database.
    """

    async def async_run(self) -> Dict[str, Any]:
        """Fetch campaign assets."""
        from ..database_helpers import list_assets

        logger.info(f"Collecting assets for campaign {self.campaign_id}")

        assets = list_assets(
            user_id=None,
            campaign_id=self.campaign_id,
            asset_type="image",
            limit=1000,
            offset=0,
        )

        logger.info(f"Found {len(assets)} image assets for campaign {self.campaign_id}")

        if len(assets) < 2:
            raise ValueError(
                f"Need at least 2 image assets, but campaign has {len(assets)}"
            )

        # Store asset IDs for next tasks
        asset_ids = [asset.id for asset in assets]

        return {
            "asset_count": len(assets),
            "asset_ids": asset_ids,
        }


class AssetGroupingTask(AsyncCampaignTask):
    """
    Task 2: Group assets by room/area.

    Parses asset names to extract room groupings (e.g., "Bedroom 1", "Kitchen").
    Groups assets so we can select representative pairs from each room.
    """

    async def async_run(self) -> Dict[str, Any]:
        """Group assets by room/area name."""
        from ..database_helpers import list_assets
        import re

        logger.info(f"Grouping assets for campaign {self.campaign_id}")

        assets = list_assets(
            user_id=None,
            campaign_id=self.campaign_id,
            asset_type="image",
            limit=1000,
            offset=0,
        )

        # Group assets by extracting room name from asset name
        # Strip ONLY the last number to normalize room names
        # E.g., "Bedroom 1 3" -> "Bedroom 1", "Kitchen 5" -> "Kitchen"
        groups = {}

        for asset in assets:
            name = getattr(asset, "name", "") or ""

            # Strip only the last trailing number
            # "Kitchen 5" -> "Kitchen"
            # "Bedroom 1 3" -> "Bedroom 1"
            # "Living Room 2" -> "Living Room"
            group_name = re.sub(r'\s+\d+$', '', name.strip()) or "Other"
            group_name = group_name.strip() or "Other"

            if group_name not in groups:
                groups[group_name] = []

            groups[group_name].append({
                "id": asset.id,
                "name": name,
                "url": getattr(asset, "url", ""),
            })

        logger.info(f"Found {len(groups)} asset groups: {list(groups.keys())}")

        # Log group sizes
        for group_name, group_assets in groups.items():
            logger.info(f"  {group_name}: {len(group_assets)} assets")

        return {
            "groups": groups,
            "group_names": list(groups.keys()),
            "total_groups": len(groups),
        }


class GroupSelectionTask(AsyncCampaignTask):
    """
    Task 3: Use LLM to select 7 groups in narrative order.

    Analyzes available room groups and campaign context to select
    the 7 best groups for a compelling property video story.
    """

    def requires(self):
        """Depends on asset grouping."""
        return AssetGroupingTask(
            job_id=self.job_id,
            campaign_id=self.campaign_id
        )

    async def async_run(self) -> Dict[str, Any]:
        """Select 7 groups using LLM for narrative ordering."""
        from ..database_helpers import get_campaign_by_id, get_client_by_id
        from openai import OpenAI
        import os
        import json

        logger.info(f"Selecting groups for campaign {self.campaign_id}")

        # Get grouped assets
        grouping_result = self.input().get_output_data()
        groups = grouping_result["groups"]
        group_names = grouping_result["group_names"]

        if len(group_names) < 7:
            logger.warning(f"Only {len(group_names)} groups available, need 7. Using what we have.")
            selected_groups = group_names
        else:
            # Get campaign context
            campaign = get_campaign_by_id(self.campaign_id, None)
            campaign_context = {}
            if campaign:
                campaign_context = {
                    "goal": campaign.get("goal", ""),
                    "name": campaign.get("name", ""),
                    "property_type": campaign.get("propertyType", "luxury property"),
                }

            # Get brand guidelines
            brand_guidelines = ""
            if campaign and campaign.get("clientId"):
                client = get_client_by_id(campaign["clientId"], None)
                if client and client.get("brandGuidelines"):
                    brand_guidelines = client["brandGuidelines"]

            # Build prompt for LLM
            prompt = f"""You are creating a luxury property video. You need to select 7 room/area groups in the perfect narrative order for a compelling property showcase video.

Available groups: {', '.join(group_names)}

Campaign context:
- Property type: {campaign_context.get('property_type', 'luxury property')}
- Campaign goal: {campaign_context.get('goal', 'showcase property features')}
- Campaign name: {campaign_context.get('name', '')}

Brand guidelines: {brand_guidelines or 'None provided'}

Select exactly 7 groups in the order they should appear in the video. Consider:
1. Start with an inviting entrance or exterior
2. Flow naturally through the property
3. Highlight key selling points (bedrooms, kitchen, bathrooms)
4. End with a memorable space (view, outdoor area, or luxury amenity)

Return ONLY a JSON array of exactly 7 group names in order, like:
["Exterior", "Living Room", "Kitchen", "Master Bedroom", "Bathroom", "Deck", "Pool"]

If there are multiple variants of a room (e.g., "Bedroom 1", "Bedroom 2"), choose the most important one.
"""

            # Call OpenAI
            openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

            response = openai_client.chat.completions.create(
                model="gpt-4o-mini",
                max_tokens=1024,
                messages=[{"role": "user", "content": prompt}]
            )

            response_text = response.choices[0].message.content.strip()
            logger.info(f"LLM response: {response_text}")

            # Parse JSON response
            try:
                # Extract JSON from response (in case there's extra text)
                import re
                json_match = re.search(r'\[.*\]', response_text, re.DOTALL)
                if json_match:
                    response_text = json_match.group(0)

                selected_groups = json.loads(response_text)

                # Validate all selected groups exist
                valid_selected = []
                for group in selected_groups:
                    if group in groups:
                        valid_selected.append(group)
                    else:
                        logger.warning(f"LLM selected group '{group}' not found in available groups")

                selected_groups = valid_selected[:7]  # Ensure max 7

            except json.JSONDecodeError as e:
                logger.error(f"Failed to parse LLM response as JSON: {e}")
                logger.error(f"Response was: {response_text}")
                # Fallback: take first 7 groups
                selected_groups = group_names[:7]

        logger.info(f"Selected {len(selected_groups)} groups in order: {selected_groups}")

        return {
            "selected_groups": selected_groups,
            "total_selected": len(selected_groups),
        }


class ImagePairSelectionTask(AsyncCampaignTask):
    """
    Task 4: Create image pairs from selected groups.

    Takes the first 2 images from each selected group to create pairs
    for video generation.
    """

    def requires(self):
        """Depends on group selection."""
        return {
            "grouping": AssetGroupingTask(
                job_id=self.job_id,
                campaign_id=self.campaign_id
            ),
            "selection": GroupSelectionTask(
                job_id=self.job_id,
                campaign_id=self.campaign_id
            ),
        }

    async def async_run(self) -> Dict[str, Any]:
        """Create image pairs from selected groups."""
        logger.info(f"Creating image pairs for campaign {self.campaign_id}")

        self.update_job_status("image_pair_selection")

        # Get inputs from dependencies
        grouping_result = self.input()["grouping"].get_output_data()
        selection_result = self.input()["selection"].get_output_data()

        groups = grouping_result["groups"]
        selected_groups = selection_result["selected_groups"]

        # Create pairs: take first 2 images from each selected group
        image_pairs = []

        for i, group_name in enumerate(selected_groups, 1):
            group_assets = groups.get(group_name, [])

            if len(group_assets) < 2:
                logger.warning(f"Group '{group_name}' has only {len(group_assets)} assets, need 2. Skipping.")
                continue

            # Take first 2 images from this group
            image1 = group_assets[0]
            image2 = group_assets[1]

            pair = (
                image1["id"],
                image2["id"],
                0.9,  # High score since these are from the same room
                f"Pair {i}: {group_name} - showcasing the space from different angles"
            )

            image_pairs.append(pair)
            logger.info(f"Created pair {i} from group '{group_name}': {image1['name']} + {image2['name']}")

        logger.info(f"Created {len(image_pairs)} image pairs from {len(selected_groups)} groups")

        # Store pairs for next tasks
        pairs_data = [
            {
                "image1_id": pair[0],
                "image2_id": pair[1],
                "score": pair[2],
                "reasoning": pair[3],
            }
            for pair in image_pairs
        ]

        return {
            "pairs_count": len(image_pairs),
            "pairs": pairs_data,
        }


class SubJobCreationTask(CampaignPipelineTask):
    """
    Task 3: Create sub-jobs for each image pair.

    Creates database records for each video generation sub-job.
    """

    clip_duration = luigi.FloatParameter(default=6.0, description="Clip duration in seconds")

    def requires(self):
        """Depends on image pair selection."""
        return ImagePairSelectionTask(
            job_id=self.job_id,
            campaign_id=self.campaign_id
        )

    def run(self):
        """Create sub-jobs in database."""
        from ..database import create_sub_job
        from ..services.scene_prompts import get_scene_prompt
        from ..config import get_settings

        self.log_task_start()

        try:
            # Get selected pairs from previous task
            pairs_data = self.input().get_output_data()
            pairs = pairs_data["pairs"]

            # Check if sub-jobs already exist for this job
            from ..database import get_db
            with get_db() as conn:
                existing_sub_jobs = conn.execute(
                    "SELECT id, sub_job_number FROM video_sub_jobs WHERE job_id = ? ORDER BY sub_job_number",
                    (self.job_id,)
                ).fetchall()

            if existing_sub_jobs:
                logger.info(f"Sub-jobs already exist for job {self.job_id}, reusing {len(existing_sub_jobs)} existing sub-jobs")
                sub_job_ids = [row["id"] for row in existing_sub_jobs]
                # Get rounded duration from existing sub-job
                with get_db() as conn:
                    first_sub_job = conn.execute(
                        "SELECT input_parameters FROM video_sub_jobs WHERE id = ?",
                        (sub_job_ids[0],)
                    ).fetchone()
                    if first_sub_job and first_sub_job["input_parameters"]:
                        import json
                        params = json.loads(first_sub_job["input_parameters"]) if isinstance(first_sub_job["input_parameters"], str) else first_sub_job["input_parameters"]
                        rounded_duration = params.get("duration", self.clip_duration)
                    else:
                        rounded_duration = self.clip_duration
            else:
                logger.info(f"Creating {len(pairs)} sub-jobs for job {self.job_id}")

                self.update_job_status("sub_job_creation")

                settings = get_settings()
                video_model = settings.VIDEO_GENERATION_MODEL

                # Round duration for Veo3
                from ..services.sub_job_orchestrator import _round_duration_for_veo3
                rounded_duration = _round_duration_for_veo3(self.clip_duration)

                sub_job_ids = []
                for i, pair in enumerate(pairs, 1):
                    # Get scene-specific prompt
                    scene_info = get_scene_prompt(i if i <= 7 else ((i - 1) % 7) + 1)

                    sub_job_id = create_sub_job(
                        job_id=self.job_id,
                        sub_job_number=i,
                        image1_asset_id=pair["image1_id"],
                        image2_asset_id=pair["image2_id"],
                        model_id=video_model,
                        input_parameters={
                            "duration": rounded_duration or scene_info["duration"],
                            "score": pair["score"],
                            "reasoning": pair["reasoning"],
                            "prompt": scene_info["prompt"],
                            "scene_number": scene_info["scene_number"],
                            "scene_name": scene_info["name"],
                            "motion_goal": scene_info["motion_goal"],
                        },
                    )
                    sub_job_ids.append(sub_job_id)

                logger.info(f"Created {len(sub_job_ids)} sub-jobs")

            # Mark complete
            self.mark_complete({
                "sub_job_count": len(sub_job_ids),
                "sub_job_ids": sub_job_ids,
                "rounded_duration": rounded_duration,
            })

        except Exception as e:
            self.log_task_failure(e)
            raise


class VideoGenerationSubTask(AsyncCampaignTask):
    """
    Sub-task for generating a single video from an image pair.

    This task is created dynamically for each sub-job.
    """

    sub_job_id = luigi.Parameter(description="Sub-job ID")
    clip_duration = luigi.FloatParameter(default=6.0)

    async def async_run(self) -> Dict[str, Any]:
        """Generate video for this image pair."""
        from ..database import get_sub_job_by_id, update_sub_job_status
        from ..database_helpers import get_asset_by_id
        from ..services.replicate_client import ReplicateClient
        from ..config import get_settings
        from ..auth import create_asset_access_token

        logger.info(f"Generating video for sub-job {self.sub_job_id}")

        # Get sub-job details
        sub_job = get_sub_job_by_id(self.sub_job_id)
        if not sub_job:
            raise ValueError(f"Sub-job {self.sub_job_id} not found")

        # Get asset URLs
        image1 = get_asset_by_id(sub_job["image1AssetId"])
        image2 = get_asset_by_id(sub_job["image2AssetId"])

        if not image1 or not image2:
            raise ValueError(f"Assets not found for sub-job {self.sub_job_id}")

        # Construct asset URLs
        settings = get_settings()
        external_url = settings.NGROK_URL or settings.BASE_URL

        # Generate access tokens
        image1_token = create_asset_access_token(sub_job["image1AssetId"])
        image2_token = create_asset_access_token(sub_job["image2AssetId"])

        # Use source URL if available
        if hasattr(image1, 'source_url') and image1.source_url:
            image1_url = image1.source_url
        else:
            base_url = f"{external_url}{image1.url}" if not image1.url.startswith('http') else image1.url
            image1_url = f"{base_url}?token={image1_token}"

        if hasattr(image2, 'source_url') and image2.source_url:
            image2_url = image2.source_url
        else:
            base_url = f"{external_url}{image2.url}" if not image2.url.startswith('http') else image2.url
            image2_url = f"{base_url}?token={image2_token}"

        # Update status
        update_sub_job_status(self.sub_job_id, "processing")

        # Get scene prompt
        input_params = sub_job.get("inputParameters", {})
        scene_prompt = input_params.get("prompt") or input_params.get("scene_prompt")

        # Generate video
        replicate_client = ReplicateClient()
        result = await asyncio.to_thread(
            replicate_client.generate_video_from_pair,
            image1_url,
            image2_url,
            model=sub_job["modelId"],
            duration=self.clip_duration,
            prompt=scene_prompt,
        )

        if not result["success"]:
            update_sub_job_status(
                self.sub_job_id,
                "failed",
                error_message=result.get("error", "Unknown error"),
            )
            raise Exception(result.get("error", "Video generation failed"))

        # Download video
        from ..services.sub_job_orchestrator import _download_video

        video_url = result["video_url"]
        clip_path = await _download_video(
            self.job_id,
            sub_job["subJobNumber"],
            video_url
        )

        # Calculate cost
        duration = result.get("duration_seconds", self.clip_duration or 6)
        if sub_job["modelId"] == "veo3":
            cost = duration * ReplicateClient.VEO3_PRICE_PER_SECOND
        else:
            cost = ReplicateClient.HAILUO2_PRICE_PER_GENERATION

        # Update as completed
        update_sub_job_status(
            self.sub_job_id,
            "completed",
            replicate_prediction_id=result.get("prediction_id"),
            video_url=video_url,
            duration_seconds=duration,
            actual_cost=cost,
            progress=1.0,
        )

        logger.info(f"Sub-job {self.sub_job_id} completed successfully")

        return {
            "clip_path": clip_path,
            "video_url": video_url,
            "cost": cost,
            "duration": duration,
        }


class ParallelVideoGenerationTask(CampaignPipelineTask):
    """
    Task 4: Launch all video generation sub-tasks in parallel.

    This is a wrapper task that depends on all sub-tasks completing.
    Uses dynamic dependencies (yield) to spawn tasks based on SubJobCreationTask output.
    """

    clip_duration = luigi.FloatParameter(default=6.0)

    def requires(self):
        """
        Depends on SubJobCreationTask to get the list of sub-jobs.
        """
        return SubJobCreationTask(
            job_id=self.job_id,
            campaign_id=self.campaign_id,
            clip_duration=self.clip_duration
        )

    def run(self):
        """Aggregate results from all sub-tasks."""
        self.log_task_start()

        try:
            self.update_job_status("sub_job_processing")

            # 1. Get sub-job IDs from the requirement (SubJobCreationTask)
            sub_jobs_output = self.input().get_output_data()

            if not sub_jobs_output:
                raise ValueError("Sub-jobs not created yet")

            sub_job_ids = sub_jobs_output["sub_job_ids"]
            rounded_duration = sub_jobs_output.get("rounded_duration", self.clip_duration)

            logger.info(f"Spawning {len(sub_job_ids)} video generation tasks")

            # 2. Create the sub-tasks dynamically
            sub_tasks = [
                VideoGenerationSubTask(
                    job_id=self.job_id,
                    campaign_id=self.campaign_id,
                    sub_job_id=sub_job_id,
                    clip_duration=rounded_duration
                )
                for sub_job_id in sub_job_ids
            ]

            # 3. Yield them to let Luigi run them
            # Luigi will pause this task, run the sub-tasks, and resume here
            yield sub_tasks

            # 4. Collect results
            sub_task_results = []
            for sub_task in sub_tasks:
                result = sub_task.get_output_data()
                if result:
                    sub_task_results.append(result)

            successful = [r for r in sub_task_results if r.get("clip_path")]
            failed = len(sub_task_results) - len(successful)

            logger.info(f"Video generation complete: {len(successful)} succeeded, {failed} failed")

            if not successful:
                raise Exception("All sub-jobs failed")

            # Mark complete with aggregated results
            self.mark_complete({
                "total_clips": len(sub_task_results),
                "successful_clips": len(successful),
                "failed_clips": failed,
                "clip_paths": [r["clip_path"] for r in successful],
                "clip_urls": [r["video_url"] for r in successful],
                "total_cost": sum(r.get("cost", 0.0) for r in successful),
            })

        except Exception as e:
            self.log_task_failure(e)
            raise


class AudioGenerationTask(AsyncCampaignTask):
    """
    Task 5: Generate background music for the video.
    """

    def requires(self):
        """Depends on video generation completing."""
        return ParallelVideoGenerationTask(
            job_id=self.job_id,
            campaign_id=self.campaign_id
        )

    async def async_run(self) -> Dict[str, Any]:
        """Generate progressive audio."""
        from ..services.musicgen_client import MusicGenClient
        from ..services.scene_prompts import get_all_scenes
        import tempfile
        from pathlib import Path
        import requests

        logger.info(f"Generating audio for job {self.job_id}")

        self.update_job_status("audio_generation")

        # Get clip count from previous task
        video_results = self.input().get_output_data()
        num_scenes = video_results["successful_clips"]

        # Get scene prompts
        all_scenes = get_all_scenes()
        scene_prompts = []
        for i in range(num_scenes):
            scene_index = i % len(all_scenes)
            scene_prompts.append(all_scenes[scene_index])

        # Generate audio
        musicgen_client = MusicGenClient()
        result = await asyncio.to_thread(
            musicgen_client.generate_progressive_audio,
            scene_prompts,
            duration_per_scene=4,
        )

        if not result["success"]:
            logger.warning(f"Audio generation failed: {result['error']}")
            return {"audio_generated": False, "error": result["error"]}

        # Download audio
        audio_url = result["final_audio_url"]
        temp_dir = Path(tempfile.gettempdir()) / f"job_{self.job_id}"
        temp_dir.mkdir(exist_ok=True)
        audio_path = temp_dir / "background_music.mp3"

        response = await asyncio.to_thread(
            requests.get, audio_url, stream=True, timeout=300
        )
        response.raise_for_status()

        with open(audio_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)

        logger.info(f"Audio downloaded to {audio_path}")

        return {
            "audio_generated": True,
            "audio_path": str(audio_path),
            "audio_url": audio_url,
        }


class VideoCombinationTask(AsyncCampaignTask):
    """
    Task 6: Combine all video clips into one video.
    """

    def requires(self):
        """Depends on video generation."""
        return ParallelVideoGenerationTask(
            job_id=self.job_id,
            campaign_id=self.campaign_id
        )

    async def async_run(self) -> Dict[str, Any]:
        """Combine video clips."""
        from ..services.video_combiner import combine_video_clips
        from pathlib import Path
        import tempfile

        logger.info(f"Combining clips for job {self.job_id}")

        self.update_job_status("video_combining")

        # Get clip paths
        video_results = self.input().get_output_data()
        clip_paths = video_results["clip_paths"]

        # Create output path
        temp_dir = Path(tempfile.gettempdir()) / f"job_{self.job_id}"
        combined_path = temp_dir / "combined.mp4"

        # Combine videos
        success, output_path, metadata = await asyncio.to_thread(
            combine_video_clips,
            clip_paths,
            str(combined_path),
            transition_duration=0.0,
            output_resolution="1920x1080",
            output_fps=30,
            keep_audio=False,
        )

        if not success:
            raise Exception("Failed to combine video clips")

        logger.info(f"Combined video created at {output_path}")

        return {
            "combined_path": output_path,
            "metadata": metadata,
        }


class AudioMergingTask(AsyncCampaignTask):
    """
    Task 7: Merge audio with combined video.
    """

    def requires(self):
        """Depends on both video combination and audio generation."""
        return {
            "video": VideoCombinationTask(
                job_id=self.job_id,
                campaign_id=self.campaign_id
            ),
            "audio": AudioGenerationTask(
                job_id=self.job_id,
                campaign_id=self.campaign_id
            ),
        }

    async def async_run(self) -> Dict[str, Any]:
        """Merge audio with video."""
        from ..services.video_combiner import add_audio_to_video
        from pathlib import Path
        import tempfile

        logger.info(f"Merging audio for job {self.job_id}")

        # Get inputs
        video_result = self.input()["video"].get_output_data()
        audio_result = self.input()["audio"].get_output_data()

        combined_path = video_result["combined_path"]

        # Check if audio was generated
        if not audio_result.get("audio_generated"):
            logger.warning("No audio generated, skipping merge")
            return {
                "final_path": combined_path,
                "audio_merged": False,
            }

        audio_path = audio_result["audio_path"]

        # Merge
        temp_dir = Path(tempfile.gettempdir()) / f"job_{self.job_id}"
        final_path = temp_dir / "final_with_audio.mp4"

        success, output_path, error = await asyncio.to_thread(
            add_audio_to_video,
            combined_path,
            audio_path,
            str(final_path),
            audio_fade_duration=0.5,
        )

        if not success:
            logger.warning(f"Failed to merge audio: {error}, using video without audio")
            return {
                "final_path": combined_path,
                "audio_merged": False,
            }

        logger.info(f"Audio merged successfully: {output_path}")

        return {
            "final_path": output_path,
            "audio_merged": True,
        }


class VideoStorageTask(AsyncCampaignTask):
    """
    Task 8: Store final video in database.

    This is the final task in the pipeline.
    """

    def requires(self):
        """Depends on audio merging."""
        return AudioMergingTask(
            job_id=self.job_id,
            campaign_id=self.campaign_id
        )

    async def async_run(self) -> Dict[str, Any]:
        """Store final video in database."""
        from ..database import get_db

        logger.info(f"Storing final video for job {self.job_id}")

        # Get final video path
        merge_result = self.input().get_output_data()
        final_path = merge_result["final_path"]

        # Read video file
        with open(final_path, 'rb') as f:
            video_data = f.read()

        # Store in database
        with get_db() as conn:
            conn.execute(
                """
                UPDATE generated_videos
                SET video_data = ?
                WHERE id = ?
                """,
                (video_data, self.job_id)
            )
            conn.commit()

        combined_url = f"/api/v3/videos/{self.job_id}/combined"

        logger.info(f"Stored final video (size={len(video_data)} bytes)")

        # Update job status to completed
        self.update_job_status("completed", video_url=combined_url)

        return {
            "video_url": combined_url,
            "video_size": len(video_data),
        }
</file>

<file path="backend/auth.py">
"""Authentication utilities for JWT tokens and password hashing."""
import os
import secrets
import bcrypt
from datetime import datetime, timedelta
from typing import Optional, Dict, Any
from jose import JWTError, jwt
from fastapi import Depends, HTTPException, status, Security, Request
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials, APIKeyHeader
try:
    from .database import (
        get_user_by_username,
        update_user_last_login,
        get_api_key_by_hash,
        update_api_key_last_used
    )
except ImportError:
    from database import (
        get_user_by_username,
        update_user_last_login,
        get_api_key_by_hash,
        update_api_key_last_used
    )

# Configuration
SECRET_KEY = os.getenv("SECRET_KEY", secrets.token_urlsafe(32))
ALGORITHM = "HS256"
# Token expiration: 5 days (7200 minutes) for persistent login
ACCESS_TOKEN_EXPIRE_MINUTES = int(os.getenv("ACCESS_TOKEN_EXPIRE_MINUTES", "7200"))

# Security schemes
bearer_scheme = HTTPBearer()
api_key_header = APIKeyHeader(name="X-API-Key", auto_error=False)

# Cookie name
COOKIE_NAME = "access_token"

def verify_password(plain_password: str, hashed_password: str) -> bool:
    """Verify a password against its hash using bcrypt."""
    # Convert to bytes and truncate to 72 bytes for bcrypt
    password_bytes = plain_password.encode('utf-8')[:72]
    hash_bytes = hashed_password.encode('utf-8')
    return bcrypt.checkpw(password_bytes, hash_bytes)

def get_password_hash(password: str) -> str:
    """Hash a password using bcrypt."""
    # Convert to bytes and truncate to 72 bytes for bcrypt
    password_bytes = password.encode('utf-8')[:72]
    salt = bcrypt.gensalt(rounds=12)
    hashed = bcrypt.hashpw(password_bytes, salt)
    return hashed.decode('utf-8')

def create_access_token(data: dict, expires_delta: Optional[timedelta] = None) -> str:
    """Create a JWT access token."""
    to_encode = data.copy()
    if expires_delta:
        expire = datetime.utcnow() + expires_delta
    else:
        expire = datetime.utcnow() + timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)

    to_encode.update({"exp": expire})
    encoded_jwt = jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)
    return encoded_jwt

def create_asset_access_token(asset_id: str, expires_delta: Optional[timedelta] = None) -> str:
    """
    Create a temporary JWT token for public asset access.

    This allows external services (like Replicate) to download assets
    without user authentication.

    Args:
        asset_id: The asset ID to grant access to
        expires_delta: Optional expiration time (default: 2 hours)

    Returns:
        JWT token string
    """
    if expires_delta is None:
        expires_delta = timedelta(hours=2)  # Default: 2 hours

    to_encode = {
        "asset_id": asset_id,
        "type": "asset_access",
        "exp": datetime.utcnow() + expires_delta
    }

    encoded_jwt = jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)
    return encoded_jwt

def verify_asset_access_token(token: str) -> Optional[str]:
    """
    Verify an asset access token and return the asset ID.

    Args:
        token: JWT token string

    Returns:
        Asset ID if valid, None otherwise
    """
    try:
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])

        # Check token type
        if payload.get("type") != "asset_access":
            return None

        # Return asset ID
        return payload.get("asset_id")
    except JWTError:
        return None

def decode_access_token(token: str) -> Optional[Dict[str, Any]]:
    """Decode and validate a JWT token."""
    try:
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        return payload
    except JWTError:
        return None

def authenticate_user(username: str, password: str) -> Optional[Dict[str, Any]]:
    """Authenticate a user by username and password."""
    user = get_user_by_username(username)
    if not user:
        return None
    if not verify_password(password, user["hashed_password"]):
        return None
    if not user["is_active"]:
        return None
    return user

def generate_api_key() -> str:
    """Generate a secure random API key."""
    return f"sk_{secrets.token_urlsafe(32)}"

def hash_api_key(api_key: str) -> str:
    """Hash an API key for storage using bcrypt."""
    key_bytes = api_key.encode('utf-8')[:72]
    salt = bcrypt.gensalt(rounds=12)
    hashed = bcrypt.hashpw(key_bytes, salt)
    return hashed.decode('utf-8')

def verify_api_key(api_key: str, key_hash: str) -> bool:
    """Verify an API key against its hash using bcrypt."""
    key_bytes = api_key.encode('utf-8')[:72]
    hash_bytes = key_hash.encode('utf-8')
    return bcrypt.checkpw(key_bytes, hash_bytes)

# Dependency for JWT token authentication
async def get_current_user_from_token(
    credentials: HTTPAuthorizationCredentials = Depends(bearer_scheme)
) -> Dict[str, Any]:
    """Get current user from JWT token."""
    credentials_exception = HTTPException(
        status_code=status.HTTP_401_UNAUTHORIZED,
        detail="Could not validate credentials",
        headers={"WWW-Authenticate": "Bearer"},
    )

    token = credentials.credentials
    payload = decode_access_token(token)

    if payload is None:
        raise credentials_exception

    username: str = payload.get("sub")
    if username is None:
        raise credentials_exception

    user = get_user_by_username(username)
    if user is None:
        raise credentials_exception

    if not user["is_active"]:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Inactive user"
        )

    return user

# Internal function for API key authentication
def _verify_api_key_and_get_user(api_key: str) -> Optional[Dict[str, Any]]:
    """Internal function to verify API key and get user (synchronous)."""
    from .database import get_db

    with get_db() as conn:
        rows = conn.execute(
            """
            SELECT ak.*, u.username, u.email, u.is_active as user_is_active, u.is_admin
            FROM api_keys ak
            JOIN users u ON ak.user_id = u.id
            WHERE ak.is_active = 1
            """
        ).fetchall()

        for row in rows:
            if verify_api_key(api_key, row["key_hash"]):
                # Check if expired
                if row["expires_at"]:
                    expires_at = datetime.fromisoformat(row["expires_at"])
                    if datetime.utcnow() > expires_at:
                        continue

                # Check if user is active
                if not row["user_is_active"]:
                    continue

                # Update last used timestamp
                update_api_key_last_used(row["key_hash"])

                return {
                    "id": row["user_id"],
                    "username": row["username"],
                    "email": row["email"],
                    "is_active": bool(row["user_is_active"]),
                    "is_admin": bool(row["is_admin"])
                }

    return None

# Dependency for API key authentication
async def get_current_user_from_api_key(
    api_key: Optional[str] = Security(api_key_header)
) -> Optional[Dict[str, Any]]:
    """Get current user from API key (optional)."""
    if not api_key:
        return None

    return _verify_api_key_and_get_user(api_key)

# Combined authentication dependency (accepts either JWT or API key)
async def get_current_user(
    token_user: Optional[Dict[str, Any]] = Depends(lambda: None),
    api_key_user: Optional[Dict[str, Any]] = Depends(get_current_user_from_api_key)
) -> Dict[str, Any]:
    """Get current user from either JWT token or API key."""
    # Try API key first
    if api_key_user:
        return api_key_user

    # Try JWT token
    try:
        from fastapi import Request
        # This is a workaround to get the token from the request
        # In a real implementation, you would use proper dependency injection
        credentials_exception = HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Could not validate credentials",
            headers={"WWW-Authenticate": "Bearer"},
        )
        raise credentials_exception
    except:
        pass

    # If no authentication method succeeded
    raise HTTPException(
        status_code=status.HTTP_401_UNAUTHORIZED,
        detail="Not authenticated. Provide either a Bearer token or X-API-Key header.",
        headers={"WWW-Authenticate": "Bearer"},
    )

# Simplified combined authentication
async def verify_auth(
    request: Request,
    credentials: Optional[HTTPAuthorizationCredentials] = Depends(HTTPBearer(auto_error=False)),
    api_key: Optional[str] = Security(api_key_header)
) -> Dict[str, Any]:
    """Verify authentication from cookie, Bearer token, or API key."""

    # Bypass authentication in local development
    base_url = os.getenv("BASE_URL", "http://localhost:8000")
    if base_url.startswith("http://localhost") or base_url.startswith("http://127.0.0.1"):
        # Return a mock user for local development
        return {
            "id": 1,
            "username": "dev_user",
            "email": "dev@localhost",
            "is_active": True,
            "is_admin": True,
            "created_at": datetime.utcnow().isoformat()
        }

    # Try cookie first (most common for web UI)
    cookie_token = request.cookies.get(COOKIE_NAME)
    if cookie_token:
        payload = decode_access_token(cookie_token)
        if payload:
            username = payload.get("sub")
            if username:
                user = get_user_by_username(username)
                if user and user["is_active"]:
                    update_user_last_login(user["id"])
                    return user

    # Try API key
    if api_key:
        user = _verify_api_key_and_get_user(api_key)
        if user:
            return user

    # Try Bearer token
    if credentials:
        user = await get_current_user_from_token(credentials)
        if user:
            # Update last login for token auth
            update_user_last_login(user["id"])
            return user

    # No valid authentication
    raise HTTPException(
        status_code=status.HTTP_401_UNAUTHORIZED,
        detail="Not authenticated. Login required.",
        headers={"WWW-Authenticate": "Bearer"},
    )

# Admin-only dependency
async def get_current_admin_user(
    current_user: Dict[str, Any] = Depends(verify_auth)
) -> Dict[str, Any]:
    """Verify that the current user is an admin."""
    if not current_user.get("is_admin"):
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Admin privileges required"
        )
    return current_user
</file>

<file path="backend/config.py">
"""Centralized configuration management for the entire backend."""

from functools import lru_cache
from typing import Literal, Optional

from pydantic import Field, field_validator
from pydantic_settings import BaseSettings, SettingsConfigDict


class Settings(BaseSettings):
    # Main backend settings
    ENVIRONMENT: Literal["development", "staging", "production"] = "development"
    HOST: str = "0.0.0.0"
    PORT: int = Field(8000, ge=1, le=65535)
    BASE_URL: str = "https://mds.ngrok.dev"  # Set to ngrok URL for local dev, or deployed URL for production
    NGROK_URL: Optional[str] = None  # Public URL for external services (Replicate, webhooks)

    # AI/ML settings
    REPLICATE_API_KEY: Optional[str] = None
    OPENAI_API_KEY: Optional[str] = None
    ANTHROPIC_API_KEY: Optional[str] = None
    OPENROUTER_API_KEY: Optional[str] = None
    XAI_API_KEY: Optional[str] = None  # For Grok models

    # Storage settings
    VIDEO_STORAGE_PATH: str = "./DATA/videos"

    # Upscaler settings
    UPSCALER_MODEL: str = "philz1337x/clarity-upscaler"  # Configurable Replicate upscaler model

    # Video generation settings (for image-pair to video workflow)
    VIDEO_GENERATION_MODEL: Literal["veo3", "hailuo-2.0"] = "veo3"  # Default model for image-to-video generation

    # Prompt parser settings (from prompt_parser_service)
    APP_ENV: Literal["development", "staging", "production"] = "development"
    LOG_LEVEL: str = "INFO"
    REDIS_URL: str = "redis://localhost:6379/0"  # Will use SQLite instead
    RATE_LIMIT_PER_MINUTE: int = Field(10, ge=1)  # Aligned to PRD
    USE_MOCK_LLM: bool = False
    DEFAULT_LLM_PROVIDER: str = Field("openrouter", description="Default LLM provider (openrouter for GPT-5-nano, openai for GPT-4o, claude)")

    model_config = SettingsConfigDict(
        env_file=".env",
        env_file_encoding="utf-8",
        case_sensitive=False,
        extra="allow",  # Allow extra env vars
    )

    @field_validator("RATE_LIMIT_PER_MINUTE", mode="before")
    @classmethod
    def _clean_rate_limit(cls, value):
        if isinstance(value, str):
            value = value.strip()
            if value == "":
                return None
        return int(value) if value is not None else None

    @field_validator("USE_MOCK_LLM", mode="before")
    @classmethod
    def _clean_use_mock(cls, value):
        if isinstance(value, str):
            normalized = value.strip().lower()
            if normalized in {"1", "true", "yes", "on"}:
                return True
            if normalized in {"0", "false", "no", "off", ""}:
                return False
        return value


def get_settings() -> Settings:
    """Return settings instance."""
    import os
    import logging
    logger = logging.getLogger(__name__)

    # Debug: Check environment variables
    base_url_env = os.environ.get("BASE_URL", "NOT_SET")
    logger.error(f"[CONFIG DEBUG] BASE_URL environment variable: {base_url_env}")

    settings = Settings()
    logger.error(f"[CONFIG DEBUG] Settings.BASE_URL after init: {settings.BASE_URL}")
    logger.error(f"[CONFIG DEBUG] Settings dict: {settings.model_dump()}")

    return settings
</file>

<file path="backend/services/asset_downloader.py">
"""
Asset Downloader Service.

This module handles downloading assets from URLs, validating them, and storing
them as blobs in the database for V3 API asset handling.
"""

import logging
import uuid
import requests
import mimetypes
import subprocess
import tempfile
import os
from typing import Optional, Dict, Any, Tuple
from pathlib import Path
from PIL import Image
import io

# Optional: python-magic for file type validation (falls back to mimetypes if not available)
try:
    import magic

    MAGIC_AVAILABLE = True
except ImportError:
    MAGIC_AVAILABLE = False

from ..database_helpers import get_db

# Configure logging
logger = logging.getLogger(__name__)

# Configuration constants
MAX_DOWNLOAD_SIZE_MB = 100  # Maximum file size to download
DOWNLOAD_TIMEOUT_SECONDS = 60  # Timeout for download requests
ALLOWED_ASSET_DOMAINS = ["*"]  # Allow all domains for now

# Supported content types
SUPPORTED_IMAGE_TYPES = [
    "image/jpeg",
    "image/jpg",
    "image/png",
    "image/webp",
    "image/gif",
    "image/svg+xml",
]

SUPPORTED_VIDEO_TYPES = [
    "video/mp4",
    "video/webm",
    "video/quicktime",
    "video/x-msvideo",
    "video/x-matroska",
]

SUPPORTED_AUDIO_TYPES = [
    "audio/mpeg",
    "audio/mp3",
    "audio/wav",
    "audio/ogg",
    "audio/webm",
]

SUPPORTED_DOCUMENT_TYPES = [
    "application/pdf",
    "application/msword",
    "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
]


class AssetDownloadError(Exception):
    """Exception raised when asset download fails."""

    pass


def download_asset_from_url(
    url: str, asset_type: str, expected_content_type: Optional[str] = None
) -> Tuple[bytes, str, Dict[str, Any]]:
    """
    Download an asset from a URL and validate it.

    Args:
        url: The URL to download from
        asset_type: Expected asset type ("image", "video", "audio", "document")
        expected_content_type: Optional expected MIME type

    Returns:
        Tuple of (data, content_type, metadata)

    Raises:
        AssetDownloadError: If download fails or validation fails
    """
    logger.info(f"Downloading asset from URL: {url[:100]}...")

    try:
        # Validate URL format
        if not url.startswith(("http://", "https://")):
            raise AssetDownloadError("URL must start with http:// or https://")

        # TODO: Add domain validation if ALLOWED_ASSET_DOMAINS is not ["*"]

        # Make HEAD request first to check size
        try:
            head_response = requests.head(
                url,
                timeout=10,
                allow_redirects=True,
                headers={"User-Agent": "AdVideoGeneration/1.0"},
            )
            content_length = head_response.headers.get("Content-Length")

            if content_length:
                size_mb = int(content_length) / (1024 * 1024)
                if size_mb > MAX_DOWNLOAD_SIZE_MB:
                    raise AssetDownloadError(
                        f"File too large: {size_mb:.1f}MB (max: {MAX_DOWNLOAD_SIZE_MB}MB)"
                    )
        except requests.RequestException as e:
            logger.warning(f"HEAD request failed: {e}, proceeding with GET")

        # Download the asset
        response = requests.get(
            url,
            timeout=DOWNLOAD_TIMEOUT_SECONDS,
            stream=True,
            headers={"User-Agent": "AdVideoGeneration/1.0"},
        )
        response.raise_for_status()

        # Check Content-Length from response headers
        content_length = response.headers.get("Content-Length")
        if content_length:
            size_mb = int(content_length) / (1024 * 1024)
            if size_mb > MAX_DOWNLOAD_SIZE_MB:
                raise AssetDownloadError(
                    f"File too large: {size_mb:.1f}MB (max: {MAX_DOWNLOAD_SIZE_MB}MB)"
                )

        # Download in chunks with size limit
        data = bytearray()
        max_size_bytes = MAX_DOWNLOAD_SIZE_MB * 1024 * 1024

        for chunk in response.iter_content(chunk_size=8192):
            if chunk:
                data.extend(chunk)
                if len(data) > max_size_bytes:
                    raise AssetDownloadError(
                        f"File exceeds maximum size of {MAX_DOWNLOAD_SIZE_MB}MB"
                    )

        data = bytes(data)
        logger.info(f"Downloaded {len(data)} bytes from {url[:50]}...")

        # Get content type
        content_type = response.headers.get("Content-Type", "").split(";")[0].strip()
        if not content_type:
            # Try to guess from URL extension
            content_type, _ = mimetypes.guess_type(url)
            if not content_type:
                # Try magic library for file type detection if available
                if MAGIC_AVAILABLE:
                    try:
                        mime = magic.Magic(mime=True)
                        content_type = mime.from_buffer(data)
                    except Exception as e:
                        logger.warning(f"Failed to detect content type with magic: {e}")
                        content_type = "application/octet-stream"
                else:
                    logger.warning(
                        "Content type detection: python-magic not available, using fallback"
                    )
                    content_type = "application/octet-stream"

        # Validate content type matches asset type
        _validate_content_type(content_type, asset_type)

        # Extract metadata based on asset type
        metadata = _extract_metadata(data, content_type, asset_type)

        logger.info(f"Successfully downloaded and validated asset: {content_type}")
        return data, content_type, metadata

    except requests.RequestException as e:
        logger.error(f"Failed to download asset from {url}: {e}")
        raise AssetDownloadError(f"Download failed: {str(e)}")
    except Exception as e:
        logger.error(f"Unexpected error downloading asset: {e}")
        raise AssetDownloadError(f"Unexpected error: {str(e)}")


def store_blob(data: bytes, content_type: str) -> str:
    """
    Store asset data as a blob in the database.

    Args:
        data: The asset data as bytes
        content_type: The MIME type of the asset

    Returns:
        The blob ID (UUID)

    Raises:
        Exception: If storage fails
    """
    blob_id = str(uuid.uuid4())
    size_bytes = len(data)

    logger.info(f"Storing blob {blob_id} ({size_bytes} bytes, {content_type})")

    try:
        with get_db() as conn:
            conn.execute(
                """
                INSERT INTO asset_blobs (id, data, content_type, size_bytes)
                VALUES (?, ?, ?, ?)
                """,
                (blob_id, data, content_type, size_bytes),
            )
            conn.commit()

        logger.info(f"Successfully stored blob {blob_id}")
        return blob_id

    except Exception as e:
        logger.error(f"Failed to store blob: {e}")
        raise


def get_blob_by_id(blob_id: str) -> Optional[Tuple[bytes, str]]:
    """
    Retrieve a blob from the database.

    Args:
        blob_id: The blob UUID

    Returns:
        Tuple of (data, content_type) or None if not found
    """
    try:
        with get_db() as conn:
            cursor = conn.execute(
                "SELECT data, content_type FROM asset_blobs WHERE id = ?", (blob_id,)
            )
            row = cursor.fetchone()

            if row:
                return bytes(row["data"]), row["content_type"]
            return None

    except Exception as e:
        logger.error(f"Failed to retrieve blob {blob_id}: {e}")
        return None


def _validate_content_type(content_type: str, asset_type: str) -> None:
    """
    Validate that content type matches expected asset type.

    Args:
        content_type: The MIME type
        asset_type: Expected asset type ("image", "video", "audio", "document")

    Raises:
        AssetDownloadError: If content type doesn't match asset type
    """
    content_type_lower = content_type.lower()

    if asset_type == "image":
        if content_type_lower not in SUPPORTED_IMAGE_TYPES:
            raise AssetDownloadError(
                f"Invalid image type: {content_type}. Supported: {', '.join(SUPPORTED_IMAGE_TYPES)}"
            )
    elif asset_type == "video":
        if content_type_lower not in SUPPORTED_VIDEO_TYPES:
            raise AssetDownloadError(
                f"Invalid video type: {content_type}. Supported: {', '.join(SUPPORTED_VIDEO_TYPES)}"
            )
    elif asset_type == "audio":
        if content_type_lower not in SUPPORTED_AUDIO_TYPES:
            raise AssetDownloadError(
                f"Invalid audio type: {content_type}. Supported: {', '.join(SUPPORTED_AUDIO_TYPES)}"
            )
    elif asset_type == "document":
        if content_type_lower not in SUPPORTED_DOCUMENT_TYPES:
            raise AssetDownloadError(
                f"Invalid document type: {content_type}. Supported: {', '.join(SUPPORTED_DOCUMENT_TYPES)}"
            )
    else:
        raise AssetDownloadError(f"Unknown asset type: {asset_type}")


def _extract_metadata(
    data: bytes, content_type: str, asset_type: str
) -> Dict[str, Any]:
    """
    Extract metadata from asset data.

    Args:
        data: The asset data
        content_type: The MIME type
        asset_type: The asset type

    Returns:
        Dictionary of metadata (width, height, duration, etc.)
    """
    metadata: Dict[str, Any] = {"size": len(data)}

    try:
        if asset_type == "image":
            # Extract image dimensions
            image = Image.open(io.BytesIO(data))
            metadata["width"] = image.width
            metadata["height"] = image.height
            metadata["format"] = image.format.lower() if image.format else "unknown"

        elif asset_type == "video":
            # For now, just extract format from content type
            # TODO: Use ffprobe for video metadata extraction
            format_map = {
                "video/mp4": "mp4",
                "video/webm": "webm",
                "video/quicktime": "mov",
                "video/x-msvideo": "avi",
                "video/x-matroska": "mkv",
            }
            metadata["format"] = format_map.get(content_type.lower(), "unknown")
            # Placeholder values - would need ffprobe for real extraction
            metadata["width"] = None
            metadata["height"] = None
            metadata["duration"] = None

        elif asset_type == "audio":
            # Extract format from content type
            format_map = {
                "audio/mpeg": "mp3",
                "audio/mp3": "mp3",
                "audio/wav": "wav",
                "audio/ogg": "ogg",
                "audio/webm": "webm",
            }
            metadata["format"] = format_map.get(content_type.lower(), "unknown")
            metadata["duration"] = None  # Would need audio library for real extraction

        elif asset_type == "document":
            # Extract format from content type
            format_map = {
                "application/pdf": "pdf",
                "application/msword": "doc",
                "application/vnd.openxmlformats-officedocument.wordprocessingml.document": "docx",
            }
            metadata["format"] = format_map.get(content_type.lower(), "unknown")
            metadata["page_count"] = None  # Would need PDF library for real extraction

    except Exception as e:
        logger.warning(f"Failed to extract metadata: {e}")
        # Return basic metadata even if extraction fails
        # Handle special cases like svg+xml -> svg
        format_str = content_type.split("/")[-1] if "/" in content_type else "unknown"
        # Clean up compound formats (e.g., svg+xml -> svg)
        if "+" in format_str:
            format_str = format_str.split("+")[0]
        metadata["format"] = format_str

    return metadata


def generate_thumbnail(
    asset_data: bytes, content_type: str, asset_type: str, max_size: int = 128
) -> Optional[bytes]:
    """
    Generate a thumbnail for an asset.

    Args:
        asset_data: The asset binary data
        content_type: MIME type of the asset
        asset_type: Asset type ("image", "video", "audio", "document")
        max_size: Maximum dimension for thumbnail (default 128px)

    Returns:
        Thumbnail data as bytes, or None if generation fails
    """
    try:
        if asset_type == "image" and content_type.startswith("image/"):
            # Generate thumbnail for images using PIL
            image = Image.open(io.BytesIO(asset_data))

            # Convert to RGB if necessary (for PNG with transparency, etc.)
            if image.mode in ("RGBA", "LA", "P"):
                # Create white background
                background = Image.new("RGB", image.size, (255, 255, 255))
                if image.mode == "P":
                    image = image.convert("RGBA")
                background.paste(
                    image, mask=image.split()[-1] if image.mode == "RGBA" else None
                )
                image = background

            # Generate thumbnail
            image.thumbnail((max_size, max_size), Image.Resampling.LANCZOS)

            # Save as JPEG
            output = io.BytesIO()
            image.save(output, format="JPEG", quality=85)
            return output.getvalue()

        elif asset_type == "video" and content_type.startswith("video/"):
            # Generate thumbnail for videos using ffmpeg
            # Write asset data to temporary file
            with tempfile.NamedTemporaryFile(suffix=".mp4", delete=False) as temp_file:
                temp_file.write(asset_data)
                temp_input = temp_file.name

            try:
                # Create temporary output file for thumbnail
                with tempfile.NamedTemporaryFile(
                    suffix=".jpg", delete=False
                ) as temp_output:
                    temp_thumb = temp_output.name

                # Use ffmpeg to extract frame at 1 second
                cmd = [
                    "ffmpeg",
                    "-y",
                    "-i",
                    temp_input,
                    "-ss",
                    "00:00:01",
                    "-vframes",
                    "1",
                    "-vf",
                    f"scale='min({max_size},iw)':'min({max_size},ih)':force_original_aspect_ratio=decrease",
                    "-q:v",
                    "3",
                    temp_thumb,
                ]

                result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)

                if result.returncode == 0 and os.path.exists(temp_thumb):
                    with open(temp_thumb, "rb") as f:
                        return f.read()
                else:
                    logger.warning(
                        f"ffmpeg thumbnail generation failed: {result.stderr}"
                    )
                    return None

            finally:
                # Clean up temporary files
                try:
                    os.unlink(temp_input)
                    if "temp_thumb" in locals():
                        os.unlink(temp_thumb)
                except:
                    pass

        elif asset_type == "document" and content_type == "application/pdf":
            # For PDFs, we could generate a thumbnail of the first page
            # This would require additional libraries like PyPDF2 + PIL
            # For now, return None
            logger.info("PDF thumbnail generation not implemented yet")
            return None

        else:
            # No thumbnail generation for audio or other types
            return None

    except Exception as e:
        logger.warning(f"Failed to generate thumbnail: {e}")
        return None


def generate_and_store_thumbnail(
    asset_data: bytes, content_type: str, asset_type: str, max_size: int = 128
) -> Optional[str]:
    """
    Generate a thumbnail and store it as a blob.

    Args:
        asset_data: The asset binary data
        content_type: MIME type of the asset
        asset_type: Asset type ("image", "video", "audio", "document")
        max_size: Maximum dimension for thumbnail

    Returns:
        Blob ID of the stored thumbnail, or None if generation fails
    """
    thumbnail_data = generate_thumbnail(asset_data, content_type, asset_type, max_size)
    if thumbnail_data:
        try:
            blob_id = store_blob(thumbnail_data, "image/jpeg")
            logger.info(f"Generated and stored thumbnail blob {blob_id}")
            return blob_id
        except Exception as e:
            logger.error(f"Failed to store thumbnail blob: {e}")
            return None
    return None
</file>

<file path="backend/services/replicate_client.py">
"""
Replicate API client for video generation with polling logic.

This module handles all interactions with the Replicate API for image and video generation,
including polling for prediction status with exponential backoff retry logic.
"""

import logging
import time
from os import environ
from typing import Dict, List, Optional, Any
import requests

# Configure logging
logger = logging.getLogger(__name__)


class ReplicateClient:
    """
    Client for interacting with Replicate API for image and video generation.

    This client provides methods for:
    - Generating images using Flux-Schnell model (or other image models)
    - Generating videos using SkyReels-2 model
    - Polling prediction status with automatic retries
    - Estimating costs for operations

    Note: Image upscaling is handled through the standard image generation
    workflow via generate_image(). Upscaler models from the 'super-resolution'
    collection accept an 'image' input parameter along with upscaling parameters
    (scale, dynamic, sharpen, etc.) instead of just 'prompt'.

    Attributes:
        api_key (str): Replicate API key for authentication
        base_url (str): Base URL for Replicate API
        session (requests.Session): HTTP session for API requests
    """

    # Model pricing (in USD)
    FLUX_SCHNELL_PRICE_PER_IMAGE = 0.003
    SKYREELS2_PRICE_PER_SECOND = 0.10
    UPSCALER_PRICE_PER_IMAGE = 0.016  # Reference pricing for clarity-upscaler
    VEO3_PRICE_PER_SECOND = 0.15  # Estimated, update when official pricing available
    HAILUO2_PRICE_PER_GENERATION = 0.20  # Estimated, update when official pricing available

    # Default models
    DEFAULT_IMAGE_MODEL = "black-forest-labs/flux-schnell"
    DEFAULT_VIDEO_MODEL = "fofr/skyreels-2"
    DEFAULT_UPSCALER_MODEL = "philz1337x/clarity-upscaler"  # Configurable via settings

    # Image-to-video models (for pair generation)
    VEO3_MODEL = "google/veo-3.1"
    HAILUO2_MODEL = "minimax/hailuo-02"

    # Polling configuration
    DEFAULT_POLL_INTERVAL = 5  # seconds
    DEFAULT_TIMEOUT = 600  # 10 minutes
    MAX_BACKOFF_DELAY = 45  # seconds

    def __init__(self, api_key: Optional[str] = None):
        """
        Initialize the Replicate client.

        Args:
            api_key (str, optional): Replicate API key. If None, will attempt to
                                    read from REPLICATE_API_KEY environment variable.

        Raises:
            ValueError: If no API key is provided or found in environment
        """
        self.api_key = api_key or environ.get('REPLICATE_API_KEY')
        if not self.api_key:
            raise ValueError(
                "Replicate API key not provided. Either pass api_key parameter "
                "or set REPLICATE_API_KEY environment variable."
            )

        self.base_url = "https://api.replicate.com/v1"
        self.session = requests.Session()
        self.session.headers.update({
            "Authorization": f"Token {self.api_key}",
            "Content-Type": "application/json"
        })

        logger.info("ReplicateClient initialized successfully")

    def generate_image(
        self,
        prompt: str,
        model: str = DEFAULT_IMAGE_MODEL
    ) -> Dict[str, Any]:
        """
        Generate an image using Replicate's Flux-Schnell model.

        Args:
            prompt (str): Text description of the image to generate
            model (str): Model identifier (default: black-forest-labs/flux-schnell)

        Returns:
            dict: Response containing:
                - success (bool): Whether the request was successful
                - image_url (str): URL of the generated image (if successful)
                - error (str): Error message (if failed)
                - prediction_id (str): Replicate prediction ID for polling

        Example:
            >>> client = ReplicateClient()
            >>> result = client.generate_image("a red sports car")
            >>> if result['success']:
            ...     print(f"Image URL: {result['image_url']}")
        """
        logger.info(f"Generating image with prompt: '{prompt[:50]}...'")

        try:
            # Create prediction
            response = self.session.post(
                f"{self.base_url}/predictions",
                json={
                    "version": self._get_model_version(model),
                    "input": {"prompt": prompt}
                },
                timeout=30
            )
            response.raise_for_status()

            prediction_data = response.json()
            prediction_id = prediction_data.get('id')

            if not prediction_id:
                logger.error("No prediction ID returned from API")
                return {
                    "success": False,
                    "image_url": None,
                    "error": "No prediction ID returned from API",
                    "prediction_id": None
                }

            logger.info(f"Image generation started, prediction ID: {prediction_id}")

            # Poll for completion
            poll_result = self.poll_prediction(prediction_id)

            if poll_result['status'] == 'succeeded':
                # Extract image URL from output
                output = poll_result.get('output')
                image_url = output[0] if isinstance(output, list) else output

                logger.info(f"Image generation succeeded: {image_url}")
                return {
                    "success": True,
                    "image_url": image_url,
                    "error": None,
                    "prediction_id": prediction_id
                }
            else:
                error_msg = poll_result.get('error', f"Generation failed with status: {poll_result['status']}")
                logger.error(f"Image generation failed: {error_msg}")
                return {
                    "success": False,
                    "image_url": None,
                    "error": error_msg,
                    "prediction_id": prediction_id
                }

        except requests.exceptions.Timeout:
            logger.error("Request timeout while generating image")
            return {
                "success": False,
                "image_url": None,
                "error": "Request timeout",
                "prediction_id": None
            }
        except requests.exceptions.RequestException as e:
            logger.error(f"Network error while generating image: {str(e)}")
            return {
                "success": False,
                "image_url": None,
                "error": f"Network error: {str(e)}",
                "prediction_id": None
            }
        except Exception as e:
            logger.error(f"Unexpected error while generating image: {str(e)}")
            return {
                "success": False,
                "image_url": None,
                "error": f"Unexpected error: {str(e)}",
                "prediction_id": None
            }

    def generate_video(
        self,
        image_urls: List[str],
        model: str = DEFAULT_VIDEO_MODEL
    ) -> Dict[str, Any]:
        """
        Generate a video by stitching together images using SkyReels-2 model.

        Args:
            image_urls (list[str]): List of image URLs to stitch into a video
            model (str): Model identifier (default: fofr/skyreels-2)

        Returns:
            dict: Response containing:
                - success (bool): Whether the request was successful
                - video_url (str): URL of the generated video (if successful)
                - error (str): Error message (if failed)
                - prediction_id (str): Replicate prediction ID for polling
                - duration_seconds (int): Duration of the generated video

        Example:
            >>> client = ReplicateClient()
            >>> images = ["https://example.com/img1.jpg", "https://example.com/img2.jpg"]
            >>> result = client.generate_video(images)
            >>> if result['success']:
            ...     print(f"Video URL: {result['video_url']}")
        """
        if not image_urls:
            logger.error("No image URLs provided for video generation")
            return {
                "success": False,
                "video_url": None,
                "error": "No image URLs provided",
                "prediction_id": None,
                "duration_seconds": 0
            }

        logger.info(f"Generating video from {len(image_urls)} images")

        try:
            # Create prediction for video generation
            response = self.session.post(
                f"{self.base_url}/predictions",
                json={
                    "version": self._get_model_version(model),
                    "input": {
                        "image_urls": image_urls
                    }
                },
                timeout=30
            )
            response.raise_for_status()

            prediction_data = response.json()
            prediction_id = prediction_data.get('id')

            if not prediction_id:
                logger.error("No prediction ID returned from API")
                return {
                    "success": False,
                    "video_url": None,
                    "error": "No prediction ID returned from API",
                    "prediction_id": None,
                    "duration_seconds": 0
                }

            logger.info(f"Video generation started, prediction ID: {prediction_id}")

            # Poll for completion (videos take longer, so use extended timeout)
            poll_result = self.poll_prediction(prediction_id, timeout=1200)  # 20 minutes

            if poll_result['status'] == 'succeeded':
                # Extract video URL from output
                output = poll_result.get('output')
                video_url = output[0] if isinstance(output, list) else output

                # Estimate duration based on number of images (rough estimate)
                duration_seconds = len(image_urls) * 2  # Assume ~2 seconds per image

                logger.info(f"Video generation succeeded: {video_url}")
                return {
                    "success": True,
                    "video_url": video_url,
                    "error": None,
                    "prediction_id": prediction_id,
                    "duration_seconds": duration_seconds
                }
            else:
                error_msg = poll_result.get('error', f"Generation failed with status: {poll_result['status']}")
                logger.error(f"Video generation failed: {error_msg}")
                return {
                    "success": False,
                    "video_url": None,
                    "error": error_msg,
                    "prediction_id": prediction_id,
                    "duration_seconds": 0
                }

        except requests.exceptions.Timeout:
            logger.error("Request timeout while generating video")
            return {
                "success": False,
                "video_url": None,
                "error": "Request timeout",
                "prediction_id": None,
                "duration_seconds": 0
            }
        except requests.exceptions.RequestException as e:
            logger.error(f"Network error while generating video: {str(e)}")
            return {
                "success": False,
                "video_url": None,
                "error": f"Network error: {str(e)}",
                "prediction_id": None,
                "duration_seconds": 0
            }
        except Exception as e:
            logger.error(f"Unexpected error while generating video: {str(e)}")
            return {
                "success": False,
                "video_url": None,
                "error": f"Unexpected error: {str(e)}",
                "prediction_id": None,
                "duration_seconds": 0
            }

    def poll_prediction(
        self,
        prediction_id: str,
        timeout: int = DEFAULT_TIMEOUT,
        interval: int = DEFAULT_POLL_INTERVAL
    ) -> Dict[str, Any]:
        """
        Poll a prediction until it completes or times out.

        Implements exponential backoff on errors:
        - First retry: 5 seconds
        - Second retry: 15 seconds
        - Third+ retry: 45 seconds

        Args:
            prediction_id (str): The prediction ID to poll
            timeout (int): Maximum time to wait in seconds (default: 600)
            interval (int): Polling interval in seconds (default: 5)

        Returns:
            dict: Prediction result containing:
                - status (str): Final status (succeeded, failed, canceled, timeout)
                - output (any): Output data if succeeded
                - error (str): Error message if failed

        Example:
            >>> client = ReplicateClient()
            >>> result = client.poll_prediction("abc123")
            >>> if result['status'] == 'succeeded':
            ...     print(result['output'])
        """
        logger.info(f"Polling prediction {prediction_id} (timeout: {timeout}s, interval: {interval}s)")

        start_time = time.time()
        retry_count = 0
        backoff_delays = [5, 15, 45]  # Exponential backoff sequence

        while True:
            # Check timeout
            elapsed = time.time() - start_time
            if elapsed > timeout:
                logger.error(f"Polling timeout after {elapsed:.1f}s")
                return {
                    "status": "timeout",
                    "output": None,
                    "error": f"Polling timeout after {timeout} seconds"
                }

            try:
                # Get prediction status
                response = self.session.get(
                    f"{self.base_url}/predictions/{prediction_id}",
                    timeout=30
                )
                response.raise_for_status()

                data = response.json()
                status = data.get('status')

                logger.debug(f"Prediction {prediction_id} status: {status}")

                # Check if prediction is complete
                if status == 'succeeded':
                    logger.info(f"Prediction {prediction_id} succeeded")
                    return {
                        "status": "succeeded",
                        "output": data.get('output'),
                        "error": None
                    }
                elif status == 'failed':
                    error_msg = data.get('error', 'Unknown error')
                    logger.error(f"Prediction {prediction_id} failed: {error_msg}")
                    return {
                        "status": "failed",
                        "output": None,
                        "error": error_msg
                    }
                elif status == 'canceled':
                    logger.warning(f"Prediction {prediction_id} was canceled")
                    return {
                        "status": "canceled",
                        "output": None,
                        "error": "Prediction was canceled"
                    }

                # Still processing, wait before next poll
                time.sleep(interval)
                retry_count = 0  # Reset retry count on successful poll

            except requests.exceptions.HTTPError as e:
                # Handle rate limiting (429) specially
                if e.response.status_code == 429:
                    logger.warning(f"Rate limit hit, backing off...")
                    delay = backoff_delays[min(retry_count, len(backoff_delays) - 1)]
                    time.sleep(delay)
                    retry_count += 1
                    continue
                else:
                    logger.error(f"HTTP error while polling: {str(e)}")
                    return {
                        "status": "failed",
                        "output": None,
                        "error": f"HTTP error: {str(e)}"
                    }

            except requests.exceptions.Timeout:
                logger.warning(f"Poll request timeout, retrying with backoff...")
                delay = backoff_delays[min(retry_count, len(backoff_delays) - 1)]
                time.sleep(delay)
                retry_count += 1
                continue

            except requests.exceptions.RequestException as e:
                logger.error(f"Network error while polling: {str(e)}")
                # Apply backoff for network errors
                if retry_count < 3:
                    delay = backoff_delays[retry_count]
                    logger.warning(f"Retrying after {delay}s backoff...")
                    time.sleep(delay)
                    retry_count += 1
                    continue
                else:
                    # Max retries exceeded
                    return {
                        "status": "failed",
                        "output": None,
                        "error": f"Network error after {retry_count} retries: {str(e)}"
                    }

            except Exception as e:
                logger.error(f"Unexpected error while polling: {str(e)}")
                return {
                    "status": "failed",
                    "output": None,
                    "error": f"Unexpected error: {str(e)}"
                }

    def estimate_cost(self, num_images: int, video_duration: int) -> float:
        """
        Calculate estimated cost for image and video generation.

        Pricing:
        - Flux-Schnell: $0.003 per image
        - SkyReels-2: $0.10 per second of video

        Args:
            num_images (int): Number of images to generate
            video_duration (int): Duration of video in seconds

        Returns:
            float: Total estimated cost in USD

        Example:
            >>> client = ReplicateClient()
            >>> cost = client.estimate_cost(num_images=10, video_duration=20)
            >>> print(f"Estimated cost: ${cost:.2f}")
            Estimated cost: $2.03
        """
        image_cost = num_images * self.FLUX_SCHNELL_PRICE_PER_IMAGE
        video_cost = video_duration * self.SKYREELS2_PRICE_PER_SECOND
        total_cost = image_cost + video_cost

        logger.info(
            f"Cost estimate - Images: {num_images} x ${self.FLUX_SCHNELL_PRICE_PER_IMAGE} = ${image_cost:.3f}, "
            f"Video: {video_duration}s x ${self.SKYREELS2_PRICE_PER_SECOND} = ${video_cost:.2f}, "
            f"Total: ${total_cost:.2f}"
        )

        return total_cost

    def generate_video_from_pair(
        self,
        image1_url: str,
        image2_url: str,
        model: str = "veo3",
        duration: Optional[float] = None,
        prompt: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        Generate a video from a pair of images (first frame and last frame).

        Supports Veo 3.1 and Hailuo-02 models for image-to-video generation.

        Args:
            image1_url: URL of the first/starting image
            image2_url: URL of the second/ending image
            model: Model to use ('veo3' or 'hailuo-2.0')
            duration: Optional video duration in seconds (model-specific defaults apply)
            prompt: Optional text prompt to guide the generation

        Returns:
            dict: Response containing:
                - success (bool): Whether the request was successful
                - video_url (str): URL of the generated video (if successful)
                - error (str): Error message (if failed)
                - prediction_id (str): Replicate prediction ID for polling
                - duration_seconds (float): Duration of the generated video

        Example:
            >>> client = ReplicateClient()
            >>> result = client.generate_video_from_pair(
            ...     "https://example.com/img1.jpg",
            ...     "https://example.com/img2.jpg",
            ...     model="veo3",
            ...     duration=5.0
            ... )
            >>> if result['success']:
            ...     print(f"Video URL: {result['video_url']}")
        """
        logger.info(f"Generating video from image pair using {model}")

        # Determine which model to use
        if model.lower() in ["veo3", "veo-3", "veo-3.1"]:
            return self._generate_with_veo3(image1_url, image2_url, duration, prompt)
        elif model.lower() in ["hailuo-2.0", "hailuo2", "hailuo-02"]:
            return self._generate_with_hailuo2(image1_url, image2_url, duration, prompt)
        else:
            logger.error(f"Unsupported model: {model}")
            return {
                "success": False,
                "video_url": None,
                "error": f"Unsupported model: {model}. Use 'veo3' or 'hailuo-2.0'",
                "prediction_id": None,
                "duration_seconds": 0.0,
            }

    def _generate_with_veo3(
        self,
        image1_url: str,
        image2_url: str,
        duration: Optional[float],
        prompt: Optional[str],
    ) -> Dict[str, Any]:
        """Generate video using Veo 3.1 model with first and last frame."""
        try:
            # Veo3 only accepts duration of 4, 6, or 8 seconds
            requested_duration = float(duration) if duration else 8.0

            # Round to nearest valid duration
            if requested_duration <= 5.0:
                valid_duration = 4
            elif requested_duration <= 7.0:
                valid_duration = 6
            else:
                valid_duration = 8

            if requested_duration != valid_duration:
                logger.info(f"Rounded duration from {requested_duration}s to {valid_duration}s for Veo3 compatibility")

            # Build input parameters for Veo 3.1
            input_params = {
                "image": image1_url,  # First frame
                "last_frame": image2_url,  # Last frame
                "duration": valid_duration,  # Must be 4, 6, or 8 (int)
                "resolution": "1080p",  # Default to high quality
                "aspect_ratio": "16:9",  # Default aspect ratio
                "generate_audio": False,  # No audio for now
                "prompt": prompt or "Smooth transition between images",  # Required by Veo3
            }

            logger.error(f"[VEO3 DEBUG] Image1 URL: {image1_url}")
            logger.error(f"[VEO3 DEBUG] Image2 URL: {image2_url}")
            logger.error(f"[VEO3 DEBUG] Duration: {valid_duration}s (requested={requested_duration}s)")
            logger.info(f"Creating Veo3 prediction with rounded duration={valid_duration}s (requested={requested_duration}s)")

            # Create prediction
            response = self.session.post(
                f"{self.base_url}/predictions",
                json={
                    "version": self._get_model_version(self.VEO3_MODEL),
                    "input": input_params,
                },
                timeout=30,
            )
            if response.status_code != 201:
                logger.error(f"Veo3 API error ({response.status_code}): {response.text}")
            response.raise_for_status()

            prediction_data = response.json()
            prediction_id = prediction_data.get("id")

            if not prediction_id:
                logger.error("No prediction ID returned from Veo3 API")
                return {
                    "success": False,
                    "video_url": None,
                    "error": "No prediction ID returned from API",
                    "prediction_id": None,
                    "duration_seconds": 0.0,
                }

            logger.info(f"Veo3 video generation started, prediction ID: {prediction_id}")

            # Poll for completion (videos take longer, use 20min timeout)
            poll_result = self.poll_prediction(prediction_id, timeout=1200)

            if poll_result["status"] == "succeeded":
                # Extract video URL from output
                output = poll_result.get("output")
                video_url = output if isinstance(output, str) else output[0]

                logger.info(f"Veo3 video generation succeeded: {video_url}")
                return {
                    "success": True,
                    "video_url": video_url,
                    "error": None,
                    "prediction_id": prediction_id,
                    "duration_seconds": duration or 8,
                }
            else:
                error_msg = poll_result.get(
                    "error", f"Generation failed with status: {poll_result['status']}"
                )
                logger.error(f"Veo3 video generation failed: {error_msg}")
                return {
                    "success": False,
                    "video_url": None,
                    "error": error_msg,
                    "prediction_id": prediction_id,
                    "duration_seconds": 0.0,
                }

        except Exception as e:
            logger.error(f"Error generating video with Veo3: {e}", exc_info=True)
            return {
                "success": False,
                "video_url": None,
                "error": f"Veo3 generation error: {str(e)}",
                "prediction_id": None,
                "duration_seconds": 0.0,
            }

    def _generate_with_hailuo2(
        self,
        image1_url: str,
        image2_url: str,
        duration: Optional[float],
        prompt: Optional[str],
    ) -> Dict[str, Any]:
        """Generate video using Hailuo-02 model with first and last frame."""
        try:
            # Build input parameters for Hailuo-02
            input_params = {
                "first_frame_image": image1_url,
                "last_frame_image": image2_url,
                "duration": int(duration) if duration else 6,  # 6 or 10 seconds
                "resolution": "1080p",  # "512p", "768p", or "1080p"
                "prompt_optimizer": True,
            }

            if prompt:
                input_params["prompt"] = prompt
            else:
                # Default prompt for smooth transition
                input_params["prompt"] = "smooth cinematic transition between images"

            logger.info(f"Creating Hailuo-02 prediction with params: {input_params}")

            # Create prediction
            response = self.session.post(
                f"{self.base_url}/predictions",
                json={
                    "version": self._get_model_version(self.HAILUO2_MODEL),
                    "input": input_params,
                },
                timeout=30,
            )
            response.raise_for_status()

            prediction_data = response.json()
            prediction_id = prediction_data.get("id")

            if not prediction_id:
                logger.error("No prediction ID returned from Hailuo-02 API")
                return {
                    "success": False,
                    "video_url": None,
                    "error": "No prediction ID returned from API",
                    "prediction_id": None,
                    "duration_seconds": 0.0,
                }

            logger.info(f"Hailuo-02 video generation started, prediction ID: {prediction_id}")

            # Poll for completion (videos take longer, use 20min timeout)
            poll_result = self.poll_prediction(prediction_id, timeout=1200)

            if poll_result["status"] == "succeeded":
                # Extract video URL from output
                output = poll_result.get("output")
                video_url = output if isinstance(output, str) else output[0]

                logger.info(f"Hailuo-02 video generation succeeded: {video_url}")
                return {
                    "success": True,
                    "video_url": video_url,
                    "error": None,
                    "prediction_id": prediction_id,
                    "duration_seconds": input_params["duration"],
                }
            else:
                error_msg = poll_result.get(
                    "error", f"Generation failed with status: {poll_result['status']}"
                )
                logger.error(f"Hailuo-02 video generation failed: {error_msg}")
                return {
                    "success": False,
                    "video_url": None,
                    "error": error_msg,
                    "prediction_id": prediction_id,
                    "duration_seconds": 0.0,
                }

        except Exception as e:
            logger.error(f"Error generating video with Hailuo-02: {e}", exc_info=True)
            return {
                "success": False,
                "video_url": None,
                "error": f"Hailuo-02 generation error: {str(e)}",
                "prediction_id": None,
                "duration_seconds": 0.0,
            }

    def _get_model_version(self, model: str) -> str:
        """
        Get the model version string for Replicate API.

        For simplicity, this returns the model string as-is. In production,
        you would maintain a mapping of model names to their version hashes.

        Args:
            model (str): Model identifier

        Returns:
            str: Model version identifier
        """
        # In a real implementation, you would query the Replicate API
        # to get the latest version hash for the model, or maintain
        # a mapping of model names to version hashes
        return model

    def __enter__(self):
        """Context manager entry."""
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """Context manager exit - close session."""
        self.session.close()
        logger.info("ReplicateClient session closed")
</file>

<file path="backend/services/video_combiner.py">
"""
Video Combiner Service.

This module handles combining multiple video clips into a single video using ffmpeg.
Supports concatenation with optional transitions and maintains clip order.
"""

import logging
import subprocess
import os
import tempfile
from pathlib import Path
from typing import List, Optional, Tuple, Dict, Any

logger = logging.getLogger(__name__)


class VideoCombinerError(Exception):
    """Exception raised when video combining fails."""
    pass


def check_ffmpeg_available() -> bool:
    """
    Check if ffmpeg is available on the system.

    Returns:
        True if ffmpeg is available, False otherwise
    """
    import shutil
    # Just check if ffmpeg binary exists in PATH
    # Avoids potential timeout issues with running ffmpeg -version
    return shutil.which("ffmpeg") is not None


def combine_video_clips(
    clip_paths: List[str],
    output_path: str,
    transition_duration: float = 0.0,
    output_resolution: str = "1920x1080",
    output_fps: int = 30,
    keep_audio: bool = False,
) -> Tuple[bool, Optional[str], Optional[Dict[str, Any]]]:
    """
    Combine multiple video clips into a single video file.

    Args:
        clip_paths: List of paths to video clips (in order)
        output_path: Path for the output combined video
        transition_duration: Duration of crossfade transitions in seconds (0 = no transition)
        output_resolution: Output resolution (e.g., "1920x1080")
        output_fps: Output frames per second
        keep_audio: Whether to keep audio from clips (default: False)

    Returns:
        Tuple of (success, output_path, metadata):
            - success: True if combining succeeded
            - output_path: Path to the combined video (if successful)
            - metadata: Dict with duration, size, etc. (if successful)

    Raises:
        VideoCombinerError: If combining fails
    """
    if not clip_paths:
        raise VideoCombinerError("No clip paths provided")

    if not check_ffmpeg_available():
        raise VideoCombinerError("ffmpeg is not available on this system")

    # Ensure output directory exists
    output_dir = Path(output_path).parent
    output_dir.mkdir(parents=True, exist_ok=True)

    logger.info(f"Combining {len(clip_paths)} video clips into {output_path}")

    try:
        if transition_duration > 0:
            # Use xfade filter for crossfade transitions
            success, error = _combine_with_transitions(
                clip_paths, output_path, transition_duration, output_resolution,
                output_fps, keep_audio
            )
        else:
            # Simple concatenation without transitions
            success, error = _combine_simple_concat(
                clip_paths, output_path, output_resolution, output_fps, keep_audio
            )

        if not success:
            logger.error(f"Video combining failed: {error}")
            return False, None, None

        # Get metadata of combined video
        metadata = _get_video_metadata(output_path)

        logger.info(f"Successfully combined {len(clip_paths)} clips into {output_path}")
        return True, output_path, metadata

    except Exception as e:
        logger.error(f"Error combining videos: {e}", exc_info=True)
        raise VideoCombinerError(f"Failed to combine videos: {e}")


def _combine_simple_concat(
    clip_paths: List[str],
    output_path: str,
    output_resolution: str,
    output_fps: int,
    keep_audio: bool,
) -> Tuple[bool, Optional[str]]:
    """
    Combine videos using simple concatenation (no transitions).

    Uses ffmpeg concat demuxer for efficient concatenation.
    """
    # Create a temporary file list for concat demuxer
    with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:
        concat_file = f.name
        for clip_path in clip_paths:
            # Use absolute paths
            abs_path = os.path.abspath(clip_path)
            f.write(f"file '{abs_path}'\n")

    try:
        # Build ffmpeg command
        cmd = [
            "ffmpeg",
            "-f", "concat",
            "-safe", "0",
            "-i", concat_file,
            "-c:v", "libx264",
            "-preset", "medium",
            "-crf", "23",  # Quality (lower = better, 18-28 is good range)
            "-s", output_resolution,
            "-r", str(output_fps),
        ]

        if keep_audio:
            cmd.extend(["-c:a", "aac", "-b:a", "192k"])
        else:
            cmd.extend(["-an"])  # No audio

        cmd.extend([
            "-y",  # Overwrite output file
            output_path
        ])

        logger.debug(f"Running ffmpeg command: {' '.join(cmd)}")

        # Run ffmpeg
        result = subprocess.run(
            cmd,
            capture_output=True,
            timeout=600,  # 10 minute timeout
            text=True
        )

        if result.returncode != 0:
            error_msg = result.stderr or "Unknown error"
            logger.error(f"ffmpeg concat failed: {error_msg}")
            return False, error_msg

        return True, None

    except subprocess.TimeoutExpired:
        return False, "ffmpeg timeout (process took too long)"
    except Exception as e:
        return False, f"Unexpected error: {str(e)}"
    finally:
        # Clean up temp file
        try:
            os.unlink(concat_file)
        except:
            pass


def _combine_with_transitions(
    clip_paths: List[str],
    output_path: str,
    transition_duration: float,
    output_resolution: str,
    output_fps: int,
    keep_audio: bool,
) -> Tuple[bool, Optional[str]]:
    """
    Combine videos with crossfade transitions using xfade filter.

    Note: This is more complex and slower than simple concat.
    """
    if len(clip_paths) < 2:
        # Fall back to simple concat for single clip
        return _combine_simple_concat(
            clip_paths, output_path, output_resolution, output_fps, keep_audio
        )

    try:
        # Build complex filter for crossfade
        # This gets complex with many clips, so we'll use a simpler approach:
        # Re-encode each clip and use concat filter with crossfade

        # For now, use simple concat and log a warning
        # Full xfade implementation would require building a complex filtergraph
        logger.warning(
            f"Crossfade transitions requested but using simple concat for performance. "
            f"Transition duration {transition_duration}s ignored."
        )

        return _combine_simple_concat(
            clip_paths, output_path, output_resolution, output_fps, keep_audio
        )

        # TODO: Implement full xfade filter chain for smooth transitions
        # This would involve:
        # 1. Getting duration of each clip
        # 2. Building xfade filter chain with offset calculations
        # 3. Handling audio crossfades separately
        # Example: [0:v][1:v]xfade=transition=fade:duration=1:offset=4[v01];[v01][2:v]xfade...

    except Exception as e:
        return False, f"Transition error: {str(e)}"


def _get_video_metadata(video_path: str) -> Dict[str, Any]:
    """
    Get metadata about a video file using ffprobe.

    Args:
        video_path: Path to video file

    Returns:
        Dict with duration, size, resolution, etc.
    """
    try:
        # Get video info with ffprobe
        result = subprocess.run(
            [
                "ffprobe",
                "-v", "quiet",
                "-print_format", "json",
                "-show_format",
                "-show_streams",
                video_path
            ],
            capture_output=True,
            timeout=10,
            text=True
        )

        if result.returncode == 0:
            import json
            data = json.loads(result.stdout)

            # Extract useful metadata
            format_data = data.get("format", {})
            video_stream = next(
                (s for s in data.get("streams", []) if s.get("codec_type") == "video"),
                {}
            )

            metadata = {
                "duration": float(format_data.get("duration", 0)),
                "size_bytes": int(format_data.get("size", 0)),
                "format": format_data.get("format_name", "unknown"),
                "width": video_stream.get("width"),
                "height": video_stream.get("height"),
                "fps": eval(video_stream.get("r_frame_rate", "30/1")),  # e.g., "30/1" -> 30.0
                "codec": video_stream.get("codec_name", "unknown"),
            }

            return metadata

        else:
            logger.warning(f"ffprobe failed for {video_path}")
            return {}

    except Exception as e:
        logger.error(f"Error getting video metadata: {e}")
        return {}


def store_clip_and_combined(
    job_id: int,
    clip_paths: List[str],
    combined_path: str,
    data_dir: str = "./DATA/videos",
) -> Tuple[List[str], str]:
    """
    Store individual clips and combined video in organized directory structure.

    Args:
        job_id: Job ID for directory organization
        clip_paths: List of paths to individual clips
        combined_path: Path to combined video
        data_dir: Base data directory

    Returns:
        Tuple of (clip_urls, combined_url) - API-accessible URLs
    """
    base_path = Path(data_dir) / str(job_id)
    clips_dir = base_path / "clips"
    clips_dir.mkdir(parents=True, exist_ok=True)

    clip_urls = []

    # Copy clips to organized structure
    for i, clip_path in enumerate(clip_paths, 1):
        clip_filename = f"clip_{i:03d}.mp4"
        dest_path = clips_dir / clip_filename

        # Copy the file
        import shutil
        shutil.copy2(clip_path, dest_path)

        # Generate URL
        clip_url = f"/api/v3/videos/{job_id}/clips/{clip_filename}"
        clip_urls.append(clip_url)

    # Copy combined video
    combined_dest = base_path / "combined.mp4"
    import shutil
    shutil.copy2(combined_path, combined_dest)

    combined_url = f"/api/v3/videos/{job_id}/combined"

    logger.info(
        f"Stored {len(clip_urls)} clips and combined video for job {job_id}"
    )

    return clip_urls, combined_url


def add_audio_to_video(
    video_path: str,
    audio_path: str,
    output_path: str,
    audio_fade_duration: float = 0.5,
) -> Tuple[bool, Optional[str], Optional[str]]:
    """
    Add an audio track to a video file using FFmpeg.

    Args:
        video_path: Path to input video file
        audio_path: Path to audio file (mp3, wav, etc.)
        output_path: Path for output video with audio
        audio_fade_duration: Fade in/out duration in seconds (default: 0.5)

    Returns:
        Tuple of (success, output_path, error_message)

    Example:
        success, path, error = add_audio_to_video(
            "combined_video.mp4",
            "background_music.mp3",
            "final_video.mp4"
        )
    """
    if not check_ffmpeg_available():
        return False, None, "ffmpeg is not available on this system"

    if not os.path.exists(video_path):
        return False, None, f"Video file not found: {video_path}"

    if not os.path.exists(audio_path):
        return False, None, f"Audio file not found: {audio_path}"

    logger.info(f"Adding audio to video: {video_path} + {audio_path} -> {output_path}")

    try:
        # Ensure output directory exists
        output_dir = Path(output_path).parent
        output_dir.mkdir(parents=True, exist_ok=True)

        # Build FFmpeg command to merge video and audio
        # -i video_path: input video
        # -i audio_path: input audio
        # -c:v copy: copy video codec (no re-encoding)
        # -c:a aac: encode audio as AAC
        # -b:a 192k: audio bitrate
        # -map 0:v:0: use video from first input
        # -map 1:a:0: use audio from second input
        # -shortest: end output when shortest input ends
        # -af: audio filter for fade in/out

        audio_filter = f"afade=t=in:st=0:d={audio_fade_duration},afade=t=out:st=0:d={audio_fade_duration}"

        cmd = [
            "ffmpeg",
            "-i", video_path,  # Input video
            "-i", audio_path,  # Input audio
            "-c:v", "copy",  # Copy video without re-encoding
            "-c:a", "aac",  # Encode audio as AAC
            "-b:a", "192k",  # Audio bitrate
            "-map", "0:v:0",  # Map video from first input
            "-map", "1:a:0",  # Map audio from second input
            "-af", audio_filter,  # Apply audio fades
            "-shortest",  # Match duration to shortest input
            "-y",  # Overwrite output file
            output_path
        ]

        logger.debug(f"FFmpeg command: {' '.join(cmd)}")

        # Execute FFmpeg
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=600,  # 10 minute timeout
        )

        if result.returncode != 0:
            error_msg = f"FFmpeg failed: {result.stderr}"
            logger.error(error_msg)
            return False, None, error_msg

        if not os.path.exists(output_path):
            error_msg = "Output file was not created"
            logger.error(error_msg)
            return False, None, error_msg

        file_size = os.path.getsize(output_path)
        logger.info(f"Successfully added audio to video: {output_path} ({file_size} bytes)")

        return True, output_path, None

    except subprocess.TimeoutExpired:
        error_msg = "FFmpeg operation timed out"
        logger.error(error_msg)
        return False, None, error_msg

    except Exception as e:
        error_msg = f"Error adding audio to video: {e}"
        logger.error(error_msg, exc_info=True)
        return False, None, error_msg


def cleanup_temp_clips(clip_paths: List[str]) -> None:
    """
    Clean up temporary clip files.

    Args:
        clip_paths: List of temporary clip paths to delete
    """
    for clip_path in clip_paths:
        try:
            if os.path.exists(clip_path):
                os.unlink(clip_path)
                logger.debug(f"Deleted temporary clip: {clip_path}")
        except Exception as e:
            logger.warning(f"Failed to delete temp clip {clip_path}: {e}")
</file>

<file path="backend/migrate.py">
"""Database migration runner.

This module applies the complete schema from schema.sql on server init.
All migrations are idempotent - safe to run multiple times.
"""

import sqlite3
from pathlib import Path
import os

# Get data directory from environment variable, default to ./DATA
DATA_DIR = Path(os.getenv("DATA", "./DATA"))
DATA_DIR.mkdir(exist_ok=True)

DB_PATH = DATA_DIR / "scenes.db"
SCHEMA_PATH = Path(__file__).parent / "schema.sql"


def run_migrations():
    """
    Apply all database migrations from schema.sql.

    This function is idempotent - safe to run on every server startup.
    Uses CREATE TABLE IF NOT EXISTS and CREATE INDEX IF NOT EXISTS.
    """
    if not SCHEMA_PATH.exists():
        raise FileNotFoundError(f"Schema file not found: {SCHEMA_PATH}")

    # Read schema file
    with open(SCHEMA_PATH, 'r') as f:
        schema_sql = f.read()

    # Connect to database
    conn = sqlite3.connect(str(DB_PATH))
    conn.row_factory = sqlite3.Row

    try:
        # STEP 1: Add missing columns to existing tables
        # These ALTER TABLE statements are idempotent - they'll fail silently if column exists
        print("Running pre-migration column additions...")

        # Add client_id and campaign_id to generated_images if missing
        try:
            conn.execute("ALTER TABLE generated_images ADD COLUMN client_id TEXT")
            print("  ✓ Added client_id to generated_images")
        except sqlite3.OperationalError:
            pass  # Column already exists

        try:
            conn.execute("ALTER TABLE generated_images ADD COLUMN campaign_id TEXT")
            print("  ✓ Added campaign_id to generated_images")
        except sqlite3.OperationalError:
            pass  # Column already exists

        # Add client_id and campaign_id to generated_videos if missing
        try:
            conn.execute("ALTER TABLE generated_videos ADD COLUMN client_id TEXT")
            print("  ✓ Added client_id to generated_videos")
        except sqlite3.OperationalError:
            pass  # Column already exists

        try:
            conn.execute("ALTER TABLE generated_videos ADD COLUMN campaign_id TEXT")
            print("  ✓ Added campaign_id to generated_videos")
        except sqlite3.OperationalError:
            pass  # Column already exists

        # Add blob_data to assets if missing
        try:
            conn.execute("ALTER TABLE assets ADD COLUMN blob_data BLOB")
            print("  ✓ Added blob_data to assets")
        except sqlite3.OperationalError:
            pass  # Column already exists

        # Add thumbnail_data to generated_videos if missing
        try:
            conn.execute("ALTER TABLE generated_videos ADD COLUMN thumbnail_data BLOB")
            print("  ✓ Added thumbnail_data to generated_videos")
        except sqlite3.OperationalError:
            pass  # Column already exists

        # Add blob_id to assets if missing (for V3 blob storage)
        try:
            conn.execute("ALTER TABLE assets ADD COLUMN blob_id TEXT")
            print("  ✓ Added blob_id to assets")
        except sqlite3.OperationalError:
            pass  # Column already exists

        # Add source_url to assets if missing (for V3 asset tracking)
        try:
            conn.execute("ALTER TABLE assets ADD COLUMN source_url TEXT")
            print("  ✓ Added source_url to assets")
        except sqlite3.OperationalError:
            pass  # Column already exists

        # Add thumbnail_blob_id to assets if missing (for V3 thumbnail storage)
        try:
            conn.execute("ALTER TABLE assets ADD COLUMN thumbnail_blob_id TEXT")
            print("  ✓ Added thumbnail_blob_id to assets")
        except sqlite3.OperationalError:
            pass  # Column already exists

        # Add homepage to clients if missing (for V3 client management)
        try:
            conn.execute("ALTER TABLE clients ADD COLUMN homepage TEXT")
            print("  ✓ Added homepage to clients")
        except sqlite3.OperationalError:
            pass  # Column already exists

        # Add metadata to clients if missing (for V3 client management)
        try:
            conn.execute("ALTER TABLE clients ADD COLUMN metadata TEXT")
            print("  ✓ Added metadata to clients")
        except sqlite3.OperationalError:
            pass  # Column already exists

        # Add product_url to campaigns if missing (for V3 campaign management)
        try:
            conn.execute("ALTER TABLE campaigns ADD COLUMN product_url TEXT")
            print("  ✓ Added product_url to campaigns")
        except sqlite3.OperationalError:
            pass  # Column already exists

        # Add metadata to campaigns if missing (for V3 campaign management)
        try:
            conn.execute("ALTER TABLE campaigns ADD COLUMN metadata TEXT")
            print("  ✓ Added metadata to campaigns")
        except sqlite3.OperationalError:
            pass  # Column already exists

        conn.commit()
        print("✓ Pre-migration column additions complete")

        # STEP 2: Execute main schema
        # Execute schema using executescript for proper multi-statement handling
        # executescript handles triggers, transactions, etc. properly
        conn.executescript(schema_sql)
        print("✓ Database migrations applied successfully")

        # Verify critical tables exist
        cursor = conn.execute("SELECT name FROM sqlite_master WHERE type='table' ORDER BY name")
        tables = [row[0] for row in cursor.fetchall()]

        critical_tables = [
            'users', 'api_keys', 'clients', 'campaigns', 'assets',
            'creative_briefs', 'generated_scenes', 'generated_images',
            'generated_videos', 'genesis_videos'
        ]

        missing_tables = [t for t in critical_tables if t not in tables]
        if missing_tables:
            print(f"⚠ Warning: Missing tables: {', '.join(missing_tables)}")
        else:
            print(f"✓ All {len(critical_tables)} critical tables verified")

        return True

    except Exception as e:
        print(f"✗ Migration failed: {e}")
        conn.rollback()
        raise
    finally:
        conn.close()


if __name__ == "__main__":
    # Run migrations when executed directly
    run_migrations()
</file>

<file path="backend/workflows/runner.py">
"""
Luigi workflow runner for FastAPI integration.

Provides utilities for running Luigi workflows from FastAPI endpoints
and monitoring their progress.
"""

import luigi
import logging
import asyncio
import subprocess
import json
import sys
import os
from typing import Dict, Any, List, Optional
from concurrent.futures import ThreadPoolExecutor
from pathlib import Path

from .campaign_pipeline import CampaignPipelineWorkflow

logger = logging.getLogger(__name__)

# Thread pool for running Luigi workflows
_luigi_executor = ThreadPoolExecutor(max_workers=5, thread_name_prefix="luigi")


def run_pipeline_sync(
    job_id: int,
    campaign_id: str,
    clip_duration: Optional[float] = None,
    num_pairs: Optional[int] = None,
    workers: int = 10,
    use_local_scheduler: bool = False,  # Use central scheduler daemon
) -> Dict[str, Any]:
    """
    Run the campaign pipeline workflow synchronously via subprocess.

    This runs Luigi as a separate process to avoid signal handler conflicts
    when called from FastAPI background tasks.

    Args:
        job_id: Parent job ID
        campaign_id: Campaign ID
        clip_duration: Optional clip duration in seconds
        num_pairs: Optional target number of image pairs
        workers: Number of Luigi workers (default: 10)
        use_local_scheduler: Use local scheduler instead of central (default: False - uses central daemon)

    Returns:
        Dict with success status and details
    """
    print(f"[LUIGI DEBUG 1] Starting Luigi pipeline for job {job_id}, campaign {campaign_id}", flush=True)
    logger.info(
        f"Starting Luigi pipeline for job {job_id}, campaign {campaign_id}"
    )

    try:
        # Use the current Python interpreter (will be venv Python in production)
        python_executable = sys.executable
        print(f"[LUIGI DEBUG 1.5] Using Python: {python_executable}", flush=True)
        logger.info(f"Using Python executable: {python_executable}")

        # Build Luigi command-line arguments
        cmd = [
            python_executable, "-m", "luigi",
            "--module", "backend.workflows.campaign_pipeline",
            "CampaignPipelineWorkflow",
            "--job-id", str(job_id),
            "--campaign-id", campaign_id,
            "--workers", str(workers),
        ]

        if clip_duration is not None:
            cmd.extend(["--clip-duration", str(clip_duration)])

        if num_pairs is not None:
            cmd.extend(["--num-pairs", str(num_pairs)])

        if use_local_scheduler:
            cmd.append("--local-scheduler")

        # Run Luigi as subprocess
        print(f"[LUIGI DEBUG 2] Running Luigi command: {' '.join(cmd)}", flush=True)
        logger.info(f"Running Luigi command: {' '.join(cmd)}")

        print(f"[LUIGI DEBUG 3] About to call subprocess.run", flush=True)
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=7200,  # 2 hours timeout for large campaigns
        )
        print(f"[LUIGI DEBUG 4] subprocess.run completed with return code {result.returncode}", flush=True)

        if result.returncode == 0:
            print(f"[LUIGI DEBUG 5] Luigi pipeline completed successfully for job {job_id}", flush=True)
            logger.info(f"Luigi pipeline completed successfully for job {job_id}")
            logger.debug(f"Luigi stdout: {result.stdout}")
            return {
                "success": True,
                "job_id": job_id,
                "message": "Pipeline completed successfully",
            }
        else:
            print(f"[LUIGI DEBUG 6] Luigi pipeline FAILED for job {job_id} with code {result.returncode}", flush=True)
            print(f"[LUIGI DEBUG 7] stderr: {result.stderr[:500]}", flush=True)
            print(f"[LUIGI DEBUG 8] stdout: {result.stdout[:500]}", flush=True)
            logger.error(f"Luigi pipeline failed for job {job_id}")
            logger.error(f"Luigi stderr: {result.stderr}")
            logger.error(f"Luigi stdout: {result.stdout}")
            return {
                "success": False,
                "job_id": job_id,
                "message": f"Pipeline failed with exit code {result.returncode}",
                "error": result.stderr,
            }

    except subprocess.TimeoutExpired:
        print(f"[LUIGI DEBUG 9] Luigi pipeline TIMED OUT for job {job_id}", flush=True)
        logger.error(f"Luigi pipeline timed out for job {job_id}")
        return {
            "success": False,
            "job_id": job_id,
            "error": "Pipeline execution timed out after 1 hour",
        }
    except Exception as e:
        print(f"[LUIGI DEBUG 10] Exception running Luigi for job {job_id}: {e}", flush=True)
        logger.error(f"Error running Luigi pipeline for job {job_id}: {e}", exc_info=True)
        return {
            "success": False,
            "job_id": job_id,
            "error": str(e),
        }


async def run_pipeline_async(
    job_id: int,
    campaign_id: str,
    clip_duration: Optional[float] = None,
    num_pairs: Optional[int] = None,
    workers: int = 10,
    use_local_scheduler: bool = False,  # Use central scheduler daemon
) -> Dict[str, Any]:
    """
    Run the campaign pipeline workflow asynchronously.

    This is the recommended way to run Luigi workflows from FastAPI.
    It runs the workflow in a thread pool so it doesn't block the event loop.

    Args:
        job_id: Parent job ID
        campaign_id: Campaign ID
        clip_duration: Optional clip duration in seconds
        num_pairs: Optional target number of image pairs
        workers: Number of Luigi workers (default: 10)
        use_local_scheduler: Use local scheduler instead of central (default: False - uses central daemon)

    Returns:
        Dict with success status and details
    """
    print(f"[LUIGI ASYNC 1] run_pipeline_async called for job {job_id}", flush=True)
    loop = asyncio.get_event_loop()

    print(f"[LUIGI ASYNC 2] About to run_in_executor for job {job_id}", flush=True)
    try:
        result = await loop.run_in_executor(
            _luigi_executor,
            run_pipeline_sync,
            job_id,
            campaign_id,
            clip_duration,
            num_pairs,
            workers,
            use_local_scheduler,
        )
        print(f"[LUIGI ASYNC 3] run_in_executor completed for job {job_id} with result: {result.get('success', 'unknown')}", flush=True)
        return result
    except Exception as e:
        print(f"[LUIGI ASYNC 4] Exception in run_pipeline_async for job {job_id}: {e}", flush=True)
        raise


def get_pipeline_status(job_id: int) -> Dict[str, Any]:
    """
    Get the current status of a Luigi pipeline.

    Queries the database for Luigi task states and computes overall progress.

    Args:
        job_id: Job ID to check

    Returns:
        Dict with pipeline status information
    """
    from ..database import get_db

    try:
        with get_db() as conn:
            # Get all task states for this job
            rows = conn.execute(
                """
                SELECT task_name, status, created_at, completed_at, output_data
                FROM luigi_task_state
                WHERE job_id = ?
                ORDER BY created_at ASC
                """,
                (job_id,),
            ).fetchall()

            if not rows:
                return {
                    "job_id": job_id,
                    "status": "not_started",
                    "tasks": [],
                    "progress": 0.0,
                }

            # Process task states
            tasks = []
            completed_count = 0
            total_count = len(rows)

            for row in rows:
                task_info = {
                    "name": row["task_name"],
                    "status": row["status"],
                    "created_at": row["created_at"],
                    "completed_at": row["completed_at"],
                }

                if row["status"] == "completed":
                    completed_count += 1

                tasks.append(task_info)

            # Compute overall status
            if completed_count == total_count:
                overall_status = "completed"
            elif any(t["status"] == "failed" for t in tasks):
                overall_status = "failed"
            elif any(t["status"] == "running" for t in tasks):
                overall_status = "running"
            else:
                overall_status = "pending"

            # Compute progress
            progress = (completed_count / total_count) * 100 if total_count > 0 else 0.0

            return {
                "job_id": job_id,
                "status": overall_status,
                "tasks": tasks,
                "progress": round(progress, 2),
                "completed_tasks": completed_count,
                "total_tasks": total_count,
            }

    except Exception as e:
        logger.error(f"Error getting pipeline status for job {job_id}: {e}")
        return {
            "job_id": job_id,
            "status": "error",
            "error": str(e),
        }


def get_all_pipeline_statuses(limit: int = 20, offset: int = 0) -> List[Dict[str, Any]]:
    """
    Get statuses for all recent pipelines.

    Args:
        limit: Maximum number of pipelines to return
        offset: Offset for pagination

    Returns:
        List of pipeline status dicts
    """
    from ..database import get_db

    try:
        with get_db() as conn:
            # Get unique job IDs
            rows = conn.execute(
                """
                SELECT DISTINCT job_id
                FROM luigi_task_state
                ORDER BY job_id DESC
                LIMIT ? OFFSET ?
                """,
                (limit, offset),
            ).fetchall()

            job_ids = [row["job_id"] for row in rows]

            # Get status for each job
            statuses = [get_pipeline_status(job_id) for job_id in job_ids]

            return statuses

    except Exception as e:
        logger.error(f"Error getting all pipeline statuses: {e}")
        return []


def cancel_pipeline(job_id: int) -> Dict[str, Any]:
    """
    Cancel a running pipeline.

    Marks all pending/running tasks as cancelled.

    Args:
        job_id: Job ID to cancel

    Returns:
        Dict with cancellation result
    """
    from ..database import get_db

    try:
        with get_db() as conn:
            # Update all non-completed tasks to cancelled
            cursor = conn.execute(
                """
                UPDATE luigi_task_state
                SET status = 'cancelled'
                WHERE job_id = ?
                AND status IN ('pending', 'running')
                """,
                (job_id,),
            )
            conn.commit()

            cancelled_count = cursor.rowcount

            logger.info(f"Cancelled {cancelled_count} tasks for job {job_id}")

            return {
                "success": True,
                "job_id": job_id,
                "cancelled_tasks": cancelled_count,
            }

    except Exception as e:
        logger.error(f"Error cancelling pipeline for job {job_id}: {e}")
        return {
            "success": False,
            "job_id": job_id,
            "error": str(e),
        }


def retry_failed_pipeline(job_id: int) -> Dict[str, Any]:
    """
    Retry a failed pipeline from the last successful checkpoint.

    Marks all failed tasks as pending so Luigi will retry them.

    Args:
        job_id: Job ID to retry

    Returns:
        Dict with retry result
    """
    from ..database import get_db

    try:
        with get_db() as conn:
            # Reset failed tasks to pending
            cursor = conn.execute(
                """
                UPDATE luigi_task_state
                SET status = 'pending'
                WHERE job_id = ?
                AND status = 'failed'
                """,
                (job_id,),
            )
            conn.commit()

            reset_count = cursor.rowcount

            logger.info(f"Reset {reset_count} failed tasks for job {job_id}")

            return {
                "success": True,
                "job_id": job_id,
                "reset_tasks": reset_count,
                "message": f"Reset {reset_count} tasks to pending. Re-run the pipeline to retry.",
            }

    except Exception as e:
        logger.error(f"Error retrying pipeline for job {job_id}: {e}")
        return {
            "success": False,
            "job_id": job_id,
            "error": str(e),
        }


# Export public API
__all__ = [
    "run_pipeline_sync",
    "run_pipeline_async",
    "get_pipeline_status",
    "get_all_pipeline_statuses",
    "cancel_pipeline",
    "retry_failed_pipeline",
]
</file>

<file path="backend/database.py">
"""Database models and operations for storing generated scenes."""

import sqlite3
import json
import os
import logging
from datetime import datetime
from pathlib import Path
from typing import List, Optional, Dict, Any
from contextlib import contextmanager

logger = logging.getLogger(__name__)

# Get data directory from environment variable, default to ./DATA
DATA_DIR = Path(os.getenv("DATA", "./DATA"))
DATA_DIR.mkdir(exist_ok=True)

DB_PATH = DATA_DIR / "scenes.db"


def init_db():
    """Initialize the database with required tables.

    Uses the migration system to apply schema from schema.sql.
    All migrations are idempotent - safe to run on every server startup.
    """
    try:
        from .migrate import run_migrations
    except ImportError:
        from migrate import run_migrations
    run_migrations()


@contextmanager
def get_db():
    """Context manager for database connections."""
    conn = sqlite3.connect(str(DB_PATH))
    conn.row_factory = sqlite3.Row
    try:
        yield conn
    finally:
        conn.close()


def save_generated_scene(
    prompt: str,
    scene_data: dict,
    model: str,
    metadata: Optional[dict] = None,
    brief_id: Optional[str] = None,
) -> int:
    """Save a generated scene to the database."""
    with get_db() as conn:
        cursor = conn.execute(
            """
            INSERT INTO generated_scenes (prompt, scene_data, model, metadata, brief_id)
            VALUES (?, ?, ?, ?, ?)
            """,
            (
                prompt,
                json.dumps(scene_data),
                model,
                json.dumps(metadata) if metadata else None,
                brief_id,
            ),
        )
        conn.commit()
        return cursor.lastrowid or 0


def get_scene_by_id(scene_id: int) -> Optional[Dict[str, Any]]:
    """Retrieve a specific scene by ID."""
    with get_db() as conn:
        row = conn.execute(
            "SELECT * FROM generated_scenes WHERE id = ?", (scene_id,)
        ).fetchone()

        if row:
            return {
                "id": row["id"],
                "prompt": row["prompt"],
                "scene_data": json.loads(row["scene_data"]),
                "model": row["model"],
                "created_at": row["created_at"],
                "metadata": json.loads(row["metadata"]) if row["metadata"] else None,
            }
    return None


def list_scenes(
    limit: int = 50, offset: int = 0, model: Optional[str] = None
) -> List[Dict[str, Any]]:
    """List generated scenes with pagination and optional model filter."""
    query = "SELECT * FROM generated_scenes"
    params = []

    if model:
        query += " WHERE model = ?"
        params.append(model)

    query += " ORDER BY created_at DESC LIMIT ? OFFSET ?"
    params.extend([limit, offset])

    with get_db() as conn:
        rows = conn.execute(query, params).fetchall()

        return [
            {
                "id": row["id"],
                "prompt": row["prompt"],
                "scene_data": json.loads(row["scene_data"]),
                "model": row["model"],
                "created_at": row["created_at"],
                "metadata": json.loads(row["metadata"]) if row["metadata"] else None,
            }
            for row in rows
        ]


def get_scene_count(model: Optional[str] = None) -> int:
    """Get total count of scenes, optionally filtered by model."""
    query = "SELECT COUNT(*) as count FROM generated_scenes"
    params = []

    if model:
        query += " WHERE model = ?"
        params.append(model)

    with get_db() as conn:
        row = conn.execute(query, params).fetchone()
        return row["count"]


def get_models_list() -> List[str]:
    """Get list of unique models that have generated scenes."""
    with get_db() as conn:
        rows = conn.execute(
            "SELECT DISTINCT model FROM generated_scenes ORDER BY model"
        ).fetchall()
        return [row["model"] for row in rows]


def delete_scene(scene_id: int) -> bool:
    """Delete a scene by ID."""
    with get_db() as conn:
        cursor = conn.execute("DELETE FROM generated_scenes WHERE id = ?", (scene_id,))
        conn.commit()
        return cursor.rowcount > 0


def save_generated_video(
    prompt: str,
    video_url: str,
    model_id: str,
    parameters: dict,
    collection: Optional[str] = None,
    metadata: Optional[dict] = None,
    status: str = "completed",
    brief_id: Optional[str] = None,
    client_id: Optional[str] = None,
    campaign_id: Optional[str] = None,
) -> int:
    """Save a generated video to the database."""
    with get_db() as conn:
        cursor = conn.execute(
            """
            INSERT INTO generated_videos (prompt, video_url, model_id, parameters, collection, metadata, status, brief_id, client_id, campaign_id)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """,
            (
                prompt,
                video_url,
                model_id,
                json.dumps(parameters),
                collection,
                json.dumps(metadata) if metadata else None,
                status,
                brief_id,
                client_id,
                campaign_id,
            ),
        )
        conn.commit()
        return cursor.lastrowid or 0


def update_job_parameters(job_id: int, parameters: Dict[str, Any]) -> bool:
    """Update job parameters JSON field"""
    try:
        with get_db() as conn:
            # Ensure parameters is JSON serializable
            parameters_json = json.dumps(parameters)

            conn.execute(
                "UPDATE jobs SET parameters = ? WHERE id = ?", (parameters_json, job_id)
            )
            conn.commit()
            return True
    except Exception as e:
        logger.error(f"Failed to update job parameters for job {job_id}: {e}")
        return False


def update_video_status(
    video_id: int,
    status: str,
    video_url: Optional[str] = None,
    metadata: Optional[dict] = None,
) -> None:
    """Update the status and optionally the video_url and metadata of a video."""
    with get_db() as conn:
        if video_url is not None:
            conn.execute(
                """
                UPDATE generated_videos
                SET status = ?, video_url = ?, metadata = ?
                WHERE id = ?
                """,
                (
                    status,
                    video_url,
                    json.dumps(metadata) if metadata else None,
                    video_id,
                ),
            )
        else:
            if metadata is not None:
                conn.execute(
                    """
                    UPDATE generated_videos
                    SET status = ?, metadata = ?
                    WHERE id = ?
                    """,
                    (status, json.dumps(metadata), video_id),
                )
            else:
                conn.execute(
                    """
                    UPDATE generated_videos
                    SET status = ?
                    WHERE id = ?
                    """,
                    (status, video_id),
                )
        conn.commit()


def mark_download_attempted(video_id: int) -> bool:
    """Mark that a download has been attempted for a video. Returns False if already attempted."""
    with get_db() as conn:
        # Check if already attempted
        row = conn.execute(
            "SELECT download_attempted FROM generated_videos WHERE id = ?", (video_id,)
        ).fetchone()

        if row and row["download_attempted"]:
            return False  # Already attempted

        # Mark as attempted
        conn.execute(
            "UPDATE generated_videos SET download_attempted = 1 WHERE id = ?",
            (video_id,),
        )
        conn.commit()
        return True


def increment_download_retries(video_id: int) -> int:
    """Increment the download retry counter and return the new count."""
    with get_db() as conn:
        conn.execute(
            "UPDATE generated_videos SET download_retries = download_retries + 1 WHERE id = ?",
            (video_id,),
        )
        conn.commit()

        row = conn.execute(
            "SELECT download_retries FROM generated_videos WHERE id = ?", (video_id,)
        ).fetchone()

        return row["download_retries"] if row else 0


def mark_download_failed(video_id: int, error: str) -> None:
    """Mark a video download as permanently failed."""
    with get_db() as conn:
        conn.execute(
            """
            UPDATE generated_videos
            SET status = 'failed', download_error = ?
            WHERE id = ?
            """,
            (error, video_id),
        )
        conn.commit()


def get_video_by_id(video_id: int) -> Optional[Dict[str, Any]]:
    """Retrieve a specific video by ID."""
    with get_db() as conn:
        row = conn.execute(
            "SELECT * FROM generated_videos WHERE id = ?", (video_id,)
        ).fetchone()

        if row:
            return {
                "id": row["id"],
                "prompt": row["prompt"],
                "video_url": row["video_url"],
                "model_id": row["model_id"],
                "parameters": json.loads(row["parameters"]),
                "status": row["status"],
                "created_at": row["created_at"],
                "collection": row["collection"],
                "brief_id": row["brief_id"],
                "metadata": json.loads(row["metadata"]) if row["metadata"] else None,
            }
    return None


def list_videos(
    limit: int = 50,
    offset: int = 0,
    model_id: Optional[str] = None,
    collection: Optional[str] = None,
    brief_id: Optional[str] = None,
) -> List[Dict[str, Any]]:
    """List generated videos with pagination and optional filters."""
    # Include thumbnail_data for single-query optimization
    query = """SELECT id, prompt, video_url, model_id, parameters, status,
                      created_at, collection, metadata, brief_id, error_message,
                      client_id, campaign_id, thumbnail_data
               FROM generated_videos WHERE 1=1"""
    params = []

    if model_id:
        query += " AND model_id = ?"
        params.append(model_id)

    if collection:
        query += " AND collection = ?"
        params.append(collection)

    query += " ORDER BY created_at DESC LIMIT ? OFFSET ?"
    params.extend([limit, offset])

    with get_db() as conn:
        rows = conn.execute(query, params).fetchall()

        import base64

        results = []
        for row in rows:
            # Use data URI for thumbnails if cached, otherwise fallback to endpoint
            thumbnail_url = f"/api/videos/{row['id']}/thumbnail"
            if row["thumbnail_data"]:
                # Encode as base64 data URI for immediate display without additional HTTP requests
                thumbnail_b64 = base64.b64encode(row["thumbnail_data"]).decode("utf-8")
                thumbnail_url = f"data:image/jpeg;base64,{thumbnail_b64}"

            results.append(
                {
                    "id": row["id"],
                    "prompt": row["prompt"],
                    "video_url": row["video_url"],
                    "thumbnail_url": thumbnail_url,
                    "model_id": row["model_id"],
                    "parameters": json.loads(row["parameters"]),
                    "status": row["status"],
                    "created_at": row["created_at"],
                    "collection": row["collection"],
                    "brief_id": row["brief_id"],
                    "metadata": json.loads(row["metadata"])
                    if row["metadata"]
                    else None,
                }
            )

        return results


def count_videos(
    model_id: Optional[str] = None,
    collection: Optional[str] = None,
    brief_id: Optional[str] = None,
) -> int:
    """Count total number of videos with optional filters."""
    query = """SELECT COUNT(*) as total FROM generated_videos WHERE 1=1"""
    params = []

    if model_id:
        query += " AND model_id = ?"
        params.append(model_id)

    if collection:
        query += " AND collection = ?"
        params.append(collection)

    if brief_id:
        query += " AND brief_id = ?"
        params.append(brief_id)

    with get_db() as conn:
        row = conn.execute(query, params).fetchone()
        return row["total"] if row else 0


def delete_video(video_id: int) -> bool:
    """Delete a video by ID."""
    with get_db() as conn:
        cursor = conn.execute("DELETE FROM generated_videos WHERE id = ?", (video_id,))
        conn.commit()
        return cursor.rowcount > 0


def save_genesis_video(
    scene_data: dict,
    video_path: str,
    quality: str,
    duration: float,
    fps: int,
    resolution: tuple = (1920, 1080),
    scene_context: Optional[str] = None,
    object_descriptions: Optional[dict] = None,
    metadata: Optional[dict] = None,
) -> int:
    """Save a Genesis-rendered video to the database."""
    with get_db() as conn:
        cursor = conn.execute(
            """
            INSERT INTO genesis_videos
            (scene_data, video_path, quality, duration, fps, resolution, scene_context, object_descriptions, metadata)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            """,
            (
                json.dumps(scene_data),
                video_path,
                quality,
                duration,
                fps,
                f"{resolution[0]}x{resolution[1]}",
                scene_context,
                json.dumps(object_descriptions) if object_descriptions else None,
                json.dumps(metadata) if metadata else None,
            ),
        )
        conn.commit()
        return cursor.lastrowid


def get_genesis_video_by_id(video_id: int) -> Optional[Dict[str, Any]]:
    """Retrieve a specific Genesis video by ID."""
    with get_db() as conn:
        row = conn.execute(
            "SELECT * FROM genesis_videos WHERE id = ?", (video_id,)
        ).fetchone()

        if row:
            return {
                "id": row["id"],
                "scene_data": json.loads(row["scene_data"]),
                "video_path": row["video_path"],
                "quality": row["quality"],
                "duration": row["duration"],
                "fps": row["fps"],
                "resolution": row["resolution"],
                "scene_context": row["scene_context"],
                "object_descriptions": json.loads(row["object_descriptions"])
                if row["object_descriptions"]
                else None,
                "status": row["status"],
                "created_at": row["created_at"],
                "metadata": json.loads(row["metadata"]) if row["metadata"] else None,
            }
    return None


def list_genesis_videos(
    limit: int = 50, offset: int = 0, quality: Optional[str] = None
) -> List[Dict[str, Any]]:
    """List Genesis videos with pagination and optional quality filter."""
    query = "SELECT * FROM genesis_videos WHERE 1=1"
    params = []

    if quality:
        query += " AND quality = ?"
        params.append(quality)

    query += " ORDER BY created_at DESC LIMIT ? OFFSET ?"
    params.extend([limit, offset])

    with get_db() as conn:
        rows = conn.execute(query, params).fetchall()

        return [
            {
                "id": row["id"],
                "scene_data": json.loads(row["scene_data"]),
                "video_path": row["video_path"],
                "quality": row["quality"],
                "duration": row["duration"],
                "fps": row["fps"],
                "resolution": row["resolution"],
                "scene_context": row["scene_context"],
                "object_descriptions": json.loads(row["object_descriptions"])
                if row["object_descriptions"]
                else None,
                "status": row["status"],
                "created_at": row["created_at"],
                "metadata": json.loads(row["metadata"]) if row["metadata"] else None,
            }
            for row in rows
        ]


def delete_genesis_video(video_id: int) -> bool:
    """Delete a Genesis video by ID."""
    with get_db() as conn:
        cursor = conn.execute("DELETE FROM genesis_videos WHERE id = ?", (video_id,))
        conn.commit()
        return cursor.rowcount > 0


def get_genesis_video_count(quality: Optional[str] = None) -> int:
    """Get total count of Genesis videos, optionally filtered by quality."""
    query = "SELECT COUNT(*) as count FROM genesis_videos"
    params = []

    if quality:
        query += " WHERE quality = ?"
        params.append(quality)

    with get_db() as conn:
        row = conn.execute(query, params).fetchone()
        return row["count"]


# Image generation functions
def save_generated_image(
    prompt: str,
    image_url: str,
    model_id: str,
    parameters: dict,
    collection: Optional[str] = None,
    metadata: Optional[dict] = None,
    status: str = "completed",
    brief_id: Optional[str] = None,
    client_id: Optional[str] = None,
    campaign_id: Optional[str] = None,
) -> int:
    """Save a generated image to the database."""
    with get_db() as conn:
        cursor = conn.execute(
            """
            INSERT INTO generated_images (prompt, image_url, model_id, parameters, collection, metadata, status, brief_id, client_id, campaign_id)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """,
            (
                prompt,
                image_url,
                model_id,
                json.dumps(parameters),
                collection,
                json.dumps(metadata) if metadata else None,
                status,
                brief_id,
                client_id,
                campaign_id,
            ),
        )
        conn.commit()
        return cursor.lastrowid or 0


def update_image_status(
    image_id: int,
    status: str,
    image_url: Optional[str] = None,
    metadata: Optional[dict] = None,
) -> None:
    """Update the status and optionally the image_url and metadata of an image."""
    with get_db() as conn:
        if image_url is not None:
            conn.execute(
                """
                UPDATE generated_images
                SET status = ?, image_url = ?, metadata = ?
                WHERE id = ?
                """,
                (
                    status,
                    image_url,
                    json.dumps(metadata) if metadata else None,
                    image_id,
                ),
            )
        else:
            if metadata is not None:
                conn.execute(
                    """
                    UPDATE generated_images
                    SET status = ?, metadata = ?
                    WHERE id = ?
                    """,
                    (status, json.dumps(metadata), image_id),
                )
            else:
                conn.execute(
                    """
                    UPDATE generated_images
                    SET status = ?
                    WHERE id = ?
                    """,
                    (status, image_id),
                )
        conn.commit()


def mark_image_download_attempted(image_id: int) -> bool:
    """Mark that a download has been attempted for an image. Returns False if already attempted."""
    with get_db() as conn:
        # Check if already attempted
        row = conn.execute(
            "SELECT download_attempted FROM generated_images WHERE id = ?", (image_id,)
        ).fetchone()

        if row and row["download_attempted"]:
            return False  # Already attempted

        # Mark as attempted
        conn.execute(
            "UPDATE generated_images SET download_attempted = 1 WHERE id = ?",
            (image_id,),
        )
        conn.commit()
        return True


def increment_image_download_retries(image_id: int) -> int:
    """Increment the download retry counter for an image and return the new count."""
    with get_db() as conn:
        conn.execute(
            "UPDATE generated_images SET download_retries = download_retries + 1 WHERE id = ?",
            (image_id,),
        )
        conn.commit()

        row = conn.execute(
            "SELECT download_retries FROM generated_images WHERE id = ?", (image_id,)
        ).fetchone()

        return row["download_retries"] if row else 0


def mark_image_download_failed(image_id: int, error: str) -> None:
    """Mark an image download as permanently failed."""
    with get_db() as conn:
        conn.execute(
            """
            UPDATE generated_images
            SET status = 'failed', download_error = ?
            WHERE id = ?
            """,
            (error, image_id),
        )
        conn.commit()


def get_image_by_id(image_id: int) -> Optional[Dict[str, Any]]:
    """Retrieve a specific image by ID."""
    import os

    # Get base URL for full URLs
    base_url = os.getenv("BASE_URL", "").strip()

    with get_db() as conn:
        row = conn.execute(
            "SELECT * FROM generated_images WHERE id = ?", (image_id,)
        ).fetchone()

        if row:
            return {
                "id": row["id"],
                "prompt": row["prompt"],
                "image_url": _convert_to_full_url(row["image_url"], base_url),
                "thumbnail_url": _convert_to_full_url(
                    f"/api/images/{row['id']}/thumbnail", base_url
                ),
                "model_id": row["model_id"],
                "parameters": json.loads(row["parameters"]),
                "status": row["status"],
                "created_at": row["created_at"],
                "collection": row["collection"],
                "brief_id": row["brief_id"],
                "metadata": json.loads(row["metadata"]) if row["metadata"] else None,
            }
    return None


def list_images(
    limit: int = 50,
    offset: int = 0,
    model_id: Optional[str] = None,
    collection: Optional[str] = None,
    brief_id: Optional[str] = None,
) -> List[Dict[str, Any]]:
    """List generated images with pagination and optional filters."""
    import os

    query = "SELECT * FROM generated_images WHERE 1=1"
    params = []

    if model_id:
        query += " AND model_id = ?"
        params.append(model_id)

    if collection:
        query += " AND collection = ?"
        params.append(collection)

    query += " ORDER BY created_at DESC LIMIT ? OFFSET ?"
    params.extend([limit, offset])

    # Get base URL for full URLs
    base_url = os.getenv("BASE_URL", "").strip()

    with get_db() as conn:
        rows = conn.execute(query, params).fetchall()

        return [
            {
                "id": row["id"],
                "prompt": row["prompt"],
                "image_url": _convert_to_full_url(row["image_url"], base_url),
                "thumbnail_url": _convert_to_full_url(
                    f"/api/images/{row['id']}/thumbnail", base_url
                ),
                "model_id": row["model_id"],
                "parameters": json.loads(row["parameters"]),
                "status": row["status"],
                "created_at": row["created_at"],
                "collection": row["collection"],
                "brief_id": row["brief_id"],
                "metadata": json.loads(row["metadata"]) if row["metadata"] else None,
            }
            for row in rows
        ]


def _convert_to_full_url(url: str, base_url: str) -> str:
    """Convert relative URL to full URL using BASE_URL if available."""
    if not url:
        return url
    if url.startswith("http"):
        return url  # Already a full URL
    if base_url:
        return f"{base_url}{url}"
    return url  # Return relative URL


def delete_image(image_id: int) -> bool:
    """Delete an image by ID."""
    with get_db() as conn:
        cursor = conn.execute("DELETE FROM generated_images WHERE id = ?", (image_id,))
        conn.commit()
        return cursor.rowcount > 0


# Audio generation helper functions
def save_generated_audio(
    prompt: str,
    audio_url: str,
    model_id: str,
    parameters: dict,
    collection: Optional[str] = None,
    metadata: Optional[dict] = None,
    status: str = "completed",
    brief_id: Optional[str] = None,
    client_id: Optional[str] = None,
    campaign_id: Optional[str] = None,
    duration: Optional[float] = None,
) -> int:
    """Save a generated audio to the database."""
    with get_db() as conn:
        cursor = conn.execute(
            """
            INSERT INTO generated_audio (prompt, audio_url, model_id, parameters, collection, metadata, status, brief_id, client_id, campaign_id, duration)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """,
            (
                prompt,
                audio_url,
                model_id,
                json.dumps(parameters),
                collection,
                json.dumps(metadata) if metadata else None,
                status,
                brief_id,
                client_id,
                campaign_id,
                duration,
            ),
        )
        conn.commit()
        return cursor.lastrowid or 0


def update_audio_status(
    audio_id: int,
    status: str,
    audio_url: Optional[str] = None,
    metadata: Optional[dict] = None,
) -> None:
    """Update the status and optionally the audio_url and metadata of an audio."""
    with get_db() as conn:
        if audio_url is not None:
            conn.execute(
                """
                UPDATE generated_audio
                SET status = ?, audio_url = ?, metadata = ?
                WHERE id = ?
                """,
                (
                    status,
                    audio_url,
                    json.dumps(metadata) if metadata else None,
                    audio_id,
                ),
            )
        else:
            if metadata is not None:
                conn.execute(
                    """
                    UPDATE generated_audio
                    SET status = ?, metadata = ?
                    WHERE id = ?
                    """,
                    (status, json.dumps(metadata), audio_id),
                )
            else:
                conn.execute(
                    """
                    UPDATE generated_audio
                    SET status = ?
                    WHERE id = ?
                    """,
                    (status, audio_id),
                )
        conn.commit()


def get_audio_by_id(audio_id: int) -> Optional[Dict[str, Any]]:
    """Retrieve a specific audio by ID."""
    import os

    base_url = os.getenv("BASE_URL", "").strip()

    with get_db() as conn:
        row = conn.execute(
            "SELECT * FROM generated_audio WHERE id = ?", (audio_id,)
        ).fetchone()

        if row:
            return {
                "id": row["id"],
                "prompt": row["prompt"],
                "audio_url": _convert_to_full_url(row["audio_url"], base_url),
                "model_id": row["model_id"],
                "parameters": json.loads(row["parameters"])
                if row["parameters"]
                else {},
                "collection": row["collection"],
                "status": row["status"],
                "created_at": row["created_at"],
                "metadata": json.loads(row["metadata"]) if row["metadata"] else {},
                "duration": row["duration"],
                "brief_id": row["brief_id"],
                "client_id": row["client_id"],
                "campaign_id": row["campaign_id"],
            }
    return None


def list_audio(
    limit: int = 50,
    offset: int = 0,
    collection: Optional[str] = None,
    status: Optional[str] = None,
    client_id: Optional[str] = None,
    campaign_id: Optional[str] = None,
) -> List[Dict[str, Any]]:
    """List generated audio with optional filters."""
    import os

    base_url = os.getenv("BASE_URL", "").strip()

    with get_db() as conn:
        query = "SELECT * FROM generated_audio WHERE 1=1"
        params: List[Any] = []

        if collection:
            query += " AND collection = ?"
            params.append(collection)
        if status:
            query += " AND status = ?"
            params.append(status)
        if client_id:
            query += " AND client_id = ?"
            params.append(client_id)
        if campaign_id:
            query += " AND campaign_id = ?"
            params.append(campaign_id)

        query += " ORDER BY created_at DESC LIMIT ? OFFSET ?"
        params.extend([limit, offset])

        rows = conn.execute(query, params).fetchall()

        return [
            {
                "id": row["id"],
                "prompt": row["prompt"],
                "audio_url": _convert_to_full_url(row["audio_url"], base_url),
                "model_id": row["model_id"],
                "parameters": json.loads(row["parameters"])
                if row["parameters"]
                else {},
                "collection": row["collection"],
                "status": row["status"],
                "created_at": row["created_at"],
                "metadata": json.loads(row["metadata"]) if row["metadata"] else {},
                "duration": row["duration"],
                "brief_id": row["brief_id"],
                "client_id": row["client_id"],
                "campaign_id": row["campaign_id"],
            }
            for row in rows
        ]


def delete_audio(audio_id: int) -> bool:
    """Delete an audio by ID."""
    with get_db() as conn:
        cursor = conn.execute("DELETE FROM generated_audio WHERE id = ?", (audio_id,))
        conn.commit()
        return cursor.rowcount > 0


def mark_audio_download_attempted(audio_id: int) -> bool:
    """Mark that a download has been attempted for an audio. Returns False if already attempted."""
    with get_db() as conn:
        # Check if already attempted
        row = conn.execute(
            "SELECT download_attempted FROM generated_audio WHERE id = ?", (audio_id,)
        ).fetchone()

        if row and row["download_attempted"]:
            return False  # Already attempted

        # Mark as attempted
        conn.execute(
            "UPDATE generated_audio SET download_attempted = 1 WHERE id = ?",
            (audio_id,),
        )
        conn.commit()
        return True


def increment_audio_download_retries(audio_id: int) -> int:
    """Increment the download retry counter for an audio and return the new count."""
    with get_db() as conn:
        conn.execute(
            "UPDATE generated_audio SET download_retries = download_retries + 1 WHERE id = ?",
            (audio_id,),
        )
        conn.commit()

        row = conn.execute(
            "SELECT download_retries FROM generated_audio WHERE id = ?", (audio_id,)
        ).fetchone()

        return row["download_retries"] if row else 0


def mark_audio_download_failed(audio_id: int, error: str) -> None:
    """Mark an audio download as permanently failed."""
    with get_db() as conn:
        conn.execute(
            """
            UPDATE generated_audio
            SET status = 'failed', download_error = ?
            WHERE id = ?
            """,
            (error, audio_id),
        )
        conn.commit()


# Authentication helper functions
def create_user(
    username: str, email: str, hashed_password: str, is_admin: bool = False
) -> int:
    """Create a new user."""
    with get_db() as conn:
        cursor = conn.execute(
            """
            INSERT INTO users (username, email, hashed_password, is_admin)
            VALUES (?, ?, ?, ?)
            """,
            (username, email, hashed_password, is_admin),
        )
        conn.commit()
        return cursor.lastrowid


def get_user_by_username(username: str) -> Optional[Dict[str, Any]]:
    """Get user by username."""
    with get_db() as conn:
        row = conn.execute(
            "SELECT * FROM users WHERE username = ?", (username,)
        ).fetchone()

        if row:
            return {
                "id": row["id"],
                "username": row["username"],
                "email": row["email"],
                "hashed_password": row["hashed_password"],
                "is_active": bool(row["is_active"]),
                "is_admin": bool(row["is_admin"]),
                "created_at": row["created_at"],
                "last_login": row["last_login"],
            }
    return None


def update_user_last_login(user_id: int) -> None:
    """Update user's last login timestamp."""
    with get_db() as conn:
        conn.execute(
            "UPDATE users SET last_login = CURRENT_TIMESTAMP WHERE id = ?", (user_id,)
        )
        conn.commit()


def create_api_key(
    key_hash: str, name: str, user_id: int, expires_at: Optional[str] = None
) -> int:
    """Create a new API key."""
    with get_db() as conn:
        cursor = conn.execute(
            """
            INSERT INTO api_keys (key_hash, name, user_id, expires_at)
            VALUES (?, ?, ?, ?)
            """,
            (key_hash, name, user_id, expires_at),
        )
        conn.commit()
        return cursor.lastrowid


def get_api_key_by_hash(key_hash: str) -> Optional[Dict[str, Any]]:
    """Get API key by hash."""
    with get_db() as conn:
        row = conn.execute(
            """
            SELECT ak.*, u.username, u.is_active as user_is_active
            FROM api_keys ak
            JOIN users u ON ak.user_id = u.id
            WHERE ak.key_hash = ?
            """,
            (key_hash,),
        ).fetchone()

        if row:
            return {
                "id": row["id"],
                "key_hash": row["key_hash"],
                "name": row["name"],
                "user_id": row["user_id"],
                "username": row["username"],
                "is_active": bool(row["is_active"]),
                "user_is_active": bool(row["user_is_active"]),
                "created_at": row["created_at"],
                "last_used": row["last_used"],
                "expires_at": row["expires_at"],
            }
    return None


def update_api_key_last_used(key_hash: str) -> None:
    """Update API key's last used timestamp."""
    with get_db() as conn:
        conn.execute(
            "UPDATE api_keys SET last_used = CURRENT_TIMESTAMP WHERE key_hash = ?",
            (key_hash,),
        )
        conn.commit()


def list_api_keys(user_id: int) -> List[Dict[str, Any]]:
    """List all API keys for a user."""
    with get_db() as conn:
        rows = conn.execute(
            """
            SELECT id, name, is_active, created_at, last_used, expires_at
            FROM api_keys
            WHERE user_id = ?
            ORDER BY created_at DESC
            """,
            (user_id,),
        ).fetchall()

        return [
            {
                "id": row["id"],
                "name": row["name"],
                "is_active": bool(row["is_active"]),
                "created_at": row["created_at"],
                "last_used": row["last_used"],
                "expires_at": row["expires_at"],
            }
            for row in rows
        ]


def revoke_api_key(key_id: int, user_id: int) -> bool:
    """Revoke an API key."""
    with get_db() as conn:
        cursor = conn.execute(
            "UPDATE api_keys SET is_active = 0 WHERE id = ? AND user_id = ?",
            (key_id, user_id),
        )
        conn.commit()
        return cursor.rowcount > 0


# Creative Briefs CRUD functions
def save_creative_brief(
    brief_id: str,
    user_id: int,
    prompt_text: Optional[str] = None,
    image_url: Optional[str] = None,
    video_url: Optional[str] = None,
    image_data: Optional[bytes] = None,
    video_data: Optional[bytes] = None,
    creative_direction: Optional[Dict[str, Any]] = None,
    scenes: Optional[List[Dict[str, Any]]] = None,
    confidence_score: Optional[float] = None,
) -> str:
    """Save a creative brief to the database."""
    # Serialize dict/list data to JSON strings
    cd_json = json.dumps(creative_direction) if creative_direction else None
    scenes_json = json.dumps(scenes) if scenes else None

    with get_db() as conn:
        conn.execute(
            """
            INSERT OR REPLACE INTO creative_briefs
            (id, user_id, prompt_text, image_url, video_url, image_data, video_data, creative_direction, scenes, confidence_score, updated_at)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP)
            """,
            (
                brief_id,
                user_id,
                prompt_text,
                image_url,
                video_url,
                image_data,
                video_data,
                cd_json,
                scenes_json,
                confidence_score,
            ),
        )
        conn.commit()
        return brief_id


def get_creative_brief(brief_id: str, user_id: int) -> Optional[Dict[str, Any]]:
    """Get a specific creative brief by ID for a user."""
    with get_db() as conn:
        row = conn.execute(
            """
            SELECT id, user_id, prompt_text, image_url, video_url, image_data, video_data,
                   creative_direction, scenes, confidence_score,
                   created_at, updated_at
            FROM creative_briefs
            WHERE id = ? AND user_id = ?
            """,
            (brief_id, user_id),
        ).fetchone()

        if row:
            return {
                "id": row["id"],
                "user_id": row["user_id"],
                "prompt_text": row["prompt_text"],
                "image_url": row["image_url"],
                "video_url": row["video_url"],
                "image_data": row["image_data"],
                "video_data": row["video_data"],
                "creative_direction": json.loads(row["creative_direction"])
                if row["creative_direction"]
                else None,
                "scenes": json.loads(row["scenes"]) if row["scenes"] else None,
                "confidence_score": row["confidence_score"],
                "created_at": row["created_at"],
                "updated_at": row["updated_at"],
            }
    return None


def get_user_briefs(
    user_id: int, limit: int = 50, offset: int = 0
) -> List[Dict[str, Any]]:
    """Get all creative briefs for a user with pagination."""
    with get_db() as conn:
        rows = conn.execute(
            """
            SELECT id, user_id, prompt_text, image_url, video_url, image_data, video_data,
                   creative_direction, scenes, confidence_score,
                   created_at, updated_at
            FROM creative_briefs
            WHERE user_id = ?
            ORDER BY created_at DESC
            LIMIT ? OFFSET ?
            """,
            (user_id, limit, offset),
        ).fetchall()

        return [
            {
                "id": row["id"],
                "user_id": row["user_id"],
                "prompt_text": row["prompt_text"],
                "image_url": row["image_url"],
                "video_url": row["video_url"],
                "image_data": row["image_data"],
                "video_data": row["video_data"],
                "creative_direction": json.loads(row["creative_direction"])
                if row["creative_direction"]
                else None,
                "scenes": json.loads(row["scenes"]) if row["scenes"] else None,
                "confidence_score": row["confidence_score"],
                "created_at": row["created_at"],
                "updated_at": row["updated_at"],
            }
            for row in rows
        ]


def update_brief(
    brief_id: str,
    user_id: int,
    prompt_text: Optional[str] = None,
    image_url: Optional[str] = None,
    video_url: Optional[str] = None,
    image_data: Optional[bytes] = None,
    video_data: Optional[bytes] = None,
    creative_direction: Optional[Dict[str, Any]] = None,
    scenes: Optional[List[Dict[str, Any]]] = None,
    confidence_score: Optional[float] = None,
) -> bool:
    """Update a creative brief."""
    with get_db() as conn:
        # Build dynamic update query
        update_fields = []
        values = []

        if prompt_text is not None:
            update_fields.append("prompt_text = ?")
            values.append(prompt_text)
        if image_url is not None:
            update_fields.append("image_url = ?")
            values.append(image_url)
        if video_url is not None:
            update_fields.append("video_url = ?")
            values.append(video_url)
        if image_data is not None:
            update_fields.append("image_data = ?")
            values.append(image_data)
        if video_data is not None:
            update_fields.append("video_data = ?")
            values.append(video_data)
        if creative_direction is not None:
            update_fields.append("creative_direction = ?")
            values.append(json.dumps(creative_direction))
        if scenes is not None:
            update_fields.append("scenes = ?")
            values.append(json.dumps(scenes))
        if confidence_score is not None:
            update_fields.append("confidence_score = ?")
            values.append(confidence_score)

        if not update_fields:
            return False  # Nothing to update

        update_fields.append("updated_at = CURRENT_TIMESTAMP")
        values.extend([brief_id, user_id])

        query = f"""
            UPDATE creative_briefs
            SET {", ".join(update_fields)}
            WHERE id = ? AND user_id = ?
        """

        cursor = conn.execute(query, values)
        conn.commit()
        return cursor.rowcount > 0


def delete_brief(brief_id: str, user_id: int) -> bool:
    """Delete a creative brief."""
    with get_db() as conn:
        cursor = conn.execute(
            "DELETE FROM creative_briefs WHERE id = ? AND user_id = ?",
            (brief_id, user_id),
        )
        conn.commit()
        return cursor.rowcount > 0


def get_brief_count(user_id: int) -> int:
    """Get the total count of briefs for a user."""
    with get_db() as conn:
        row = conn.execute(
            "SELECT COUNT(*) as count FROM creative_briefs WHERE user_id = ?",
            (user_id,),
        ).fetchone()
        return row["count"] if row else 0


# Video generation job helper functions (for v2 API)
def update_job_progress(job_id: int, progress: dict) -> bool:
    """
    Update the progress JSON field for a job.
    The updated_at timestamp is automatically updated by the trigger.

    Args:
        job_id: The video job ID
        progress: Dictionary containing progress information

    Returns:
        True on success, False on failure
    """
    try:
        with get_db() as conn:
            cursor = conn.execute(
                "UPDATE generated_videos SET progress = ? WHERE id = ?",
                (json.dumps(progress), job_id),
            )
            conn.commit()
            return cursor.rowcount > 0
    except Exception as e:
        print(f"Error updating job progress for job {job_id}: {e}")
        return False


def get_job(job_id: int) -> Optional[Dict[str, Any]]:
    """
    Retrieve a complete job record by ID.

    Args:
        job_id: The video job ID

    Returns:
        Dictionary with all job fields, or None if not found
    """
    try:
        with get_db() as conn:
            row = conn.execute(
                "SELECT * FROM generated_videos WHERE id = ?", (job_id,)
            ).fetchone()

            if row:
                # Helper function to safely get column value
                def safe_get(key, default=None):
                    try:
                        return row[key]
                    except (KeyError, IndexError):
                        return default

                return {
                    "id": row["id"],
                    "prompt": row["prompt"],
                    "video_url": row["video_url"],
                    "model_id": row["model_id"],
                    "parameters": json.loads(row["parameters"])
                    if row["parameters"]
                    else {},
                    "status": row["status"],
                    "created_at": row["created_at"],
                    "collection": row["collection"],
                    "brief_id": safe_get("brief_id"),
                    "metadata": json.loads(row["metadata"])
                    if row["metadata"]
                    else None,
                    "download_attempted": bool(safe_get("download_attempted", 0)),
                    "download_retries": safe_get("download_retries", 0),
                    "download_error": safe_get("download_error"),
                    "progress": json.loads(safe_get("progress"))
                    if safe_get("progress")
                    else {},
                    "storyboard_data": json.loads(safe_get("storyboard_data"))
                    if safe_get("storyboard_data")
                    else None,
                    "approved": bool(safe_get("approved", 0)),
                    "approved_at": safe_get("approved_at"),
                    "estimated_cost": safe_get("estimated_cost", 0.0),
                    "actual_cost": safe_get("actual_cost", 0.0),
                    "error_message": safe_get("error_message"),
                    "updated_at": safe_get("updated_at"),
                }
    except Exception as e:
        print(f"Error retrieving job {job_id}: {e}")
        import traceback

        traceback.print_exc()
    return None


def increment_retry_count(job_id: int) -> int:
    """
    Increment the retry_count (download_retries) for a failed job.

    Args:
        job_id: The video job ID

    Returns:
        The new retry count value
    """
    try:
        with get_db() as conn:
            conn.execute(
                "UPDATE generated_videos SET download_retries = download_retries + 1 WHERE id = ?",
                (job_id,),
            )
            conn.commit()

            row = conn.execute(
                "SELECT download_retries FROM generated_videos WHERE id = ?", (job_id,)
            ).fetchone()

            return row["download_retries"] if row else 0
    except Exception as e:
        print(f"Error incrementing retry count for job {job_id}: {e}")
        return 0


def mark_job_failed(job_id: int, error_message: str) -> bool:
    """
    Mark a job as failed with an error message.
    The updated_at timestamp is automatically updated by the trigger.

    Args:
        job_id: The video job ID
        error_message: Description of the error

    Returns:
        True on success, False on failure
    """
    try:
        with get_db() as conn:
            cursor = conn.execute(
                """
                UPDATE generated_videos
                SET status = 'failed', error_message = ?
                WHERE id = ?
                """,
                (error_message, job_id),
            )
            conn.commit()
            return cursor.rowcount > 0
    except Exception as e:
        print(f"Error marking job {job_id} as failed: {e}")
        return False


def get_jobs_by_status(status: str, limit: int = 50) -> List[Dict[str, Any]]:
    """
    Get jobs with a specific status, ordered by most recently updated.

    Args:
        status: The status to filter by ('pending', 'processing', 'completed', 'failed', etc.)
        limit: Maximum number of records to return (default 50)

    Returns:
        List of job dictionaries
    """
    try:
        with get_db() as conn:
            rows = conn.execute(
                """
                SELECT * FROM generated_videos
                WHERE status = ?
                ORDER BY updated_at DESC
                LIMIT ?
                """,
                (status, limit),
            ).fetchall()

            result = []
            for row in rows:
                # Helper function to safely get column value
                def safe_get(key, default=None):
                    try:
                        return row[key]
                    except (KeyError, IndexError):
                        return default

                result.append(
                    {
                        "id": row["id"],
                        "prompt": row["prompt"],
                        "video_url": row["video_url"],
                        "model_id": row["model_id"],
                        "parameters": json.loads(row["parameters"])
                        if row["parameters"]
                        else {},
                        "status": row["status"],
                        "created_at": row["created_at"],
                        "collection": row["collection"],
                        "brief_id": safe_get("brief_id"),
                        "metadata": json.loads(row["metadata"])
                        if row["metadata"]
                        else None,
                        "download_attempted": bool(safe_get("download_attempted", 0)),
                        "download_retries": safe_get("download_retries", 0),
                        "download_error": safe_get("download_error"),
                        "progress": json.loads(safe_get("progress"))
                        if safe_get("progress")
                        else {},
                        "storyboard_data": json.loads(safe_get("storyboard_data"))
                        if safe_get("storyboard_data")
                        else None,
                        "approved": bool(safe_get("approved", 0)),
                        "approved_at": safe_get("approved_at"),
                        "estimated_cost": safe_get("estimated_cost", 0.0),
                        "actual_cost": safe_get("actual_cost", 0.0),
                        "error_message": safe_get("error_message"),
                        "updated_at": safe_get("updated_at"),
                    }
                )
            return result
    except Exception as e:
        print(f"Error retrieving jobs by status '{status}': {e}")
        import traceback

        traceback.print_exc()
        return []


def approve_storyboard(job_id: int) -> bool:
    """
    Mark a job's storyboard as approved.
    Sets approved=True and approved_at=CURRENT_TIMESTAMP.

    Args:
        job_id: The video job ID

    Returns:
        True on success, False on failure
    """
    try:
        with get_db() as conn:
            cursor = conn.execute(
                """
                UPDATE generated_videos
                SET approved = 1, approved_at = CURRENT_TIMESTAMP
                WHERE id = ?
                """,
                (job_id,),
            )
            conn.commit()
            return cursor.rowcount > 0
    except Exception as e:
        print(f"Error approving storyboard for job {job_id}: {e}")
        return False


def create_video_job(
    prompt: str,
    model_id: str,
    parameters: dict,
    estimated_cost: float,
    client_id: Optional[str] = None,
    status: str = "pending",
) -> int:
    """
    Create a new video generation job for the v2 workflow.

    Args:
        prompt: User's video concept prompt
        model_id: Model identifier being used
        parameters: Generation parameters
        estimated_cost: Estimated cost in USD
        client_id: Optional client identifier
        status: Initial status (default: 'pending')

    Returns:
        The newly created job ID
    """
    try:
        with get_db() as conn:
            # Initialize progress as empty dict
            progress = json.dumps(
                {
                    "current_stage": status,
                    "scenes_total": 0,
                    "scenes_completed": 0,
                    "current_scene": None,
                    "estimated_completion_seconds": None,
                    "message": "Job created, waiting to start",
                }
            )

            cursor = conn.execute(
                """
                INSERT INTO generated_videos
                (prompt, video_url, model_id, parameters, status, estimated_cost, progress, client_id)
                VALUES (?, '', ?, ?, ?, ?, ?, ?)
                """,
                (
                    prompt,
                    model_id,
                    json.dumps(parameters),
                    status,
                    estimated_cost,
                    progress,
                    client_id,
                ),
            )
            conn.commit()
            return cursor.lastrowid or 0
    except Exception as e:
        print(f"Error creating video job: {e}")
        import traceback

        traceback.print_exc()
        return 0


def update_storyboard_data(job_id: int, storyboard_data: List[Dict[str, Any]]) -> bool:
    """
    Update the storyboard_data field for a job.

    Args:
        job_id: The video job ID
        storyboard_data: List of storyboard entries

    Returns:
        True on success, False on failure
    """
    try:
        with get_db() as conn:
            cursor = conn.execute(
                "UPDATE generated_videos SET storyboard_data = ?, status = 'storyboard_ready' WHERE id = ?",
                (json.dumps(storyboard_data), job_id),
            )
            conn.commit()
            return cursor.rowcount > 0
    except Exception as e:
        print(f"Error updating storyboard data for job {job_id}: {e}")
        return False


def get_jobs_by_client(
    client_id: str, status: Optional[str] = None, limit: int = 50
) -> List[Dict[str, Any]]:
    """
    Get jobs for a specific client, optionally filtered by status.

    Args:
        client_id: The client identifier
        status: Optional status filter
        limit: Maximum number of records to return

    Returns:
        List of job dictionaries
    """
    try:
        with get_db() as conn:
            if status:
                query = """
                    SELECT * FROM generated_videos
                    WHERE client_id = ? AND status = ?
                    ORDER BY created_at DESC
                    LIMIT ?
                """
                params = (client_id, status, limit)
            else:
                query = """
                    SELECT * FROM generated_videos
                    WHERE client_id = ?
                    ORDER BY created_at DESC
                    LIMIT ?
                """
                params = (client_id, limit)

            rows = conn.execute(query, params).fetchall()

            result = []
            for row in rows:

                def safe_get(key, default=None):
                    try:
                        return row[key]
                    except (KeyError, IndexError):
                        return default

                result.append(
                    {
                        "id": row["id"],
                        "prompt": row["prompt"],
                        "video_url": row["video_url"],
                        "model_id": row["model_id"],
                        "parameters": json.loads(row["parameters"])
                        if row["parameters"]
                        else {},
                        "status": row["status"],
                        "created_at": row["created_at"],
                        "client_id": safe_get("client_id"),
                        "progress": json.loads(safe_get("progress"))
                        if safe_get("progress")
                        else {},
                        "storyboard_data": json.loads(safe_get("storyboard_data"))
                        if safe_get("storyboard_data")
                        else None,
                        "approved": bool(safe_get("approved", 0)),
                        "approved_at": safe_get("approved_at"),
                        "estimated_cost": safe_get("estimated_cost", 0.0),
                        "actual_cost": safe_get("actual_cost", 0.0),
                        "error_message": safe_get("error_message"),
                        "updated_at": safe_get("updated_at"),
                    }
                )
            return result
    except Exception as e:
        print(f"Error retrieving jobs for client {client_id}: {e}")
        import traceback

        traceback.print_exc()
        return []


# Asset management functions
def save_uploaded_asset(
    asset_id: str,
    user_id: int,
    filename: str,
    file_path: str,
    file_type: str,
    size_bytes: int,
) -> str:
    """Save an uploaded asset to the database."""
    with get_db() as conn:
        conn.execute(
            """
            INSERT INTO uploaded_assets (asset_id, user_id, filename, file_path, file_type, size_bytes)
            VALUES (?, ?, ?, ?, ?, ?)
            """,
            (asset_id, user_id, filename, file_path, file_type, size_bytes),
        )
        conn.commit()
        return asset_id


def get_asset_by_id(asset_id: str) -> Optional[Dict[str, Any]]:
    """Retrieve a specific asset by ID."""
    with get_db() as conn:
        row = conn.execute(
            "SELECT * FROM uploaded_assets WHERE asset_id = ?", (asset_id,)
        ).fetchone()

        if row:
            return {
                "asset_id": row["asset_id"],
                "user_id": row["user_id"],
                "filename": row["filename"],
                "file_path": row["file_path"],
                "file_type": row["file_type"],
                "size_bytes": row["size_bytes"],
                "uploaded_at": row["uploaded_at"],
            }
    return None


def list_user_assets(
    user_id: int, limit: int = 50, offset: int = 0
) -> List[Dict[str, Any]]:
    """List all assets for a user with pagination."""
    with get_db() as conn:
        rows = conn.execute(
            """
            SELECT * FROM uploaded_assets
            WHERE user_id = ?
            ORDER BY uploaded_at DESC
            LIMIT ? OFFSET ?
            """,
            (user_id, limit, offset),
        ).fetchall()

        return [
            {
                "asset_id": row["asset_id"],
                "filename": row["filename"],
                "file_type": row["file_type"],
                "size_bytes": row["size_bytes"],
                "uploaded_at": row["uploaded_at"],
            }
            for row in rows
        ]


def delete_asset(asset_id: str, user_id: int) -> bool:
    """Delete an asset by ID (only if it belongs to the user)."""
    with get_db() as conn:
        cursor = conn.execute(
            "DELETE FROM uploaded_assets WHERE asset_id = ? AND user_id = ?",
            (asset_id, user_id),
        )
        conn.commit()
        return cursor.rowcount > 0


# Video Export and Refinement functions
def increment_download_count(job_id: int) -> bool:
    """
    Increment the download count for a video job.

    Args:
        job_id: The video job ID

    Returns:
        True on success, False on failure
    """
    try:
        with get_db() as conn:
            cursor = conn.execute(
                "UPDATE generated_videos SET download_count = COALESCE(download_count, 0) + 1 WHERE id = ?",
                (job_id,),
            )
            conn.commit()
            return cursor.rowcount > 0
    except Exception as e:
        print(f"Error incrementing download count for job {job_id}: {e}")
        return False


def get_download_count(job_id: int) -> int:
    """
    Get the download count for a video job.

    Args:
        job_id: The video job ID

    Returns:
        Download count (0 if not found or error)
    """
    try:
        with get_db() as conn:
            row = conn.execute(
                "SELECT COALESCE(download_count, 0) as count FROM generated_videos WHERE id = ?",
                (job_id,),
            ).fetchone()
            return row["count"] if row else 0
    except Exception as e:
        print(f"Error getting download count for job {job_id}: {e}")
        return 0


def refine_scene_in_storyboard(
    job_id: int,
    scene_number: int,
    new_image_url: Optional[str] = None,
    new_description: Optional[str] = None,
    new_image_prompt: Optional[str] = None,
) -> bool:
    """
    Refine a specific scene in the storyboard by updating its data.

    Args:
        job_id: The video job ID
        scene_number: Scene number to refine (1-indexed)
        new_image_url: New image URL (if regenerated)
        new_description: New scene description
        new_image_prompt: New image generation prompt

    Returns:
        True on success, False on failure
    """
    try:
        job = get_job(job_id)
        if not job:
            print(f"Job {job_id} not found")
            return False

        storyboard_data = job.get("storyboard_data")
        if not storyboard_data:
            print(f"No storyboard data for job {job_id}")
            return False

        # Find the scene to update
        scene_found = False
        for entry in storyboard_data:
            scene = entry.get("scene", {})
            if scene.get("scene_number") == scene_number:
                scene_found = True

                # Update scene data
                if new_image_url:
                    entry["image_url"] = new_image_url
                    entry["generation_status"] = "completed"

                if new_description:
                    scene["description"] = new_description

                if new_image_prompt:
                    scene["image_prompt"] = new_image_prompt

                break

        if not scene_found:
            print(f"Scene {scene_number} not found in job {job_id}")
            return False

        # Update database with modified storyboard and reset approval
        with get_db() as conn:
            cursor = conn.execute(
                """
                UPDATE generated_videos
                SET storyboard_data = ?,
                    approved = 0,
                    approved_at = NULL,
                    refinement_count = COALESCE(refinement_count, 0) + 1
                WHERE id = ?
                """,
                (json.dumps(storyboard_data), job_id),
            )
            conn.commit()
            return cursor.rowcount > 0

    except Exception as e:
        print(f"Error refining scene {scene_number} in job {job_id}: {e}")
        import traceback

        traceback.print_exc()
        return False


def reorder_storyboard_scenes(job_id: int, scene_order: List[int]) -> bool:
    """
    Reorder scenes in the storyboard.

    Args:
        job_id: The video job ID
        scene_order: New order of scene numbers (e.g., [1, 3, 2, 4])

    Returns:
        True on success, False on failure
    """
    try:
        job = get_job(job_id)
        if not job:
            print(f"Job {job_id} not found")
            return False

        storyboard_data = job.get("storyboard_data")
        if not storyboard_data:
            print(f"No storyboard data for job {job_id}")
            return False

        # Validate scene_order
        current_scene_numbers = [
            entry.get("scene", {}).get("scene_number") for entry in storyboard_data
        ]
        if sorted(scene_order) != sorted(current_scene_numbers):
            print(f"Invalid scene order: {scene_order} vs {current_scene_numbers}")
            return False

        # Create a mapping of old scene numbers to entries
        scene_map = {
            entry.get("scene", {}).get("scene_number"): entry
            for entry in storyboard_data
        }

        # Reorder based on new order
        reordered_storyboard = []
        for new_position, old_scene_number in enumerate(scene_order, start=1):
            entry = scene_map[old_scene_number]
            # Update scene number to match new position
            entry["scene"]["scene_number"] = new_position
            reordered_storyboard.append(entry)

        # Update database with reordered storyboard and reset approval
        with get_db() as conn:
            cursor = conn.execute(
                """
                UPDATE generated_videos
                SET storyboard_data = ?,
                    approved = 0,
                    approved_at = NULL
                WHERE id = ?
                """,
                (json.dumps(reordered_storyboard), job_id),
            )
            conn.commit()
            return cursor.rowcount > 0

    except Exception as e:
        print(f"Error reordering scenes in job {job_id}: {e}")
        import traceback

        traceback.print_exc()
        return False


def get_refinement_count(job_id: int) -> int:
    """
    Get the refinement count for a job.

    Args:
        job_id: The video job ID

    Returns:
        Refinement count (0 if not found or error)
    """
    try:
        with get_db() as conn:
            row = conn.execute(
                "SELECT COALESCE(refinement_count, 0) as count FROM generated_videos WHERE id = ?",
                (job_id,),
            ).fetchone()
            return row["count"] if row else 0
    except Exception as e:
        print(f"Error getting refinement count for job {job_id}: {e}")
        return 0


def increment_estimated_cost(job_id: int, additional_cost: float) -> bool:
    """
    Increment the estimated cost for a job (used when refining scenes).

    Args:
        job_id: The video job ID
        additional_cost: Additional cost to add

    Returns:
        True on success, False on failure
    """
    try:
        with get_db() as conn:
            cursor = conn.execute(
                """
                UPDATE generated_videos
                SET estimated_cost = COALESCE(estimated_cost, 0.0) + ?
                WHERE id = ?
                """,
                (additional_cost, job_id),
            )
            conn.commit()
            return cursor.rowcount > 0
    except Exception as e:
        print(f"Error incrementing estimated cost for job {job_id}: {e}")
        return False


def get_generated_images_by_client(
    client_id: str, status: Optional[str] = None, limit: int = 50
) -> List[Dict[str, Any]]:
    """
    Get generated images for a specific client, optionally filtered by status.

    Args:
        client_id: The client identifier
        status: Optional status filter
        limit: Maximum number of records to return

    Returns:
        List of image dictionaries
    """
    try:
        with get_db() as conn:
            if status:
                query = """
                    SELECT id, prompt, image_url, model_id, parameters, status,
                           created_at, collection, metadata, download_attempted,
                           download_retries, download_error, brief_id, client_id, campaign_id
                    FROM generated_images
                    WHERE client_id = ? AND status = ?
                    ORDER BY created_at DESC
                    LIMIT ?
                """
                params = (client_id, status, limit)
            else:
                query = """
                    SELECT id, prompt, image_url, model_id, parameters, status,
                           created_at, collection, metadata, download_attempted,
                           download_retries, download_error, brief_id, client_id, campaign_id
                    FROM generated_images
                    WHERE client_id = ?
                    ORDER BY created_at DESC
                    LIMIT ?
                """
                params = (client_id, limit)

            rows = conn.execute(query, params).fetchall()

            result = []
            for row in rows:
                result.append(dict(row))

            return result
    except Exception as e:
        print(f"Error getting images for client {client_id}: {e}")
        return []


def get_generated_videos_by_client(
    client_id: str, status: Optional[str] = None, limit: int = 50
) -> List[Dict[str, Any]]:
    """
    Get generated videos for a specific client, optionally filtered by status.

    Args:
        client_id: The client identifier
        status: Optional status filter
        limit: Maximum number of records to return

    Returns:
        List of video dictionaries
    """
    try:
        with get_db() as conn:
            if status:
                query = """
                    SELECT id, prompt, video_url, model_id, parameters, status,
                           created_at, collection, metadata, download_attempted,
                           download_retries, download_error, progress, storyboard_data,
                           approved, approved_at, estimated_cost, actual_cost,
                           error_message, updated_at, download_count, refinement_count,
                           brief_id, client_id, campaign_id
                    FROM generated_videos
                    WHERE client_id = ? AND status = ?
                    ORDER BY created_at DESC
                    LIMIT ?
                """
                params = (client_id, status, limit)
            else:
                query = """
                    SELECT id, prompt, video_url, model_id, parameters, status,
                           created_at, collection, metadata, download_attempted,
                           download_retries, download_error, progress, storyboard_data,
                           approved, approved_at, estimated_cost, actual_cost,
                           error_message, updated_at, download_count, refinement_count,
                           brief_id, client_id, campaign_id
                    FROM generated_videos
                    WHERE client_id = ?
                    ORDER BY created_at DESC
                    LIMIT ?
                """
                params = (client_id, limit)

            rows = conn.execute(query, params).fetchall()

            result = []
            for row in rows:
                result.append(dict(row))

            return result
    except Exception as e:
        print(f"Error getting videos for client {client_id}: {e}")
        return []


def get_generated_images_by_campaign(
    campaign_id: str, status: Optional[str] = None, limit: int = 50
) -> List[Dict[str, Any]]:
    """
    Get generated images for a specific campaign, optionally filtered by status.

    Args:
        campaign_id: The campaign identifier
        status: Optional status filter
        limit: Maximum number of records to return

    Returns:
        List of image dictionaries
    """
    try:
        with get_db() as conn:
            if status:
                query = """
                    SELECT id, prompt, image_url, model_id, parameters, status,
                           created_at, collection, metadata, download_attempted,
                           download_retries, download_error, brief_id, client_id, campaign_id
                    FROM generated_images
                    WHERE campaign_id = ? AND status = ?
                    ORDER BY created_at DESC
                    LIMIT ?
                """
                params = (campaign_id, status, limit)
            else:
                query = """
                    SELECT id, prompt, image_url, model_id, parameters, status,
                           created_at, collection, metadata, download_attempted,
                           download_retries, download_error, brief_id, client_id, campaign_id
                    FROM generated_images
                    WHERE campaign_id = ?
                    ORDER BY created_at DESC
                    LIMIT ?
                """
                params = (campaign_id, limit)

            rows = conn.execute(query, params).fetchall()

            result = []
            for row in rows:
                result.append(dict(row))

            return result
    except Exception as e:
        print(f"Error getting images for campaign {campaign_id}: {e}")
        return []


def get_generated_videos_by_campaign(
    campaign_id: str, status: Optional[str] = None, limit: int = 50
) -> List[Dict[str, Any]]:
    """
    Get generated videos for a specific campaign, optionally filtered by status.

    Args:
        campaign_id: The campaign identifier
        status: Optional status filter
        limit: Maximum number of records to return

    Returns:
        List of video dictionaries
    """
    try:
        with get_db() as conn:
            if status:
                query = """
                    SELECT id, prompt, video_url, model_id, parameters, status,
                           created_at, collection, metadata, download_attempted,
                           download_retries, download_error, progress, storyboard_data,
                           approved, approved_at, estimated_cost, actual_cost,
                           error_message, updated_at, download_count, refinement_count,
                           brief_id, client_id, campaign_id
                    FROM generated_videos
                    WHERE campaign_id = ? AND status = ?
                    ORDER BY created_at DESC
                    LIMIT ?
                """
                params = (campaign_id, status, limit)
            else:
                query = """
                    SELECT id, prompt, video_url, model_id, parameters, status,
                           created_at, collection, metadata, download_attempted,
                           download_retries, download_error, progress, storyboard_data,
                           approved, approved_at, estimated_cost, actual_cost,
                           error_message, updated_at, download_count, refinement_count,
                           brief_id, client_id, campaign_id
                    FROM generated_videos
                    WHERE campaign_id = ?
                    ORDER BY created_at DESC
                    LIMIT ?
                """
                params = (campaign_id, limit)

            rows = conn.execute(query, params).fetchall()

            result = []
            for row in rows:
                result.append(dict(row))

            return result
    except Exception as e:
        print(f"Error getting videos for campaign {campaign_id}: {e}")
        return []


# ============================================================================
# VIDEO SUB-JOB MANAGEMENT (for parallel image-pair to video generation)
# ============================================================================


def create_sub_job(
    job_id: int,
    sub_job_number: int,
    image1_asset_id: str,
    image2_asset_id: str,
    model_id: str,
    input_parameters: Optional[dict] = None,
) -> str:
    """
    Create a new video sub-job for image pair processing.

    Args:
        job_id: Parent job ID (from generated_videos table)
        sub_job_number: Sequential number of this sub-job (1, 2, 3, etc.)
        image1_asset_id: ID of first image asset
        image2_asset_id: ID of second image asset
        model_id: Video generation model (veo3, hailuo-2.0, etc.)
        input_parameters: Optional dict of model-specific parameters

    Returns:
        str: ID of the created sub-job
    """
    import uuid

    sub_job_id = str(uuid.uuid4())

    with get_db() as conn:
        conn.execute(
            """
            INSERT INTO video_sub_jobs (
                id, job_id, sub_job_number, image1_asset_id, image2_asset_id,
                model_id, input_parameters, status
            )
            VALUES (?, ?, ?, ?, ?, ?, ?, 'pending')
            """,
            (
                sub_job_id,
                job_id,
                sub_job_number,
                image1_asset_id,
                image2_asset_id,
                model_id,
                json.dumps(input_parameters) if input_parameters else None,
            ),
        )
        conn.commit()

    logger.info(f"Created sub-job {sub_job_id} for job {job_id}")
    return sub_job_id


def update_sub_job_status(
    sub_job_id: str,
    status: str,
    replicate_prediction_id: Optional[str] = None,
    video_url: Optional[str] = None,
    video_blob_id: Optional[str] = None,
    duration_seconds: Optional[float] = None,
    error_message: Optional[str] = None,
    actual_cost: Optional[float] = None,
    progress: Optional[float] = None,
) -> bool:
    """
    Update a sub-job's status and related fields.

    Args:
        sub_job_id: Sub-job ID to update
        status: New status ('pending', 'processing', 'completed', 'failed')
        replicate_prediction_id: Optional Replicate prediction ID
        video_url: Optional generated video URL
        video_blob_id: Optional blob ID if video stored in database
        duration_seconds: Optional video duration
        error_message: Optional error message if failed
        actual_cost: Optional actual cost of generation
        progress: Optional progress (0.0 to 1.0)

    Returns:
        bool: True if update was successful
    """
    updates = ["status = ?", "updated_at = CURRENT_TIMESTAMP"]
    params = [status]

    if replicate_prediction_id is not None:
        updates.append("replicate_prediction_id = ?")
        params.append(replicate_prediction_id)

    if video_url is not None:
        updates.append("video_url = ?")
        params.append(video_url)

    if video_blob_id is not None:
        updates.append("video_blob_id = ?")
        params.append(video_blob_id)

    if duration_seconds is not None:
        updates.append("duration_seconds = ?")
        params.append(duration_seconds)

    if error_message is not None:
        updates.append("error_message = ?")
        params.append(error_message)

    if actual_cost is not None:
        updates.append("actual_cost = ?")
        params.append(actual_cost)

    if progress is not None:
        updates.append("progress = ?")
        params.append(progress)

    # Set timestamps based on status
    if status == "processing" and "started_at" not in updates:
        updates.append("started_at = CURRENT_TIMESTAMP")
    elif status in ["completed", "failed"] and "completed_at" not in updates:
        updates.append("completed_at = CURRENT_TIMESTAMP")

    params.append(sub_job_id)

    with get_db() as conn:
        cursor = conn.execute(
            f"UPDATE video_sub_jobs SET {', '.join(updates)} WHERE id = ?", params
        )
        conn.commit()
        return cursor.rowcount > 0


def get_sub_jobs_by_job(job_id: int) -> List[Dict[str, Any]]:
    """
    Get all sub-jobs for a parent job.

    Args:
        job_id: Parent job ID

    Returns:
        List of sub-job dictionaries
    """
    with get_db() as conn:
        rows = conn.execute(
            """
            SELECT *
            FROM video_sub_jobs
            WHERE job_id = ?
            ORDER BY sub_job_number ASC
            """,
            (job_id,),
        ).fetchall()

        return [
            {
                "id": row["id"],
                "jobId": row["job_id"],
                "subJobNumber": row["sub_job_number"],
                "image1AssetId": row["image1_asset_id"],
                "image2AssetId": row["image2_asset_id"],
                "replicatePredictionId": row["replicate_prediction_id"],
                "modelId": row["model_id"],
                "inputParameters": json.loads(row["input_parameters"])
                if row["input_parameters"]
                else None,
                "status": row["status"],
                "progress": row["progress"],
                "videoUrl": row["video_url"],
                "videoBlobId": row["video_blob_id"],
                "durationSeconds": row["duration_seconds"],
                "estimatedCost": row["estimated_cost"],
                "actualCost": row["actual_cost"],
                "errorMessage": row["error_message"],
                "retryCount": row["retry_count"],
                "startedAt": row["started_at"],
                "completedAt": row["completed_at"],
                "createdAt": row["created_at"],
                "updatedAt": row["updated_at"],
            }
            for row in rows
        ]


def get_sub_job_by_id(sub_job_id: str) -> Optional[Dict[str, Any]]:
    """
    Get a specific sub-job by ID.

    Args:
        sub_job_id: Sub-job ID

    Returns:
        Sub-job dict or None if not found
    """
    with get_db() as conn:
        row = conn.execute(
            "SELECT * FROM video_sub_jobs WHERE id = ?", (sub_job_id,)
        ).fetchone()

        if not row:
            return None

        return {
            "id": row["id"],
            "jobId": row["job_id"],
            "subJobNumber": row["sub_job_number"],
            "image1AssetId": row["image1_asset_id"],
            "image2AssetId": row["image2_asset_id"],
            "replicatePredictionId": row["replicate_prediction_id"],
            "modelId": row["model_id"],
            "inputParameters": json.loads(row["input_parameters"])
            if row["input_parameters"]
            else None,
            "status": row["status"],
            "progress": row["progress"],
            "videoUrl": row["video_url"],
            "videoBlobId": row["video_blob_id"],
            "durationSeconds": row["duration_seconds"],
            "estimatedCost": row["estimated_cost"],
            "actualCost": row["actual_cost"],
            "errorMessage": row["error_message"],
            "retryCount": row["retry_count"],
            "startedAt": row["started_at"],
            "completedAt": row["completed_at"],
            "createdAt": row["created_at"],
            "updatedAt": row["updated_at"],
        }


def get_sub_job_progress_summary(job_id: int) -> Dict[str, int]:
    """
    Get a summary of sub-job progress for a parent job.

    Args:
        job_id: Parent job ID

    Returns:
        Dict with counts: {total, pending, processing, completed, failed}
    """
    with get_db() as conn:
        rows = conn.execute(
            """
            SELECT status, COUNT(*) as count
            FROM video_sub_jobs
            WHERE job_id = ?
            GROUP BY status
            """,
            (job_id,),
        ).fetchall()

        summary = {
            "total": 0,
            "pending": 0,
            "processing": 0,
            "completed": 0,
            "failed": 0,
        }

        for row in rows:
            status = row["status"]
            count = row["count"]
            summary[status] = count
            summary["total"] += count

        return summary


def increment_sub_job_retry_count(sub_job_id: str) -> int:
    """
    Increment the retry count for a sub-job.

    Args:
        sub_job_id: Sub-job ID

    Returns:
        int: New retry count
    """
    with get_db() as conn:
        conn.execute(
            """
            UPDATE video_sub_jobs
            SET retry_count = retry_count + 1,
                updated_at = CURRENT_TIMESTAMP
            WHERE id = ?
            """,
            (sub_job_id,),
        )
        conn.commit()

        row = conn.execute(
            "SELECT retry_count FROM video_sub_jobs WHERE id = ?", (sub_job_id,)
        ).fetchone()

        return row["retry_count"] if row else 0


# Initialize database on import
init_db()
</file>

<file path="backend/schemas/assets.py">
"""
Asset Entity Types
Pydantic models for asset management, uploads, and metadata
"""

from datetime import datetime
from enum import Enum
from typing import Literal, Optional, Union
from pydantic import BaseModel, Field, field_serializer


# Format Types
class ImageFormat(str, Enum):
    JPG = "jpg"
    JPEG = "jpeg"
    PNG = "png"
    WEBP = "webp"
    GIF = "gif"
    SVG = "svg"
    SVG_XML = "svg+xml"


class VideoFormat(str, Enum):
    MP4 = "mp4"
    WEBM = "webm"
    MOV = "mov"
    AVI = "avi"
    MKV = "mkv"


class AudioFormat(str, Enum):
    MP3 = "mp3"
    WAV = "wav"
    OGG = "ogg"
    AAC = "aac"
    M4A = "m4a"


class DocumentFormat(str, Enum):
    PDF = "pdf"
    DOC = "doc"
    DOCX = "docx"
    TXT = "txt"


# Asset Type Enum
class AssetType(str, Enum):
    IMAGE = "image"
    VIDEO = "video"
    AUDIO = "audio"
    DOCUMENT = "document"


# Base Asset Model - Common fields for all asset types
class BaseAsset(BaseModel):
    """Base asset with all common fields"""

    id: str
    userId: str
    clientId: Optional[str] = (
        None  # OPTIONAL - asset may or may not be associated with a client
    )
    campaignId: Optional[str] = (
        None  # OPTIONAL - asset may be associated with a campaign
    )
    name: str
    url: str
    size: Optional[int] = None  # File size in bytes
    uploadedAt: str  # ISO 8601 timestamp
    tags: Optional[list[str]] = None
    thumbnailBlobId: Optional[str] = (
        None  # Reference to thumbnail in asset_blobs table (V3)
    )
    sourceUrl: Optional[str] = None  # Original URL where asset was downloaded from

    # NOTE: blob_data is stored in DB but NOT exposed in API responses
    # It's only used for internal storage when files are uploaded

    model_config = {
        "json_schema_extra": {
            "example": {
                "id": "123e4567-e89b-12d3-a456-426614174000",
                "userId": "1",
                "clientId": "client-uuid",  # Required
                "campaignId": "campaign-uuid",  # Optional
                "name": "example-asset",
                "url": "https://api.example.com/assets/123e4567",
                "size": 1024000,
                "uploadedAt": "2025-01-15T10:30:00Z",
                "tags": ["brand_logo", "product"],
            }
        }
    }


# Image Asset
class ImageAsset(BaseAsset):
    """Image asset with dimensions"""

    type: Literal["image"] = "image"
    format: ImageFormat
    width: int
    height: int

    model_config = {
        "json_schema_extra": {
            "example": {
                "id": "123e4567-e89b-12d3-a456-426614174000",
                "userId": "1",
                "clientId": "client-uuid",
                "campaignId": "campaign-uuid",
                "name": "example-image",
                "url": "https://api.example.com/assets/123e4567",
                "size": 1024000,
                "uploadedAt": "2025-01-15T10:30:00Z",
                "tags": ["brand_logo", "product"],
                "thumbnailBlobId": None,
                "sourceUrl": None,
                "type": "image",
                "format": "png",
                "width": 1920,
                "height": 1080,
            }
        }
    }


# Video Asset
class VideoAsset(BaseAsset):
    """Video asset with dimensions, duration, and thumbnail"""

    type: Literal["video"] = "video"
    format: VideoFormat
    width: int
    height: int
    duration: int  # Duration in seconds
    thumbnailUrl: str

    model_config = {
        "json_schema_extra": {
            "example": {
                "id": "123e4567-e89b-12d3-a456-426614174000",
                "userId": "1",
                "clientId": "client-uuid",
                "campaignId": "campaign-uuid",
                "name": "example-video",
                "url": "https://api.example.com/assets/123e4567",
                "size": 1024000,
                "uploadedAt": "2025-01-15T10:30:00Z",
                "tags": ["product_shot", "demo"],
                "thumbnailBlobId": "thumb-uuid",
                "sourceUrl": "https://example.com/video.mp4",
                "type": "video",
                "format": "mp4",
                "width": 1920,
                "height": 1080,
                "duration": 30,
                "thumbnailUrl": "https://api.example.com/assets/123e4567/thumbnail",
            }
        }
    }


# Audio Asset
class AudioAsset(BaseAsset):
    """Audio asset with duration and optional waveform"""

    type: Literal["audio"] = "audio"
    format: AudioFormat
    duration: int  # Duration in seconds
    waveformUrl: Optional[str] = None  # Optional waveform visualization URL

    model_config = {
        "json_schema_extra": {
            "example": {
                "id": "123e4567-e89b-12d3-a456-426614174000",
                "userId": "1",
                "clientId": "client-uuid",
                "campaignId": "campaign-uuid",
                "name": "example-audio",
                "url": "https://api.example.com/assets/123e4567",
                "size": 1024000,
                "uploadedAt": "2025-01-15T10:30:00Z",
                "tags": ["voiceover", "music"],
                "thumbnailBlobId": None,
                "sourceUrl": "https://example.com/audio.mp3",
                "type": "audio",
                "format": "mp3",
                "duration": 180,
                "waveformUrl": "https://api.example.com/assets/123e4567/waveform",
            }
        }
    }


# Document Asset
class DocumentAsset(BaseAsset):
    """Document asset with page count and optional thumbnail"""

    type: Literal["document"] = "document"
    format: DocumentFormat
    pageCount: Optional[int] = None
    thumbnailUrl: Optional[str] = None

    model_config = {
        "json_schema_extra": {
            "example": {
                "id": "123e4567-e89b-12d3-a456-426614174000",
                "userId": "1",
                "clientId": "client-uuid",
                "campaignId": "campaign-uuid",
                "name": "example-document",
                "url": "https://api.example.com/assets/123e4567",
                "size": 1024000,
                "uploadedAt": "2025-01-15T10:30:00Z",
                "tags": ["contract", "proposal"],
                "thumbnailBlobId": "thumb-uuid",
                "sourceUrl": "https://example.com/document.pdf",
                "type": "document",
                "format": "pdf",
                "pageCount": 10,
                "thumbnailUrl": "https://api.example.com/assets/123e4567/thumbnail",
            }
        }
    }


# Unified Asset Type (Discriminated Union)
Asset = Union[ImageAsset, VideoAsset, AudioAsset, DocumentAsset]


# Asset Tags
class VisualAssetTag(str, Enum):
    """Tags for visual assets (images/videos)"""

    FIRST_FRAME = "first_frame"
    SUBJECT = "subject"
    BRAND_LOGO = "brand_logo"
    PRODUCT_SHOT = "product_shot"
    BACKGROUND = "background"
    TRANSITION = "transition"
    CLOSING_FRAME = "closing_frame"


class AudioAssetTag(str, Enum):
    """Tags for audio assets"""

    USE_FULL_AUDIO = "use_full_audio"
    VOICE_SAMPLE = "voice_sample"


# All possible asset tags (union of visual and audio)
AssetTag = Union[VisualAssetTag, AudioAssetTag]


# Asset with metadata for video generation
class AssetWithMetadata(BaseModel):
    """
    Asset with metadata for video generation
    Includes source, tags, and priority for generation context
    """

    id: str
    url: str
    thumbnailUrl: Optional[str] = None
    type: AssetType
    source: Literal["campaign", "client", "uploaded"]
    name: str
    tags: list[str] = Field(default_factory=list)  # AssetTag values as strings
    priority: int = Field(ge=1)  # Order in the list (1-based)

    # Media-specific fields
    duration: Optional[int] = None  # For audio/video, in seconds
    waveformUrl: Optional[str] = None  # For audio visualization
    fileSize: Optional[int] = None
    mimeType: Optional[str] = None

    model_config = {
        "json_schema_extra": {
            "example": {
                "id": "123e4567-e89b-12d3-a456-426614174000",
                "url": "https://api.example.com/assets/123e4567",
                "thumbnailUrl": "https://api.example.com/assets/123e4567/thumbnail",
                "type": "video",
                "source": "campaign",
                "name": "product-demo",
                "tags": ["product_shot", "brand_logo"],
                "priority": 1,
                "duration": 30,
                "fileSize": 5242880,
                "mimeType": "video/mp4",
            }
        }
    }


# Input model for uploading a new asset
class UploadAssetInput(BaseModel):
    """Input model for asset upload requests"""

    name: str
    type: AssetType
    clientId: Optional[str] = None  # OPTIONAL - asset may be associated with a client
    campaignId: Optional[str] = (
        None  # OPTIONAL - asset may be associated with a campaign
    )
    tags: Optional[list[str]] = None

    # Note: File is handled separately via FastAPI's UploadFile
    # This model is for the form data fields


# Input model for uploading asset from URL
class UploadAssetFromUrlInput(BaseModel):
    """Input model for asset upload from URL"""

    name: str
    type: AssetType
    url: str  # URL to download asset from
    clientId: Optional[str] = None
    campaignId: Optional[str] = None
    tags: Optional[list[str]] = None

    model_config = {
        "json_schema_extra": {
            "example": {
                "name": "product-image",
                "type": "image",
                "url": "https://example.com/images/product.jpg",
                "clientId": "client-uuid",
                "campaignId": "campaign-uuid",
                "tags": ["product", "hero"],
            }
        }
    }


class AssetFromUrlItem(BaseModel):
    """Individual asset item for bulk upload from URLs"""

    name: str
    type: AssetType
    url: str  # URL to download asset from
    tags: Optional[list[str]] = None  # Per-asset tags

    model_config = {
        "json_schema_extra": {
            "example": {
                "name": "product-image-1",
                "type": "image",
                "url": "https://example.com/images/product1.jpg",
                "tags": ["product", "hero"],
            }
        }
    }


class BulkAssetFromUrlInput(BaseModel):
    """Input model for bulk asset upload from URLs"""

    assets: list[AssetFromUrlItem]  # Array of assets to upload
    clientId: Optional[str] = None  # Shared client ID for all assets
    campaignId: Optional[str] = None  # Shared campaign ID for all assets

    model_config = {
        "json_schema_extra": {
            "example": {
                "assets": [
                    {
                        "name": "product-image-1",
                        "type": "image",
                        "url": "https://example.com/images/product1.jpg",
                        "tags": ["product", "hero"],
                    },
                    {
                        "name": "product-image-2",
                        "type": "image",
                        "url": "https://example.com/images/product2.jpg",
                        "tags": ["product", "secondary"],
                    },
                ],
                "clientId": "client-uuid",
                "campaignId": "campaign-uuid",
            }
        }
    }


# Database model (internal use only - includes blob_data)
class AssetDB(BaseModel):
    """
    Internal database model for assets
    Includes blob_data field which is NOT exposed in API responses
    """

    id: str
    user_id: Optional[int] = None
    client_id: Optional[str] = None
    campaign_id: Optional[str] = None
    name: str
    asset_type: str
    url: str
    size: Optional[int] = None
    uploaded_at: datetime
    format: str
    tags: Optional[str] = None  # JSON string in DB
    width: Optional[int] = None
    height: Optional[int] = None
    duration: Optional[int] = None
    thumbnail_url: Optional[str] = None
    thumbnail_blob_id: Optional[str] = (
        None  # Reference to thumbnail in asset_blobs table (V3)
    )
    waveform_url: Optional[str] = None
    page_count: Optional[int] = None
    blob_data: Optional[bytes] = None  # Binary blob storage
    source_url: Optional[str] = None  # Original URL where asset was downloaded from

    @field_serializer("uploaded_at")
    def serialize_datetime(self, dt: datetime, _info):
        """Serialize datetime to ISO 8601 string"""
        return dt.isoformat() + "Z" if dt else None

    @field_serializer("tags")
    def serialize_tags(self, tags: Optional[str], _info):
        """Parse JSON string to list"""
        if not tags:
            return None
        import json

        try:
            return json.loads(tags)
        except:
            return None

    def to_asset_model(self) -> Asset:
        """Convert database model to appropriate Asset type"""
        import json

        # Parse tags from JSON string
        tags_list = None
        if self.tags:
            try:
                tags_list = json.loads(self.tags)
            except:
                pass

        # Common fields
        common = {
            "id": self.id,
            "userId": str(self.user_id) if self.user_id else "",
            "clientId": self.client_id,
            "campaignId": self.campaign_id,
            "name": self.name,
            "url": self.url,
            "size": self.size,
            "uploadedAt": self.uploaded_at.isoformat() + "Z",
            "tags": tags_list,
            "thumbnailBlobId": self.thumbnail_blob_id,
            "sourceUrl": self.source_url,
            "format": self.format,
        }

        # Type-specific fields
        if self.asset_type == "image":
            return ImageAsset(
                **common,
                width=self.width or 0,
                height=self.height or 0,
            )
        elif self.asset_type == "video":
            return VideoAsset(
                **common,
                width=self.width or 0,
                height=self.height or 0,
                duration=self.duration or 0,
                thumbnailUrl=self.thumbnail_url or "",
            )
        elif self.asset_type == "audio":
            return AudioAsset(
                **common,
                duration=self.duration or 0,
                waveformUrl=self.waveform_url,
            )
        elif self.asset_type == "document":
            return DocumentAsset(
                **common,
                pageCount=self.page_count,
                thumbnailUrl=self.thumbnail_url,
            )
        else:
            raise ValueError(f"Unknown asset type: {self.asset_type}")
</file>

<file path="backend/services/sub_job_orchestrator.py">
"""
Sub-Job Orchestrator Service.

This module orchestrates parallel video generation from image pairs.
It manages the full workflow:
1. Create sub-jobs for each image pair
2. Launch ALL Replicate predictions in parallel (no limit)
3. Poll all predictions concurrently
4. Download completed videos
5. Combine all clips into final video

Designed for maximum parallelism - all sub-jobs run simultaneously.
"""

import logging
import asyncio
import tempfile
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
import requests

from ..config import get_settings
from ..database import (
    create_sub_job,
    update_sub_job_status,
    get_sub_jobs_by_job,
    get_sub_job_progress_summary,
    update_video_status,
    increment_sub_job_retry_count,
)
from .replicate_client import ReplicateClient
from .video_combiner import combine_video_clips, store_clip_and_combined, add_audio_to_video
from .musicgen_client import MusicGenClient
from .scene_prompts import get_all_scenes

logger = logging.getLogger(__name__)
settings = get_settings()


class SubJobOrchestratorError(Exception):
    """Exception raised when sub-job orchestration fails."""
    pass


def _round_duration_for_veo3(duration: Optional[float]) -> int:
    """
    Round duration to valid Veo3 value (4, 6, or 8 seconds).

    Args:
        duration: Requested duration in seconds

    Returns:
        Valid duration (4, 6, or 8)
    """
    requested = float(duration) if duration else 8.0

    if requested <= 5.0:
        return 4
    elif requested <= 7.0:
        return 6
    else:
        return 8


async def process_image_pairs_to_videos(
    job_id: int,
    image_pairs: List[Tuple[str, str, float, str]],
    clip_duration: Optional[float] = None,
) -> Dict[str, Any]:
    """
    Process all image pairs into videos in parallel and combine them.

    This is the main entry point for the sub-job orchestration workflow.

    Args:
        job_id: Parent job ID
        image_pairs: List of (image1_id, image2_id, score, reasoning) tuples
        clip_duration: Optional duration for each clip in seconds

    Returns:
        Dict with status and results:
            {
                "success": bool,
                "total_clips": int,
                "successful_clips": int,
                "failed_clips": int,
                "combined_video_url": str or None,
                "clip_urls": List[str],
                "total_cost": float,
                "error": str or None
            }
    """
    logger.info(
        f"Starting sub-job orchestration for job {job_id} with {len(image_pairs)} pairs"
    )

    # Update main job status
    update_video_status(job_id, "sub_job_processing")

    # Determine video generation model from config
    video_model = settings.VIDEO_GENERATION_MODEL

    try:
        # Round duration to valid Veo3 value
        rounded_duration = _round_duration_for_veo3(clip_duration)
        if clip_duration and clip_duration != rounded_duration:
            logger.info(f"Rounded clip duration from {clip_duration}s to {rounded_duration}s for Veo3 compatibility")

        # Step 1: Create all sub-jobs in database with rounded duration
        # Import scene prompts
        from ..services.scene_prompts import get_scene_prompt

        sub_job_ids = []
        for i, (image1_id, image2_id, score, reasoning) in enumerate(image_pairs, 1):
            # Get scene-specific prompt (scenes are numbered 1-7, repeating if needed)
            scene_info = get_scene_prompt(i if i <= 7 else ((i - 1) % 7) + 1)

            sub_job_id = create_sub_job(
                job_id=job_id,
                sub_job_number=i,
                image1_asset_id=image1_id,
                image2_asset_id=image2_id,
                model_id=video_model,
                input_parameters={
                    "duration": rounded_duration or scene_info["duration"],
                    "score": score,
                    "reasoning": reasoning,
                    "prompt": scene_info["prompt"],  # Scene-specific cinematography prompt
                    "scene_number": scene_info["scene_number"],
                    "scene_name": scene_info["name"],
                    "motion_goal": scene_info["motion_goal"],
                },
            )
            sub_job_ids.append(sub_job_id)

        logger.info(f"Created {len(sub_job_ids)} sub-jobs for job {job_id}")

        # Step 2: Launch ALL video generations in parallel with rounded duration
        results = await _launch_all_sub_jobs(job_id, sub_job_ids, rounded_duration)

        # Step 3: Analyze results
        successful_clips = [r for r in results if r["success"]]
        failed_clips = [r for r in results if not r["success"]]

        logger.info(
            f"Sub-job results: {len(successful_clips)} succeeded, {len(failed_clips)} failed"
        )

        if not successful_clips:
            update_video_status(job_id, "failed")
            return {
                "success": False,
                "total_clips": len(image_pairs),
                "successful_clips": 0,
                "failed_clips": len(failed_clips),
                "combined_video_url": None,
                "clip_urls": [],
                "total_cost": 0.0,
                "error": "All sub-jobs failed",
            }

        # Step 4: Combine successful clips
        update_video_status(job_id, "video_combining")

        clip_paths = [r["clip_path"] for r in successful_clips]
        clip_urls, combined_video_path, total_cost = await _combine_clips(
            job_id, clip_paths, successful_clips
        )

        # Step 4.5: Generate progressive audio and merge with video
        update_video_status(job_id, "audio_generation")

        combined_url = await _add_music_to_video(
            job_id, combined_video_path, len(successful_clips)
        )

        # Step 5: Update main job with results
        update_video_status(
            job_id,
            "completed",
            video_url=combined_url,
            metadata={
                "total_clips": len(image_pairs),
                "successful_clips": len(successful_clips),
                "failed_clips": len(failed_clips),
                "clip_urls": clip_urls,
                "total_cost": total_cost,
            },
        )

        logger.info(
            f"Job {job_id} completed successfully. Combined {len(successful_clips)} clips."
        )

        return {
            "success": True,
            "total_clips": len(image_pairs),
            "successful_clips": len(successful_clips),
            "failed_clips": len(failed_clips),
            "combined_video_url": combined_url,
            "clip_urls": clip_urls,
            "total_cost": total_cost,
            "error": None,
        }

    except Exception as e:
        logger.error(f"Sub-job orchestration failed for job {job_id}: {e}", exc_info=True)
        update_video_status(job_id, "failed", metadata={"error": str(e)})
        raise SubJobOrchestratorError(f"Orchestration failed: {e}")


async def _launch_all_sub_jobs(
    job_id: int, sub_job_ids: List[str], clip_duration: Optional[float]
) -> List[Dict[str, Any]]:
    """
    Launch ALL sub-jobs in parallel (no concurrency limit).

    Args:
        job_id: Parent job ID
        sub_job_ids: List of sub-job IDs to process
        clip_duration: Optional clip duration

    Returns:
        List of result dicts for each sub-job
    """
    logger.info(f"Launching {len(sub_job_ids)} sub-jobs in FULL PARALLEL mode")

    # Create tasks for all sub-jobs
    tasks = [
        _process_single_sub_job(job_id, sub_job_id, clip_duration)
        for sub_job_id in sub_job_ids
    ]

    # Run all tasks concurrently (asyncio.gather runs them in parallel)
    results = await asyncio.gather(*tasks, return_exceptions=True)

    # Handle any exceptions
    processed_results = []
    for i, result in enumerate(results):
        if isinstance(result, Exception):
            logger.error(f"Sub-job {sub_job_ids[i]} raised exception: {result}")
            processed_results.append({
                "success": False,
                "sub_job_id": sub_job_ids[i],
                "error": str(result),
                "clip_path": None,
                "cost": 0.0,
            })
        else:
            processed_results.append(result)

    return processed_results


async def _process_single_sub_job(
    job_id: int, sub_job_id: str, clip_duration: Optional[float]
) -> Dict[str, Any]:
    """
    Process a single sub-job: get asset URLs, generate video, download, update status.

    Args:
        job_id: Parent job ID
        sub_job_id: Sub-job ID to process
        clip_duration: Optional clip duration

    Returns:
        Dict with success, clip_path, cost, etc.
    """
    from ..database import get_sub_job_by_id
    from ..database_helpers import get_asset_by_id

    try:
        # Get sub-job details
        sub_job = get_sub_job_by_id(sub_job_id)
        if not sub_job:
            raise ValueError(f"Sub-job {sub_job_id} not found")

        logger.info(
            f"Processing sub-job {sub_job_id} ({sub_job['subJobNumber']}) for job {job_id}"
        )

        # Get asset URLs
        image1 = get_asset_by_id(sub_job["image1AssetId"])
        image2 = get_asset_by_id(sub_job["image2AssetId"])

        if not image1 or not image2:
            raise ValueError(f"Assets not found for sub-job {sub_job_id}")

        # Construct full asset URLs - use NGROK_URL for external services like Replicate
        current_settings = get_settings()
        # Use NGROK_URL if available, otherwise fall back to BASE_URL
        external_url = current_settings.NGROK_URL or current_settings.BASE_URL

        # Generate temporary access tokens for assets (valid for 2 hours)
        from ..auth import create_asset_access_token

        image1_token = create_asset_access_token(sub_job["image1AssetId"])
        image2_token = create_asset_access_token(sub_job["image2AssetId"])

        # Prefer source_url (original URL) if available, otherwise construct full URL with token
        if hasattr(image1, 'source_url') and image1.source_url:
            image1_url = image1.source_url
        elif hasattr(image1, 'sourceUrl') and image1.sourceUrl:
            image1_url = image1.sourceUrl
        else:
            # Construct URL with access token for external services
            base_url = f"{external_url}{image1.url}" if not image1.url.startswith('http') else image1.url
            image1_url = f"{base_url}?token={image1_token}"

        if hasattr(image2, 'source_url') and image2.source_url:
            image2_url = image2.source_url
        elif hasattr(image2, 'sourceUrl') and image2.sourceUrl:
            image2_url = image2.sourceUrl
        else:
            # Construct URL with access token for external services
            base_url = f"{external_url}{image2.url}" if not image2.url.startswith('http') else image2.url
            image2_url = f"{base_url}?token={image2_token}"

        # Debug: Log URLs being sent to Replicate
        logger.error(f"[DEBUG ORCHESTRATOR] NGROK_URL: {current_settings.NGROK_URL}")
        logger.error(f"[DEBUG ORCHESTRATOR] BASE_URL: {current_settings.BASE_URL}")
        logger.error(f"[DEBUG ORCHESTRATOR] Using external_url: {external_url}")
        logger.error(f"[DEBUG ORCHESTRATOR] Image1 URL: {image1_url}")
        logger.error(f"[DEBUG ORCHESTRATOR] Image2 URL: {image2_url}")
        logger.error(f"[DEBUG ORCHESTRATOR] Model: {sub_job['modelId']}, Duration: {clip_duration}")

        # Update status to processing
        update_sub_job_status(sub_job_id, "processing")

        # Generate video using Replicate
        # Check if there's a scene-specific prompt in inputParameters
        input_params = sub_job.get("inputParameters", {})
        scene_prompt = input_params.get("prompt") or input_params.get("scene_prompt")

        replicate_client = ReplicateClient()
        result = await asyncio.to_thread(
            replicate_client.generate_video_from_pair,
            image1_url,
            image2_url,
            model=sub_job["modelId"],
            duration=clip_duration,
            prompt=scene_prompt,  # Pass scene-specific prompt if available
        )

        if not result["success"]:
            # Mark as failed
            update_sub_job_status(
                sub_job_id,
                "failed",
                error_message=result.get("error", "Unknown error"),
            )
            return {
                "success": False,
                "sub_job_id": sub_job_id,
                "error": result.get("error"),
                "clip_path": None,
                "cost": 0.0,
            }

        # Download the generated video
        video_url = result["video_url"]
        clip_path = await _download_video(job_id, sub_job["subJobNumber"], video_url)

        # Calculate cost (estimate based on duration)
        duration = result.get("duration_seconds", clip_duration or 6)
        if sub_job["modelId"] == "veo3":
            cost = duration * ReplicateClient.VEO3_PRICE_PER_SECOND
        else:  # hailuo-2.0
            cost = ReplicateClient.HAILUO2_PRICE_PER_GENERATION

        # Update sub-job as completed
        update_sub_job_status(
            sub_job_id,
            "completed",
            replicate_prediction_id=result.get("prediction_id"),
            video_url=video_url,
            duration_seconds=duration,
            actual_cost=cost,
            progress=1.0,
        )

        logger.info(f"Sub-job {sub_job_id} completed successfully")

        return {
            "success": True,
            "sub_job_id": sub_job_id,
            "clip_path": clip_path,
            "video_url": video_url,
            "cost": cost,
            "error": None,
        }

    except Exception as e:
        logger.error(f"Error processing sub-job {sub_job_id}: {e}", exc_info=True)
        update_sub_job_status(sub_job_id, "failed", error_message=str(e))
        return {
            "success": False,
            "sub_job_id": sub_job_id,
            "error": str(e),
            "clip_path": None,
            "cost": 0.0,
        }


async def _download_video(job_id: int, clip_number: int, video_url: str) -> str:
    """
    Download a video from URL to temp file.

    Args:
        job_id: Job ID for naming
        clip_number: Clip number for naming
        video_url: URL to download from

    Returns:
        Path to downloaded file
    """
    # Create temp file
    temp_dir = Path(tempfile.gettempdir()) / f"job_{job_id}"
    temp_dir.mkdir(exist_ok=True)

    temp_path = temp_dir / f"clip_{clip_number:03d}.mp4"

    logger.info(f"Downloading video from {video_url} to {temp_path}")

    # Download with streaming
    response = await asyncio.to_thread(
        requests.get, video_url, stream=True, timeout=300
    )
    response.raise_for_status()

    # Write to file
    with open(temp_path, 'wb') as f:
        for chunk in response.iter_content(chunk_size=8192):
            f.write(chunk)

    logger.info(f"Downloaded video to {temp_path} ({temp_path.stat().st_size} bytes)")

    return str(temp_path)


async def _add_music_to_video(
    job_id: int, combined_video_path: str, num_scenes: int
) -> str:
    """
    Generate progressive audio and merge with combined video.

    Args:
        job_id: Job ID
        combined_video_path: Path to combined video (without audio)
        num_scenes: Number of scenes/clips in the video

    Returns:
        URL to final video with audio
    """
    logger.info(f"Generating progressive audio for {num_scenes} scenes")

    try:
        # Get all scene templates (use first 7 or repeat if more)
        all_scenes = get_all_scenes()
        scene_prompts = []

        for i in range(num_scenes):
            # Use scenes 1-7, repeating if we have more than 7 clips
            scene_index = i % len(all_scenes)
            scene_prompts.append(all_scenes[scene_index])

        # Initialize MusicGen client
        musicgen_client = MusicGenClient()

        # Generate progressive audio across all scenes
        result = await asyncio.to_thread(
            musicgen_client.generate_progressive_audio,
            scene_prompts,
            duration_per_scene=4,  # 4 seconds per scene
        )

        if not result["success"]:
            logger.error(f"Music generation failed: {result['error']}")
            # Fall back to video without music
            logger.warning("Proceeding without background music")
            return await _store_final_video(job_id, combined_video_path)

        # Download the audio file
        audio_url = result["final_audio_url"]
        temp_dir = Path(tempfile.gettempdir()) / f"job_{job_id}"
        audio_path = temp_dir / "background_music.mp3"

        logger.info(f"Downloading audio from {audio_url}")

        response = await asyncio.to_thread(
            requests.get, audio_url, stream=True, timeout=300
        )
        response.raise_for_status()

        with open(audio_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)

        logger.info(f"Audio downloaded to {audio_path}")

        # Merge audio with video
        final_video_path = temp_dir / "final_with_audio.mp4"

        success, output_path, error = await asyncio.to_thread(
            add_audio_to_video,
            combined_video_path,
            str(audio_path),
            str(final_video_path),
            audio_fade_duration=0.5,
        )

        if not success:
            logger.error(f"Failed to merge audio with video: {error}")
            # Fall back to video without music
            logger.warning("Proceeding without background music")
            return await _store_final_video(job_id, combined_video_path)

        logger.info(f"Successfully merged audio with video: {output_path}")

        # Store the final video with audio
        return await _store_final_video(job_id, output_path)

    except Exception as e:
        logger.error(f"Error adding music to video: {e}", exc_info=True)
        # Fall back to video without music
        logger.warning("Proceeding without background music due to error")
        return await _store_final_video(job_id, combined_video_path)


async def _store_final_video(job_id: int, video_path: str) -> str:
    """
    Store the final combined video in database as blob and return its URL.

    Args:
        job_id: Job ID
        video_path: Path to final video file

    Returns:
        URL to the stored video
    """
    from ..database import get_db

    # Read the video file into memory
    with open(video_path, 'rb') as f:
        video_data = f.read()

    # Store video blob in database
    with get_db() as conn:
        conn.execute(
            """
            UPDATE generated_videos
            SET video_data = ?
            WHERE id = ?
            """,
            (video_data, job_id)
        )
        conn.commit()

    combined_url = f"/api/v3/videos/{job_id}/combined"

    logger.info(f"Stored final video in database (job_id={job_id}, size={len(video_data)} bytes)")

    return combined_url


async def _combine_clips(
    job_id: int, clip_paths: List[str], clip_results: List[Dict[str, Any]]
) -> Tuple[List[str], str, float]:
    """
    Combine all clips into final video.

    Args:
        job_id: Job ID
        clip_paths: Paths to individual clips
        clip_results: Results from sub-job processing (for cost calculation)

    Returns:
        Tuple of (clip_urls, combined_temp_path, total_cost)
    """
    logger.info(f"Combining {len(clip_paths)} clips for job {job_id}")

    # Create temp output path
    temp_dir = Path(tempfile.gettempdir()) / f"job_{job_id}"
    combined_temp_path = temp_dir / "combined.mp4"

    # Combine videos
    success, output_path, metadata = await asyncio.to_thread(
        combine_video_clips,
        clip_paths,
        str(combined_temp_path),
        transition_duration=0.0,  # No transitions for now
        output_resolution="1920x1080",
        output_fps=30,
        keep_audio=False,
    )

    if not success:
        raise SubJobOrchestratorError("Failed to combine video clips")

    # Store individual clips only (combined video will be stored after audio is added)
    clip_urls = []
    clips_dir = Path(settings.VIDEO_STORAGE_PATH) / str(job_id) / "clips"
    clips_dir.mkdir(parents=True, exist_ok=True)

    for i, clip_path in enumerate(clip_paths, 1):
        clip_filename = f"clip_{i:03d}.mp4"
        dest_path = clips_dir / clip_filename

        # Copy the file
        import shutil
        await asyncio.to_thread(shutil.copy2, clip_path, dest_path)

        # Generate URL
        clip_url = f"/api/v3/videos/{job_id}/clips/{clip_filename}"
        clip_urls.append(clip_url)

    # Calculate total cost
    total_cost = sum(r.get("cost", 0.0) for r in clip_results)

    logger.info(
        f"Combined video created at {combined_temp_path}, total cost: ${total_cost:.2f}"
    )

    return clip_urls, str(combined_temp_path), total_cost
</file>

<file path="backend/api/v3/models.py">
"""
Pydantic models for v3 API endpoints.

These models mirror the frontend TypeScript interfaces and provide
strict validation and serialization for the v3 API responses.
"""

from datetime import datetime
from typing import Optional, Dict, Any, List, Union, Literal
from pydantic import BaseModel, Field
from enum import Enum


# ============================================================================
# API Response Envelope
# ============================================================================


class APIResponse(BaseModel):
    """Standard API response envelope matching lib/types/api.ts"""

    data: Optional[Any] = None
    error: Optional[str] = None
    meta: Optional[Dict[str, Any]] = None

    @classmethod
    def success(
        cls, data: Any = None, meta: Optional[Dict[str, Any]] = None
    ) -> "APIResponse":
        """Create a successful response"""
        return cls(data=data, error=None, meta=meta)

    @classmethod
    def create_error(
        cls, error_msg: str, meta: Optional[Dict[str, Any]] = None
    ) -> "APIResponse":
        """Create an error response"""
        return cls(data=None, error=error_msg, meta=meta)


# ============================================================================
# Client Models
# ============================================================================


class BrandGuidelines(BaseModel):
    """Brand guidelines object within client"""

    colors: Optional[List[str]] = None
    fonts: Optional[List[str]] = None
    tone: Optional[str] = None
    restrictions: Optional[List[str]] = None
    examples: Optional[List[str]] = None


class Client(BaseModel):
    """Client model matching lib/types/client.ts"""

    id: str
    name: str
    description: Optional[str] = None
    homepage: Optional[str] = None
    brandGuidelines: Optional[BrandGuidelines] = None
    metadata: Optional[Dict[str, Any]] = None
    createdAt: str
    updatedAt: str


class ClientCreateRequest(BaseModel):
    """Request model for creating a client"""

    name: str
    description: Optional[str] = None
    homepage: Optional[str] = None
    brandGuidelines: Optional[BrandGuidelines] = None
    metadata: Optional[Dict[str, Any]] = None


class ClientUpdateRequest(BaseModel):
    """Request model for updating a client"""

    name: Optional[str] = None
    description: Optional[str] = None
    homepage: Optional[str] = None
    brandGuidelines: Optional[BrandGuidelines] = None
    metadata: Optional[Dict[str, Any]] = None


# ============================================================================
# Campaign Models
# ============================================================================


class Campaign(BaseModel):
    """Campaign model matching lib/types/campaign.ts"""

    id: str
    clientId: str
    name: str
    goal: str
    status: str
    productUrl: Optional[str] = None
    brief: Optional[Dict[str, Any]] = None
    metadata: Optional[Dict[str, Any]] = None
    createdAt: str
    updatedAt: str


class CampaignCreateRequest(BaseModel):
    """Request model for creating a campaign"""

    clientId: str
    name: str
    goal: str
    status: str = "draft"
    productUrl: Optional[str] = None
    brief: Optional[Dict[str, Any]] = None
    metadata: Optional[Dict[str, Any]] = None


class CampaignUpdateRequest(BaseModel):
    """Request model for updating a campaign"""

    name: Optional[str] = None
    goal: Optional[str] = None
    status: Optional[str] = None
    productUrl: Optional[str] = None
    brief: Optional[Dict[str, Any]] = None
    metadata: Optional[Dict[str, Any]] = None


# ============================================================================
# Job Models (Generation Workflow)
# ============================================================================


class JobStatus(str, Enum):
    """Job status enum matching frontend expectations"""

    PENDING = "pending"
    STORYBOARD_PROCESSING = "storyboard_processing"
    STORYBOARD_READY = "storyboard_ready"
    VIDEO_PROCESSING = "video_processing"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"


class JobContext(BaseModel):
    """Context object for job creation"""

    clientId: str
    campaignId: Optional[str] = None
    userId: Optional[str] = None  # Made optional - can be derived from auth token


class AdBasics(BaseModel):
    """Ad basics object for job creation"""

    product: str
    targetAudience: str
    keyMessage: str
    callToAction: str


class CreativeDirection(BaseModel):
    """Creative direction within creative object"""

    style: str
    tone: Optional[str] = None  # Made optional for flexibility
    visualElements: Optional[List[str]] = None  # Made optional for flexibility
    musicStyle: Optional[str] = None


class VideoSpecs(BaseModel):
    """Video specifications"""

    duration: float  # Duration in seconds
    format: str = "16:9"  # Aspect ratio
    resolution: Optional[str] = None  # e.g., "1080p"


class AssetInput(BaseModel):
    """Asset input for job creation - can be URL or existing asset ID"""

    url: Optional[str] = None  # URL to download asset from
    assetId: Optional[str] = None  # Existing asset ID
    type: Optional[str] = None  # Asset type (image, video, audio)
    name: Optional[str] = None  # Asset name/description
    role: Optional[str] = (
        None  # Optional hint for scene placement (e.g., "product_shot", "background")
    )


class Creative(BaseModel):
    """Creative object for job creation"""

    videoSpecs: VideoSpecs  # Video specifications
    direction: CreativeDirection
    assets: Optional[List[AssetInput]] = None  # Assets to use in generation
    storyboard: Optional[Dict[str, Any]] = None


class AdvancedSettings(BaseModel):
    """Advanced settings object for job creation"""

    duration: Optional[int] = None
    resolution: Optional[str] = None
    modelPreferences: Optional[List[str]] = None
    autoApprove: bool = Field(
        default=False, description="Auto-approve storyboard and start video rendering"
    )


class JobCreateRequest(BaseModel):
    """Request model for creating a job"""

    context: JobContext
    adBasics: AdBasics
    creative: Creative
    advanced: Optional[AdvancedSettings] = None
    generateAudio: bool = Field(
        default=False, description="Enable audio generation for the job"
    )


class Job(BaseModel):
    """Job model for polling and status"""

    id: str
    status: JobStatus
    progress: Optional[Dict[str, Any]] = None
    storyboard: Optional[Dict[str, Any]] = None
    videoUrl: Optional[str] = None
    error: Optional[str] = None
    estimatedCost: Optional[float] = None
    actualCost: Optional[float] = None
    createdAt: str
    updatedAt: str


class JobAction(str, Enum):
    """Job action enum"""

    APPROVE = "approve"
    CANCEL = "cancel"
    REGENERATE_SCENE = "regenerate_scene"


class JobActionRequest(BaseModel):
    """Request model for job actions"""

    action: JobAction
    payload: Optional[Dict[str, Any]] = None


# ============================================================================
# Cost Estimation Models
# ============================================================================


class CostEstimate(BaseModel):
    """Cost estimate response"""

    estimatedCost: float
    currency: str = "USD"
    breakdown: Optional[Dict[str, Any]] = None
    validUntil: Optional[str] = None


class DryRunRequest(BaseModel):
    """Request model for cost estimation (dry run)"""

    context: JobContext
    adBasics: AdBasics
    creative: Creative
    advanced: Optional[AdvancedSettings] = None


# ============================================================================
# Asset Models (Reusing existing schemas)
# ============================================================================

# Import existing asset models
from ...schemas.assets import Asset, UploadAssetInput, UploadAssetFromUrlInput


# ============================================================================
# Unified Asset Upload Models
# ============================================================================


class UnifiedAssetUploadInput(BaseModel):
    """Unified input model for asset upload (supports both file upload and URL)"""

    uploadType: Literal[
        "file", "url"
    ]  # Type of upload: "file" for direct upload, "url" for URL download
    name: str
    type: str  # Asset type: "image", "video", "audio", "document"
    clientId: Optional[str] = None
    campaignId: Optional[str] = None
    tags: Optional[List[str]] = None
    generateThumbnail: bool = (
        True  # Whether to auto-generate thumbnail for images/videos
    )

    # For URL uploads
    sourceUrl: Optional[str] = (
        None  # URL to download asset from (required when uploadType="url")
    )

    # Note: File data is handled separately via FastAPI's UploadFile when uploadType="file"


# Note: Asset models are extended in schemas/assets.py to include thumbnailBlobId and sourceUrl


# ============================================================================
# Audio Generation Models (Scene-based Music Generation)
# ============================================================================


class ScenePrompt(BaseModel):
    """Individual scene prompt for audio generation"""

    scene_number: int
    prompt: str
    duration: Optional[float] = None  # Override default duration

    model_config = {
        "json_schema_extra": {
            "example": {
                "scene_number": 1,
                "prompt": "Cinematic wide shot, low angle. Clear water or reflective surface gently rippling. Subtle, smooth camera push-in (dolly forward). Bright natural lighting with glistening highlights on the water/surface.",
                "duration": 4.0,
            }
        }
    }


class SceneAudioRequest(BaseModel):
    """Request model for generating audio from scene prompts"""

    scenes: list[ScenePrompt]
    default_duration: float = 4.0  # Default seconds per scene
    model_id: str = "meta/musicgen"

    model_config = {
        "json_schema_extra": {
            "example": {
                "scenes": [
                    {
                        "scene_number": 1,
                        "prompt": "Cinematic wide shot, low angle. Clear water or reflective surface gently rippling. Subtle, smooth camera push-in (dolly forward). Bright natural lighting with glistening highlights on the water/surface.",
                        "duration": 4.0,
                    },
                    {
                        "scene_number": 2,
                        "prompt": "Smooth sideways camera truck (left or right – choose direction that creates natural parallax). Luxurious bedroom with large windows or glass walls. Parallax effect: bed and foreground elements move slightly faster than the background view.",
                        "duration": 4.0,
                    },
                ],
                "default_duration": 4.0,
                "model_id": "meta/musicgen",
            }
        }
    }


class SceneAudioResponse(BaseModel):
    """Response model for scene audio generation"""

    audio_id: int
    audio_url: str
    total_duration: float
    scenes_processed: int
    model_used: str

    model_config = {
        "json_schema_extra": {
            "example": {
                "audio_id": 123,
                "audio_url": "/api/audio/123/data",
                "total_duration": 28.0,
                "scenes_processed": 7,
                "model_used": "meta/musicgen",
            }
        }
    }


# ============================================================================
# Image Pair Selection & Video Generation Models (New Feature)
# ============================================================================


class ImagePairJobCreateRequest(BaseModel):
    """Request model for creating a job from image pairs."""

    campaignId: str
    clientId: Optional[str] = None
    clipDuration: Optional[float] = None  # Duration for each clip in seconds
    numPairs: Optional[int] = None  # Optional target number of pairs to select


class SubJobStatus(str, Enum):
    """Sub-job status enum."""

    PENDING = "pending"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"


class SubJob(BaseModel):
    """Sub-job model for individual image pair to video conversion."""

    id: str
    jobId: int
    subJobNumber: int
    image1AssetId: str
    image2AssetId: str
    replicatePredictionId: Optional[str] = None
    modelId: str
    inputParameters: Optional[Dict[str, Any]] = None
    status: SubJobStatus
    progress: float = 0.0
    videoUrl: Optional[str] = None
    videoBlobId: Optional[str] = None
    durationSeconds: Optional[float] = None
    estimatedCost: Optional[float] = None
    actualCost: Optional[float] = None
    errorMessage: Optional[str] = None
    retryCount: int = 0
    startedAt: Optional[str] = None
    completedAt: Optional[str] = None
    createdAt: str
    updatedAt: str


class SubJobSummary(BaseModel):
    """Summary of sub-job progress."""

    total: int
    pending: int
    processing: int
    completed: int
    failed: int


# ============================================================================
# Property Video Generation Models
# ============================================================================


class PropertyPhoto(BaseModel):
    """Photo from crawled property website with metadata."""

    id: str = Field(..., description="Unique photo identifier")
    filename: Optional[str] = Field(None, description="Photo filename")
    url: str = Field(..., description="URL to photo")
    tags: Optional[List[str]] = Field(None, description="Photo tags")
    dominantColors: Optional[List[str]] = Field(None, description="Dominant colors")
    detectedObjects: Optional[List[str]] = Field(None, description="Detected objects")
    composition: Optional[str] = Field(None, description="Composition style")
    lighting: Optional[str] = Field(None, description="Lighting conditions")
    resolution: Optional[str] = Field(None, description="Image resolution")
    aspectRatio: Optional[str] = Field(None, description="Aspect ratio")


class PropertyInfo(BaseModel):
    """Information about luxury lodging property."""

    name: str = Field(..., description="Property name")
    location: str = Field(..., description="Property location")
    propertyType: str = Field(
        ..., description="Type of property (e.g., boutique hotel, resort)"
    )
    positioning: str = Field(
        ..., description="Brand positioning (e.g., eco-luxury, modern minimalist)"
    )


class PropertyVideoRequest(BaseModel):
    """Request to generate video from property photos."""

    propertyInfo: PropertyInfo = Field(..., description="Property information")
    photos: List[PropertyPhoto] = Field(
        ..., description="List of property photos", min_items=14
    )
    campaignId: str = Field(..., description="Campaign ID for this property")
    clipDuration: Optional[float] = Field(
        6.0, description="Duration per scene in seconds"
    )
    videoModel: Optional[str] = Field(
        "veo3", description="Video generation model (veo3 or hailuo-2.0)"
    )


class SceneImagePair(BaseModel):
    """Image pair selection for a scene."""

    sceneNumber: int
    sceneType: str
    firstImage: Dict[str, Any]
    lastImage: Dict[str, Any]
    transitionAnalysis: Dict[str, Any]


class PropertySelectionResult(BaseModel):
    """Result of Grok's property photo selection."""

    propertyName: str
    selectionMetadata: Dict[str, Any]
    scenePairs: List[SceneImagePair]
    recommendations: Optional[Dict[str, Any]] = None


class PropertyVideoJobResponse(BaseModel):
    """Response after creating property video job."""

    jobId: int
    status: str
    propertyName: str
    totalScenes: int
    selectionMetadata: Dict[str, Any]
    scenePairs: List[SceneImagePair]
</file>

<file path="backend/services/xai_client.py">
"""
xAI Grok-4-fast-1 client for AI-powered image pair selection.

This service uses Grok's vision capabilities to intelligently select
and order image pairs from campaign assets for video generation.
"""

import json
import logging
import time
from typing import List, Dict, Any, Optional, Tuple
import requests

from ..config import get_settings

logger = logging.getLogger(__name__)
settings = get_settings()


class XAIClient:
    """Client for interacting with xAI's Grok API"""

    def __init__(self, api_key: Optional[str] = None):
        """
        Initialize the xAI client.

        Args:
            api_key: Optional API key. If not provided, will use XAI_API_KEY from settings.
        """
        self.api_key = api_key or settings.XAI_API_KEY
        if not self.api_key:
            raise ValueError("XAI_API_KEY must be set in environment or passed to XAIClient")

        self.base_url = "https://api.x.ai/v1"
        self.model = "grok-4-1-fast-non-reasoning"

    def _group_assets_by_room(self, assets: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:
        """
        Group assets by room instance extracted from tags.

        For tags like ["Bedroom 1 2"], extracts "Bedroom 1" as the room instance.
        For tags like ["Ensuite Bathroom 1 3"], extracts "Ensuite Bathroom 1".
        For tags like ["Kitchen 5"], extracts "Kitchen".

        Removes the last number (photo index within room) to get room instance.
        """
        from collections import defaultdict
        import json

        room_groups = defaultdict(list)

        for asset in assets:
            tags = asset.get('tags', [])

            # Handle tags stored as JSON string
            if isinstance(tags, str):
                try:
                    tags = json.loads(tags)
                except:
                    tags = []

            if tags and len(tags) > 0:
                tag = tags[0]  # Use first tag

                # Extract room instance by removing last number
                # "Bedroom 1 2" → "Bedroom 1"
                # "Kitchen 5" → "Kitchen"
                # "Ensuite Bathroom 1 3" → "Ensuite Bathroom 1"
                parts = tag.split()
                if len(parts) >= 1:
                    # Remove the last part if it's a number (photo index)
                    if len(parts) > 1 and parts[-1].isdigit():
                        room_instance = ' '.join(parts[:-1])
                    else:
                        room_instance = tag  # Keep as-is if no trailing number

                    room_groups[room_instance].append(asset)

                    logger.debug(f"[ROOM GROUPING] Tag '{tag}' → Room instance '{room_instance}'")

        logger.info(f"[ROOM GROUPING] Created {len(room_groups)} room instance groups")
        for room_instance, group_assets in sorted(room_groups.items(), key=lambda x: len(x[1]), reverse=True)[:10]:
            logger.info(f"[ROOM GROUPING]   {room_instance}: {len(group_assets)} images")

        return dict(room_groups)

    def select_image_pairs(
        self,
        assets: List[Dict[str, Any]],
        campaign_context: Optional[Dict[str, Any]] = None,
        client_brand_guidelines: Optional[Dict[str, Any]] = None,
        num_pairs: Optional[int] = None,
    ) -> List[Tuple[str, str, float, str]]:
        """
        Two-stage selection process for luxury property videos:

        Stage 1: Select which room types to feature (e.g., best 7 rooms)
        Stage 2: For each selected room, pick best 2 images to form a pair

        Args:
            assets: List of asset dicts with keys: id, name, description, tags, url, type
            campaign_context: Optional campaign info (goal, targetAudience, keyMessage, etc.)
            client_brand_guidelines: Optional brand guidelines (colors, tone, restrictions)
            num_pairs: Optional target number of pairs. If not specified, Grok decides.

        Returns:
            List of tuples: (image1_id, image2_id, score, reasoning)
            Ordered by quality score (highest first)
        """
        import concurrent.futures

        if not assets or len(assets) < 2:
            raise ValueError("Need at least 2 assets to create pairs")

        # Filter to only image assets
        image_assets = [a for a in assets if a.get("type") == "image"]
        if len(image_assets) < 2:
            raise ValueError(f"Need at least 2 image assets, but only got {len(image_assets)}")

        logger.info(
            f"[TWO-STAGE] Selecting image pairs from {len(image_assets)} images "
            f"(filtered from {len(assets)} total assets)"
        )

        # Detect if we should use two-stage room-based selection
        # Use two-stage if: num_pairs == 7 (luxury property video) and we have room-tagged images
        num_pairs = num_pairs or 7  # Default to 7
        use_two_stage = num_pairs == 7

        if not use_two_stage:
            # Fall back to original single-stage selection
            logger.info("[TWO-STAGE] Using single-stage selection (not a 7-pair property video)")
            return self._single_stage_selection(image_assets, campaign_context, client_brand_guidelines, num_pairs)

        # STAGE 1: Group assets by room type
        room_groups = self._group_assets_by_room(image_assets)

        # Adjust num_pairs if we have fewer room types
        actual_pairs = min(num_pairs, len(room_groups))
        if len(room_groups) < num_pairs:
            logger.info(
                f"[TWO-STAGE] Only {len(room_groups)} room types available, "
                f"adjusting from {num_pairs} to {actual_pairs} pairs."
            )

        logger.info(f"[TWO-STAGE STAGE 1] Grouped {len(image_assets)} images into {len(room_groups)} room types")
        for room_type, room_assets in room_groups.items():
            logger.info(f"[TWO-STAGE STAGE 1]   {room_type}: {len(room_assets)} images")

        # STAGE 1: Select which room types to use
        try:
            selected_room_types = self._select_room_types(room_groups, actual_pairs, campaign_context)
            logger.info(f"[TWO-STAGE STAGE 1] Selected {len(selected_room_types)} room types: {selected_room_types}")
        except Exception as e:
            logger.error(f"[TWO-STAGE STAGE 1] Room selection failed: {e}. Falling back to single-stage.")
            return self._single_stage_selection(image_assets, campaign_context, client_brand_guidelines, num_pairs)

        # STAGE 2: For each selected room, select best 2 images (parallel)
        pairs = []
        logger.info(f"[TWO-STAGE STAGE 2] Selecting image pairs for {len(selected_room_types)} rooms (parallel)")

        with concurrent.futures.ThreadPoolExecutor(max_workers=7) as executor:
            future_to_room = {
                executor.submit(
                    self._select_pair_from_room,
                    room_type,
                    room_groups[room_type],
                    campaign_context
                ): room_type
                for room_type in selected_room_types
                if room_type in room_groups
            }

            for future in concurrent.futures.as_completed(future_to_room):
                room_type = future_to_room[future]
                try:
                    pair = future.result()
                    if pair:
                        pairs.append(pair)
                        logger.info(f"[TWO-STAGE STAGE 2] ✓ {room_type}: Selected pair")
                except Exception as e:
                    logger.error(f"[TWO-STAGE STAGE 2] ✗ {room_type}: Failed - {e}")

        if len(pairs) < actual_pairs:
            logger.warning(
                f"[TWO-STAGE] Only got {len(pairs)} pairs from {actual_pairs} rooms. "
                f"Some room selections failed."
            )

        logger.info(f"[TWO-STAGE] Successfully selected {len(pairs)} pairs using two-stage approach")
        return pairs

    def _single_stage_selection(
        self,
        image_assets: List[Dict[str, Any]],
        campaign_context: Optional[Dict[str, Any]],
        brand_guidelines: Optional[Dict[str, Any]],
        num_pairs: Optional[int],
    ) -> List[Tuple[str, str, float, str]]:
        """Original single-stage selection (fallback for non-property videos)."""
        prompt = self._build_selection_prompt(
            image_assets, campaign_context, brand_guidelines, num_pairs
        )

        logger.info(f"[SINGLE-STAGE] Prompt length: {len(prompt)} characters")

        response = self._call_grok_api(prompt, image_assets)
        pairs = self._parse_pairs_response(response, image_assets)

        logger.info(f"[SINGLE-STAGE] Successfully selected {len(pairs)} pairs")
        return pairs

    def _select_room_types(
        self,
        room_groups: Dict[str, List[Dict[str, Any]]],
        num_rooms: int,
        campaign_context: Optional[Dict[str, Any]] = None,
    ) -> List[str]:
        """
        Stage 1: Select which room types to feature in the video.

        Args:
            room_groups: Dict mapping room type to list of assets
            num_rooms: Number of room types to select (e.g., 7 for property video)
            campaign_context: Optional campaign info

        Returns:
            List of selected room type names
        """
        # Build room selection prompt
        prompt = f"""You are an expert luxury real estate videographer planning a property showcase video.

You need to select {num_rooms} room types to feature in a {num_rooms}-scene video (one scene per room type).

AVAILABLE ROOM TYPES:
"""

        for room_type, assets in sorted(room_groups.items(), key=lambda x: -len(x[1])):
            prompt += f"- {room_type}: {len(assets)} images available\n"

        if campaign_context:
            prompt += f"""
CAMPAIGN CONTEXT:
- Goal: {campaign_context.get('goal', 'Showcase premium property')}
- Target Audience: {campaign_context.get('targetAudience', 'Luxury buyers')}
"""

        prompt += f"""
SELECTION CRITERIA:
1. Visual Appeal: Choose rooms with most compelling imagery potential
2. Narrative Flow: Select rooms that create a cohesive property tour
3. Diversity: Balance between public spaces (living, dining, kitchen) and private (bedroom, bathroom)
4. Luxury Features: Prioritize rooms that showcase high-end finishes and unique features

REQUIRED ROOM TYPES (must include if available):
- At least 1 bedroom type (Bedroom, etc.)
- At least 1 bathroom type (Bathroom, Ensuite Bathroom, etc.)
- At least 1 exterior/outdoor type (Exterior, Pool, Patio, etc.)

RESPONSE FORMAT (JSON):
{{
  "selected_rooms": ["Room Type 1", "Room Type 2", "Room Type 3", ...]
}}

Select exactly {num_rooms} room types that will create the most compelling property video.
"""

        # Call Grok for room selection with retry logic
        logger.info(f"[ROOM SELECTION] Asking Grok to select {num_rooms} rooms from {len(room_groups)} options")

        data = self._call_grok_api(prompt, image_assets=[], temperature=0.3)
        content = data["choices"][0]["message"]["content"].strip()

        # Parse JSON response
        if "```json" in content:
            content = content.split("```json")[1].split("```")[0].strip()
        elif "```" in content:
            content = content.split("```")[1].split("```")[0].strip()

        result = json.loads(content)
        selected_rooms = result.get("selected_rooms", [])

        # Validate selected rooms exist
        valid_rooms = [r for r in selected_rooms if r in room_groups]

        if len(valid_rooms) < num_rooms:
            logger.warning(
                f"[ROOM SELECTION] Only {len(valid_rooms)} valid rooms selected, "
                f"filling remainder from available rooms"
            )
            # Fill with remaining rooms sorted by image count
            remaining = [r for r in sorted(room_groups.keys(), key=lambda x: -len(room_groups[x]))
                        if r not in valid_rooms]
            valid_rooms.extend(remaining[:num_rooms - len(valid_rooms)])

        return valid_rooms[:num_rooms]

    def _select_pair_from_room(
        self,
        room_type: str,
        room_assets: List[Dict[str, Any]],
        campaign_context: Optional[Dict[str, Any]] = None,
    ) -> Optional[Tuple[str, str, float, str]]:
        """
        Stage 2: Deterministically select first 2 images from a room instance to form a pair.

        Args:
            room_type: Name of the room instance (e.g., "Bedroom 1", "Kitchen")
            room_assets: List of assets for this room instance
            campaign_context: Optional campaign info (unused for deterministic selection)

        Returns:
            Tuple of (image1_id, image2_id, score, reasoning) or None if < 2 images
        """
        if len(room_assets) < 2:
            logger.warning(f"[PAIR SELECTION {room_type}] Only {len(room_assets)} images, need at least 2 for interpolation")
            return None  # Can't create a valid pair without 2 different images

        # Deterministic selection: just grab first 2 images from this room instance
        image1 = room_assets[0]
        image2 = room_assets[1]

        image1_tags = ', '.join(image1.get('tags', []))
        image2_tags = ', '.join(image2.get('tags', []))

        logger.info(
            f"[PAIR SELECTION {room_type}] Deterministic selection: "
            f"[{image1_tags}] → [{image2_tags}]"
        )

        return (
            image1['id'],
            image2['id'],
            0.8,  # Default confidence score
            f"Deterministic selection: first 2 images from {room_type}"
        )

    def _build_selection_prompt(
        self,
        assets: List[Dict[str, Any]],
        campaign_context: Optional[Dict[str, Any]],
        brand_guidelines: Optional[Dict[str, Any]],
        num_pairs: Optional[int],
    ) -> str:
        """Build the prompt for Grok to select image pairs."""

        # Detect if this is a luxury property video (7 pairs requested)
        is_property_video = num_pairs == 7

        if is_property_video:
            return self._build_property_scene_prompt(assets, campaign_context, brand_guidelines)
        else:
            return self._build_generic_selection_prompt(assets, campaign_context, brand_guidelines, num_pairs)

    def _build_generic_selection_prompt(
        self,
        assets: List[Dict[str, Any]],
        campaign_context: Optional[Dict[str, Any]],
        brand_guidelines: Optional[Dict[str, Any]],
        num_pairs: Optional[int],
    ) -> str:
        """Build the generic prompt for image pair selection."""

        prompt = """You are an expert creative director selecting image pairs for video generation.

Your task is to analyze the provided images and select optimal pairs that will create compelling video transitions.

SELECTION CRITERIA:
1. Visual Continuity: Colors, lighting, and composition should flow smoothly
2. Thematic Coherence: Images should tell a cohesive narrative
3. Brand Consistency: Align with brand guidelines if provided
4. Transition Potential: Consider how well images will interpolate into video

"""

        # Add campaign context if provided
        if campaign_context:
            prompt += f"""
CAMPAIGN CONTEXT:
- Goal: {campaign_context.get('goal', 'Not specified')}
- Target Audience: {campaign_context.get('targetAudience', 'Not specified')}
- Key Message: {campaign_context.get('keyMessage', 'Not specified')}
"""

        # Add brand guidelines if provided
        if brand_guidelines:
            colors = brand_guidelines.get('colors') or []
            restrictions = brand_guidelines.get('restrictions') or []
            prompt += f"""
BRAND GUIDELINES:
- Colors: {', '.join(colors) if colors else 'Not specified'}
- Tone: {brand_guidelines.get('tone', 'Not specified')}
- Restrictions: {', '.join(restrictions) if restrictions else 'None'}
"""

        # Add asset information
        prompt += f"""
AVAILABLE IMAGES ({len(assets)} total):
"""
        for i, asset in enumerate(assets, 1):
            tags = ', '.join(asset.get('tags', [])) if asset.get('tags') else 'None'
            prompt += f"""
{i}. ID: {asset['id']}
   Name: {asset.get('name', 'Unnamed')}
   Description: {asset.get('description', 'No description')}
   Tags: {tags}
"""

        # Add instructions
        target_pairs = num_pairs if num_pairs else "an appropriate number of"
        prompt += f"""
INSTRUCTIONS:
Select {target_pairs} image pairs that will create the best video sequence.
Each pair should be ordered (first image → second image) for smooth transitions.
Consider the overall narrative flow across all pairs.

RESPONSE FORMAT (JSON):
{{
  "pairs": [
    {{
      "image1_id": "asset-uuid-1",
      "image2_id": "asset-uuid-2",
      "score": 0.95,
      "reasoning": "Brief explanation of why this pair works well"
    }}
  ]
}}

Important:
- Order pairs by score (highest first)
- Scores should be 0.0 to 1.0 (1.0 = perfect pair)
- Each image can appear in multiple pairs if it makes sense narratively
- Consider the sequence: pairs should flow into each other when combined
"""

        return prompt

    def _build_property_scene_prompt(
        self,
        assets: List[Dict[str, Any]],
        campaign_context: Optional[Dict[str, Any]],
        brand_guidelines: Optional[Dict[str, Any]],
    ) -> str:
        """
        Build specialized prompt for luxury property scene-based pair selection.

        This prompt analyzes photos to select optimal first/last image pairs for
        each of 7 distinct cinematographic scenes in a luxury property video.
        """

        prompt = """You are an expert luxury real estate cinematographer and creative director.

Your task is to analyze a collection of property photos and select the BEST image pairs (first frame + last frame) for generating 7 distinct video scenes. Each scene has specific cinematography requirements and camera movements.

CRITICAL: You must select EXACTLY 7 pairs - one pair per scene type. Each pair consists of:
- First Image: The starting frame that the camera movement begins from
- Last Image: The ending frame that the camera movement ends on

The video generation AI will interpolate between these two images using the specified camera movement.

═══════════════════════════════════════════════════════════════════════════════
SCENE 1: THE HOOK (Exterior Feature) - Duration: 1.5s
═══════════════════════════════════════════════════════════════════════════════
Camera Movement: Dolly forward (push-in) with low angle
Ideal Tags: exterior, pool, courtyard, terrace, water, outdoor, landscape
Motion Goal: Immediate visual interest with water/surface movement

Selection Criteria:
- First Image: Wide shot showing water feature, pool, or reflective surface at a distance
- Last Image: Closer view of the same feature, revealing detail and texture
- Both images should have bright natural lighting with glistening highlights
- Must have clear water or reflective surfaces that suggest gentle movement
- Low angle perspective preferred (eye-level or slightly below)

Visual Requirements:
- Similar lighting conditions between first and last image
- Maintain same time of day (both daytime with similar sun position)
- Progressive reveal of architectural detail as camera moves forward
- Water/surface should be central focus in both frames

═══════════════════════════════════════════════════════════════════════════════
SCENE 2: THE HERO BEDROOM (Parallax) - Duration: 1.0s
═══════════════════════════════════════════════════════════════════════════════
Camera Movement: Lateral truck (sideways slide left or right)
Ideal Tags: bedroom, master bedroom, interior, windows, glass walls, view
Motion Goal: Create parallax effect with foreground/background depth

Selection Criteria:
- First Image: Bedroom with bed visible and windows/view in background
- Last Image: Similar view from laterally shifted position (2-4 feet sideways)
- Both images must show clear foreground (bed/furniture) and background (windows/view)
- Large windows or glass walls essential for parallax effect
- Soft natural light streaming in

Visual Requirements:
- Bed and foreground elements should be clearly separated from background
- Windows/views should be prominent in both frames
- Same room, different lateral position (NOT different angles)
- Depth between foreground and background must be visible
- Avoid head-on symmetrical shots - prefer angled compositions

Alternative Selection (if no perfect lateral pair exists):
- Use two photos from slightly different positions in the same bedroom
- Ensure both show the bed and window view from angles that suggest lateral movement

═══════════════════════════════════════════════════════════════════════════════
SCENE 3: BATHROOM VANITY (Symmetry) - Duration: 1.0s
═══════════════════════════════════════════════════════════════════════════════
Camera Movement: Lateral truck (opposite direction to Scene 2)
Ideal Tags: bathroom, vanity, mirror, sink, interior, spa
Motion Goal: Smooth sliding movement with natural reflection shifts

Selection Criteria:
- First Image: Bathroom vanity with mirror clearly visible
- Last Image: Same vanity from laterally shifted position
- Mirrors and reflective surfaces essential
- Clean, bright lighting showcasing fixtures
- Sharp focus on surfaces and textures

Visual Requirements:
- Both images should show the same vanity/mirror setup
- Reflections should be visible and prominent
- Movement direction should feel opposite to Scene 2 (if Scene 2 went left, this goes right)
- Modern, spa-like aesthetic
- Symmetrical or semi-symmetrical composition in at least one image

Alternative Selection:
- Can use double vanity if available (better for lateral movement)
- Single vanity with large mirror also works
- Ensure lighting is consistent between images

═══════════════════════════════════════════════════════════════════════════════
SCENE 4: THE FEATURE TUB/SHOWER (Depth) - Duration: 1.0s
═══════════════════════════════════════════════════════════════════════════════
Camera Movement: Dolly forward (push-in toward centerpiece)
Ideal Tags: bathroom, tub, bathtub, shower, freestanding, spa, luxury
Motion Goal: Intimate reveal emphasizing the luxury fixture

Selection Criteria:
- First Image: Wide/medium shot showing tub or shower with surrounding context
- Last Image: Closer view highlighting the fixture's textures and materials
- Background window or view strongly preferred (adds depth)
- Serene, spa-like atmosphere
- High detail on materials (stone, marble, metal)

Visual Requirements:
- Both images must feature the SAME centerpiece fixture
- Progressive zoom/approach toward the fixture
- Background elements (windows, views) should remain relatively stable
- Soft, balanced lighting creating spa atmosphere
- Texture and material quality should be prominent in closer shot

Alternative Selection:
- If no perfect tub pair exists, feature shower can substitute
- Ensure fixture is visually distinctive and photogenic
- Maintain sense of luxury and tranquility

═══════════════════════════════════════════════════════════════════════════════
SCENE 5: LIVING ROOM (The Sweep) - Duration: 1.0s
═══════════════════════════════════════════════════════════════════════════════
Camera Movement: Lateral pan or truck following room layout
Ideal Tags: living room, lounge, seating, interior, furniture, windows
Motion Goal: Sweeping reveal of space, scale, and flow

Selection Criteria:
- First Image: Wide shot capturing one end/section of living area
- Last Image: View showing connected or adjacent section (swept perspective)
- Both images should show prominent seating arrangements
- Natural light streaming in from windows
- Spacious, open feel

Visual Requirements:
- Movement should follow natural lines of furniture arrangement
- Both images in same room but from positions suggesting lateral sweep
- Maintain consistent lighting (same time of day)
- Architectural flow should be evident
- Sense of space and scale important

Alternative Selection:
- Can use open-concept living/dining if no dedicated living room
- Ensure both images convey spaciousness and luxury
- Furniture layout should guide the eye across the frame

═══════════════════════════════════════════════════════════════════════════════
SCENE 6: LIFESTYLE/DINING AREA (Atmosphere) - Duration: 1.0s
═══════════════════════════════════════════════════════════════════════════════
Camera Movement: Minimal (near-static with subtle drift)
Ideal Tags: dining, kitchen, dining room, indoor-outdoor, lifestyle, entertaining
Motion Goal: Let lighting and ambiance breathe (atmospheric moment)

Selection Criteria:
- First Image: Elegant dining area or lifestyle space with warm lighting
- Last Image: Very similar view with subtle atmospheric differences
- Warm, inviting lighting essential (golden hour or evening preferred)
- Minimal movement required (nearly identical images work well)
- Indoor-outdoor connection ideal but not required

Visual Requirements:
- Both images should be nearly identical (static scene)
- Lighting should be warm and atmospheric
- Can be same image used twice if it's perfect for atmosphere
- Dining table, outdoor dining, or entertaining space
- Lifestyle elements (table settings, candles, ambiance) valued

Alternative Selection:
- Kitchen with great lighting can substitute if no formal dining
- Outdoor terrace with dining setup works well
- Focus on mood and atmosphere over movement

═══════════════════════════════════════════════════════════════════════════════
SCENE 7: THE OUTRO (Establishing Wide) - Duration: 3.5s
═══════════════════════════════════════════════════════════════════════════════
Camera Movement: Dolly backward (pull-out) or drone pull-back
Ideal Tags: exterior, wide, establishing, view, landscape, deck, entrance, architecture
Motion Goal: Final impression - gentle retreat revealing full context

Selection Criteria:
- First Image: Closer view of signature property feature (deck, entrance, or view)
- Last Image: Wide establishing shot showing full property context
- Signature view or architectural highlight essential
- Golden hour or evening lighting preferred (warm interior glow visible)
- Most inviting time of day

Visual Requirements:
- Both images should show exterior or exterior-facing views
- Progressive reveal of property's full context
- Warm interior lighting visible through windows (if applicable)
- Final image should be THE hero shot of the property
- Calm, peaceful, aspirational feeling
- Sense of arrival or welcome

Alternative Selection:
- Can use two different wide exterior shots if perfect pull-back pair unavailable
- Evening/twilight shots highly valued for this scene
- Entrance, deck, or signature view all work well

"""

        # Add available images
        prompt += f"""
═══════════════════════════════════════════════════════════════════════════════
AVAILABLE IMAGES ({len(assets)} total photos from property):
═══════════════════════════════════════════════════════════════════════════════
"""
        logger.info(f"[PROMPT BUILDER] Building image list for {len(assets)} assets")

        for i, asset in enumerate(assets, 1):
            tags = asset.get('tags', [])
            if isinstance(tags, str):
                try:
                    tags = json.loads(tags)
                except:
                    tags = []
            tags_str = ', '.join(tags) if tags else 'No tags'

            description = asset.get('description', 'No description')
            name = asset.get('name', 'Unnamed')
            asset_id = asset['id']

            logger.info(
                f"[PROMPT BUILDER] Image {i}: "
                f"id={asset_id[:12]}... name='{name}' tags=[{tags_str}]"
            )

            prompt += f"""
Image {i}:
  ID: {asset_id}
  Name: {name}
  Tags: {tags_str}
  Description: {description}
"""

        # Add output format instructions
        prompt += """
═══════════════════════════════════════════════════════════════════════════════
OUTPUT FORMAT (JSON):
═══════════════════════════════════════════════════════════════════════════════

You must return EXACTLY 7 pairs in this order:

{{
  "pairs": [
    {{
      "image1_id": "asset-uuid-for-first-frame",
      "image2_id": "asset-uuid-for-last-frame",
      "score": 0.95,
      "scene_number": 1,
      "scene_name": "The Hook (Exterior Feature)",
      "reasoning": "Wide pool shot to close-up of water edge. Perfect for dolly forward with glistening water highlights. Both images show bright natural lighting."
    }},
    {{
      "image1_id": "asset-uuid-for-first-frame",
      "image2_id": "asset-uuid-for-last-frame",
      "score": 0.88,
      "scene_number": 2,
      "scene_name": "The Hero Bedroom (Parallax)",
      "reasoning": "Master bedroom photos from slightly different lateral positions. Clear foreground (bed) and background (windows) separation for parallax effect."
    }},
    // ... continue for all 7 scenes
  ]
}}

CRITICAL REQUIREMENTS:
✓ MUST select exactly 7 pairs (one per scene, in order)
✓ Each pair should have distinct first_image and last_image (except Scene 6 which can reuse)
✓ Score each pair 0.0-1.0 based on how well it matches scene requirements
✓ Include scene_number (1-7) and scene_name for each pair
✓ Reasoning should explain why these specific images work for this scene's camera movement
✓ Prioritize pairs that will create smooth, professional video transitions
✓ Consider the cinematography requirements (camera movement, lighting, composition)
✓ Use image tags and descriptions to identify suitable candidates
✓ If perfect pairs don't exist, select the closest matches and note compromises in reasoning

SCENE ORDER (MUST FOLLOW):
1. The Hook (Exterior Feature) - 1.5s
2. The Hero Bedroom (Parallax) - 1.0s
3. Bathroom Vanity (Symmetry) - 1.0s
4. The Feature Tub/Shower (Depth) - 1.0s
5. Living Room (The Sweep) - 1.0s
6. Lifestyle/Dining Area (Atmosphere) - 1.0s
7. The Outro (Establishing Wide) - 3.5s

Begin your analysis and selection now.
"""

        return prompt

    def _call_grok_api(
        self, prompt: str, image_assets: List[Dict[str, Any]] = None, temperature: float = 0.7
    ) -> Dict[str, Any]:
        """
        Call the xAI Grok API.

        Args:
            prompt: The text prompt
            image_assets: List of image assets (for potential vision API support)
            temperature: Sampling temperature (0.0 to 1.0)

        Returns:
            API response dict
        """
        if image_assets is None:
            image_assets = []

        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }

        # Build messages - for now using text-only
        # Future: Can enhance with image URLs if Grok vision API supports it
        messages = [
            {
                "role": "system",
                "content": "You are an expert creative director and video producer.",
            },
            {"role": "user", "content": prompt},
        ]

        payload = {
            "model": self.model,
            "messages": messages,
            "temperature": temperature,
            "response_format": {"type": "json_object"},  # Request JSON response
        }

        logger.debug(f"Calling Grok API with model {self.model}")

        # Retry logic with exponential backoff
        max_retries = 3
        base_delay = 1.0  # seconds

        for attempt in range(max_retries):
            try:
                if attempt > 0:
                    delay = base_delay * (2 ** (attempt - 1))
                    logger.info(f"[GROK RETRY] Attempt {attempt + 1}/{max_retries} after {delay}s delay")
                    time.sleep(delay)

                response = requests.post(
                    f"{self.base_url}/chat/completions",
                    headers=headers,
                    json=payload,
                    timeout=60,
                )
                response.raise_for_status()

                # Try to parse JSON response
                try:
                    result = response.json()
                    logger.debug(f"Grok API response: {result}")

                    # Validate the response has the expected structure
                    if not result.get("choices") or not result["choices"][0].get("message"):
                        raise ValueError("Response missing expected structure")

                    return result

                except (json.JSONDecodeError, ValueError) as parse_error:
                    logger.warning(
                        f"[GROK RETRY] Attempt {attempt + 1}/{max_retries} failed to parse response: {parse_error}\n"
                        f"Response text: {response.text[:500]}"
                    )
                    if attempt == max_retries - 1:
                        raise RuntimeError(f"Failed to parse Grok API response after {max_retries} attempts: {parse_error}")
                    continue

            except requests.exceptions.RequestException as e:
                logger.warning(
                    f"[GROK RETRY] Attempt {attempt + 1}/{max_retries} failed with request error: {e}"
                )
                if attempt == max_retries - 1:
                    logger.error(f"Grok API request failed after {max_retries} attempts: {e}")
                    raise RuntimeError(f"Failed to call Grok API after {max_retries} attempts: {e}")
                continue

        # Should never reach here, but just in case
        raise RuntimeError(f"Failed to call Grok API after {max_retries} attempts")

    def _parse_pairs_response(
        self, response: Dict[str, Any], assets: List[Dict[str, Any]]
    ) -> List[Tuple[str, str, float, str]]:
        """
        Parse the Grok API response into image pairs.

        Args:
            response: Raw API response
            assets: Original asset list for validation

        Returns:
            List of tuples: (image1_id, image2_id, score, reasoning)
        """
        try:
            # Extract content from response
            logger.info(f"[GROK RAW RESPONSE] ===== FULL API RESPONSE START =====")
            logger.info(json.dumps(response, indent=2))
            logger.info(f"[GROK RAW RESPONSE] ===== FULL API RESPONSE END =====")

            content = response["choices"][0]["message"]["content"]
            logger.info(f"[GROK RAW RESPONSE] Content length: {len(content)} characters")

            # Parse JSON
            data = json.loads(content)
            logger.info(f"[GROK PARSED JSON] ===== PARSED JSON START =====")
            logger.info(json.dumps(data, indent=2))
            logger.info(f"[GROK PARSED JSON] ===== PARSED JSON END =====")

            # Write parsed response to debug file
            debug_log_path = "/tmp/image_pairing_debug.log"
            with open(debug_log_path, "a") as debug_file:
                debug_file.write(f"\n{'='*80}\n")
                debug_file.write(f"GROK RESPONSE\n")
                debug_file.write(f"{'='*80}\n")
                debug_file.write(json.dumps(data, indent=2))
                debug_file.write(f"\n{'='*80}\n\n")

            pairs_data = data.get("pairs", [])

            if not pairs_data:
                raise ValueError("No pairs found in response")

            # Validate and convert to tuples
            asset_ids = {a["id"] for a in assets}
            logger.info(f"[GROK VALIDATION] Available asset IDs: {len(asset_ids)} total")
            logger.info(f"[GROK VALIDATION] Sample IDs: {list(asset_ids)[:3]}")

            pairs = []

            for i, pair in enumerate(pairs_data, 1):
                logger.info(f"[GROK VALIDATION] Processing pair {i}/{len(pairs_data)}")
                logger.info(f"[GROK VALIDATION] Pair data: {json.dumps(pair, indent=2)}")

                image1_id = pair.get("image1_id")
                image2_id = pair.get("image2_id")
                score = pair.get("score", 0.5)
                reasoning = pair.get("reasoning", "No reasoning provided")
                scene_number = pair.get("scene_number")
                scene_name = pair.get("scene_name")

                logger.info(
                    f"[GROK VALIDATION] Pair {i}: "
                    f"scene={scene_number} ({scene_name}) "
                    f"img1={image1_id[:12] if image1_id else 'None'}... "
                    f"img2={image2_id[:12] if image2_id else 'None'}... "
                    f"score={score}"
                )

                # Validate asset IDs exist
                if image1_id not in asset_ids:
                    logger.warning(
                        f"[GROK VALIDATION] REJECTED Pair {i}: "
                        f"Invalid image1_id: {image1_id}, not in asset list"
                    )
                    continue
                if image2_id not in asset_ids:
                    logger.warning(
                        f"[GROK VALIDATION] REJECTED Pair {i}: "
                        f"Invalid image2_id: {image2_id}, not in asset list"
                    )
                    continue

                # Reject same-image pairs - video interpolation requires 2 different images
                if image1_id == image2_id:
                    logger.warning(
                        f"[GROK VALIDATION] REJECTED Pair {i}: "
                        f"Same image used twice: {image1_id} (need 2 different images for interpolation)"
                    )
                    continue

                logger.info(f"[GROK VALIDATION] ACCEPTED Pair {i}")
                pairs.append((image1_id, image2_id, float(score), reasoning))

                # Write accepted pair to debug file
                debug_log_path = "/tmp/image_pairing_debug.log"
                with open(debug_log_path, "a") as debug_file:
                    debug_file.write(f"ACCEPTED Pair {i}:\n")
                    debug_file.write(f"  Scene: {scene_number} - {scene_name}\n")
                    debug_file.write(f"  Image 1: {image1_id}\n")
                    debug_file.write(f"  Image 2: {image2_id}\n")
                    debug_file.write(f"  Score: {score}\n")
                    debug_file.write(f"  Reasoning: {reasoning}\n\n")

            if not pairs:
                raise ValueError("No valid pairs after validation")

            # Sort by score (highest first)
            pairs.sort(key=lambda x: x[2], reverse=True)

            return pairs

        except (KeyError, json.JSONDecodeError, ValueError) as e:
            logger.error(f"Failed to parse Grok response: {e}", exc_info=True)
            raise ValueError(f"Invalid response format from Grok: {e}")

    def select_property_scene_pairs(
        self,
        property_info: Dict[str, Any],
        photos: List[Dict[str, Any]],
        scene_types: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        Select optimal image pairs for luxury lodging property scenes using Grok.

        Args:
            property_info: Dict with name, location, property_type, positioning
            photos: List of photo dicts with id, filename, url, tags, metadata
            scene_types: List of 7 scene type definitions

        Returns:
            Dict with scene_pairs, selection_metadata, recommendations
        """
        if len(scene_types) != 7:
            raise ValueError(f"Expected 7 scene types, got {len(scene_types)}")

        logger.info(
            f"Selecting scene pairs for '{property_info.get('name')}' "
            f"from {len(photos)} photos"
        )

        # Build comprehensive prompt
        prompt = self._build_property_selection_prompt(
            property_info, photos, scene_types
        )

        # Call Grok API
        try:
            response = self._call_grok_api(prompt, [])
            result = self._parse_property_scene_response(response, photos, scene_types)

            logger.info(f"Successfully selected {len(result['scene_pairs'])} scene pairs")
            return result

        except Exception as e:
            logger.error(f"Failed to select property scene pairs: {e}", exc_info=True)
            raise

    def _build_property_selection_prompt(
        self,
        property_info: Dict[str, Any],
        photos: List[Dict[str, Any]],
        scene_types: List[Dict[str, Any]]
    ) -> str:
        """Build the comprehensive Grok prompt for property photo selection."""

        prompt = f"""# Luxury Lodging Video Scene Image Selection

You are an expert creative director specializing in luxury hospitality marketing. Your task is to select the optimal image pairs for a 7-scene promotional video showcasing a luxury lodging property.

## Property Information
**Property Name:** {property_info.get('name', 'Unknown')}
**Location:** {property_info.get('location', 'Not specified')}
**Property Type:** {property_info.get('property_type', 'Luxury lodging')}
**Brand Positioning:** {property_info.get('positioning', 'High-end hospitality')}

## Available Photos
You have access to {len(photos)} photos crawled from the property website.

"""

        # Add photo catalog
        prompt += "### Photo Catalog\n```json\n{\n  \"photos\": [\n"
        for i, photo in enumerate(photos):
            photo_entry = {
                "id": photo.get("id"),
                "filename": photo.get("filename", ""),
                "tags": photo.get("tags", []),
                "dominant_colors": photo.get("dominant_colors", []),
                "detected_objects": photo.get("detected_objects", []),
                "composition": photo.get("composition", "unknown"),
                "lighting": photo.get("lighting", "unknown")
            }
            prompt += "    " + json.dumps(photo_entry)
            if i < len(photos) - 1:
                prompt += ","
            prompt += "\n"
        prompt += "  ]\n}\n```\n\n"

        # Add scene type requirements
        prompt += "## Scene Types & Requirements\n\n"
        prompt += "You must select exactly **2 images per scene** (first frame + last frame) for smooth video interpolation.\n\n"

        for scene in scene_types:
            prompt += f"""### Scene {scene['scene_number']}: {scene['scene_type']} ({scene['duration']} seconds)
**Purpose:** {scene['purpose']}
**Visual Priority:** {scene['visual_priority']}
**Mood:** {scene['mood']}
**Ideal Tags:** {', '.join(scene['ideal_tags'])}
**First Image:** {scene['first_image_guidance']}
**Last Image:** {scene['last_image_guidance']}
**Transition Goal:** {scene['transition_goal']}

"""

        # Add selection criteria
        prompt += """## Selection Criteria

For each scene type, evaluate images based on:

### 1. Visual Quality (30%)
- High resolution and sharpness
- Professional composition (rule of thirds, leading lines, symmetry)
- Optimal lighting (natural light preferred, golden hour highly valued)
- Color harmony and aesthetic appeal

### 2. Tag Alignment (25%)
- Strong match with scene type's ideal tags
- Relevant detected objects for scene narrative
- Appropriate setting and context

### 3. Transition Potential (25%)
- **Critical for AI video generation success**
- Similar lighting conditions between first/last image
- Compatible color palettes (smooth gradient possible)
- Compositional flow (wide → medium, or establishing → detail)
- Avoid jarring jumps in perspective or subject matter

### 4. Brand Consistency (20%)
- Aligns with property positioning and target market
- Represents property's unique character
- Appeals to luxury hospitality audience
- Authentic representation (not generic stock-photo feel)

## Output Format

Return your selections as JSON with the following structure:

```json
{
  "property_name": "{property_info.get('name', 'Unknown')}",
  "selection_metadata": {
    "total_photos_evaluated": {len(photos)},
    "selection_confidence": "high|medium|low",
    "overall_visual_quality_score": 8.5,
    "brand_coherence_score": 9.2
  },
  "scene_pairs": [
    {
      "scene_number": 1,
      "scene_type": "Grand Arrival",
      "first_image": {
        "id": "photo_id",
        "filename": "filename.jpg",
        "reasoning": "Why this image works as first frame",
        "quality_score": 9.1,
        "tag_match_score": 9.5
      },
      "last_image": {
        "id": "photo_id",
        "filename": "filename.jpg",
        "reasoning": "Why this image works as last frame",
        "quality_score": 8.8,
        "tag_match_score": 9.0
      },
      "transition_analysis": {
        "color_compatibility": "excellent|good|fair|poor",
        "lighting_consistency": "description",
        "compositional_flow": "description",
        "interpolation_confidence": 9.2
      }
    }
  ],
  "recommendations": {
    "missing_content_gaps": [],
    "photo_crawl_improvements": []
  }
}
```

## Critical Constraints

1. **Exactly 2 images per scene** (14 images total across 7 scenes)
2. **No image reuse** - each of the 14 images must be unique
3. **Transition smoothness is paramount** - prioritize interpolation success
4. **Scene progression** - consider narrative flow across all 7 scenes
5. **Color story** - maintain visual cohesion across the full 35-second video
6. **Authentic representation** - select real property photos only

Analyze all {len(photos)} photos carefully and select the optimal pairs for each scene type.
"""

        return prompt

    def _parse_property_scene_response(
        self,
        response: Dict[str, Any],
        photos: List[Dict[str, Any]],
        scene_types: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        Parse Grok's property scene selection response.

        Args:
            response: Raw API response from Grok
            photos: Original photo list for validation
            scene_types: Scene type definitions

        Returns:
            Parsed and validated result dict
        """
        try:
            # Extract content from response
            content = response["choices"][0]["message"]["content"]

            # Parse JSON
            data = json.loads(content)

            if not data.get("scene_pairs"):
                raise ValueError("No scene_pairs in Grok response")

            # Validate photo IDs exist
            photo_ids = {p["id"] for p in photos}

            validated_pairs = []
            for pair in data["scene_pairs"]:
                scene_num = pair.get("scene_number")
                if not scene_num or scene_num < 1 or scene_num > 7:
                    logger.warning(f"Invalid scene_number: {scene_num}, skipping")
                    continue

                first_img = pair.get("first_image", {})
                last_img = pair.get("last_image", {})

                first_id = first_img.get("id")
                last_id = last_img.get("id")

                # Validate IDs exist
                if first_id not in photo_ids:
                    logger.warning(f"Invalid first_image id: {first_id}, skipping scene {scene_num}")
                    continue
                if last_id not in photo_ids:
                    logger.warning(f"Invalid last_image id: {last_id}, skipping scene {scene_num}")
                    continue

                # Validate images are different
                if first_id == last_id:
                    logger.warning(f"Scene {scene_num} has same image twice, skipping")
                    continue

                validated_pairs.append(pair)

            if not validated_pairs:
                raise ValueError("No valid scene pairs after validation")

            # Sort by scene number
            validated_pairs.sort(key=lambda x: x["scene_number"])

            return {
                "property_name": data.get("property_name", "Unknown"),
                "selection_metadata": data.get("selection_metadata", {}),
                "scene_pairs": validated_pairs,
                "recommendations": data.get("recommendations", {})
            }

        except (KeyError, json.JSONDecodeError, ValueError) as e:
            logger.error(f"Failed to parse property scene response: {e}", exc_info=True)
            raise ValueError(f"Invalid response format from Grok: {e}")
</file>

<file path="backend/database_helpers.py">
"""Database helper functions for Clients and Campaigns management.

This module provides CRUD operations for:
- Clients (brand/client management with brand guidelines)
- Client Assets (logos, brand documents)
- Campaigns (marketing campaigns linked to clients)
- Campaign Assets (campaign-specific media)
- Assets (consolidated asset management with Pydantic models)
"""

import sqlite3
import json
import uuid
from typing import List, Optional, Dict, Any, Union
from contextlib import contextmanager
from pathlib import Path
import os
from datetime import datetime
import logging

# Import Pydantic asset models
from .schemas.assets import (
    Asset,
    AssetDB,
    ImageAsset,
    VideoAsset,
    AudioAsset,
    DocumentAsset,
)

# Get data directory from environment variable, default to ./DATA
DATA_DIR = Path(os.getenv("DATA", "./DATA"))
DB_PATH = DATA_DIR / "scenes.db"


@contextmanager
def get_db():
    """Context manager for database connections."""
    conn = sqlite3.connect(str(DB_PATH))
    conn.row_factory = sqlite3.Row
    try:
        yield conn
    finally:
        conn.close()


# ============================================================================
# CLIENT CRUD OPERATIONS
# ============================================================================


def create_client(
    user_id: int,
    name: str,
    description: str = "",
    homepage: Optional[str] = None,
    brand_guidelines: Optional[Dict[str, Any]] = None,
    metadata: Optional[Dict[str, Any]] = None,
) -> str:
    """Create a new client."""
    client_id = str(uuid.uuid4())

    with get_db() as conn:
        conn.execute(
            """
            INSERT INTO clients (id, user_id, name, description, homepage, brand_guidelines, metadata)
            VALUES (?, ?, ?, ?, ?, ?, ?)
            """,
            (
                client_id,
                user_id,
                name,
                description,
                homepage,
                json.dumps(brand_guidelines) if brand_guidelines else None,
                json.dumps(metadata) if metadata else None,
            ),
        )
        conn.commit()
        return client_id


def get_client_by_id(client_id: str, user_id: int) -> Optional[Dict[str, Any]]:
    """Get a client by ID (user must own the client)."""
    with get_db() as conn:
        row = conn.execute(
            "SELECT * FROM clients WHERE id = ? AND user_id = ?", (client_id, user_id)
        ).fetchone()

        if row:
            try:
                brand_guidelines = None
                if row["brand_guidelines"]:
                    try:
                        brand_guidelines = json.loads(row["brand_guidelines"])
                        print(
                            f"DEBUG: Parsed brand_guidelines for client {row['id']}: {brand_guidelines}"
                        )
                    except json.JSONDecodeError as e:
                        print(
                            f"ERROR: Failed to parse brand_guidelines for client {row['id']}: {e}"
                        )
                        brand_guidelines = None

                # Safely check for homepage and metadata columns
                # (they might not exist in older production databases)
                homepage = None
                metadata = None

                try:
                    if "homepage" in row.keys() and row["homepage"]:
                        homepage = row["homepage"]
                except KeyError:
                    pass

                try:
                    if "metadata" in row.keys() and row["metadata"]:
                        metadata = json.loads(row["metadata"])
                except (KeyError, json.JSONDecodeError):
                    metadata = None

                return {
                    "id": row["id"],
                    "name": row["name"],
                    "description": row["description"],
                    "homepage": homepage,
                    "brandGuidelines": brand_guidelines,
                    "metadata": metadata,
                    "createdAt": row["created_at"],
                    "updatedAt": row["updated_at"],
                }
            except Exception as e:
                print(f"ERROR: Failed to process client {row['id']}: {e}")
                return None
    return None


def list_clients(
    user_id: int, limit: int = 100, offset: int = 0
) -> List[Dict[str, Any]]:
    """List all clients for a user."""
    with get_db() as conn:
        rows = conn.execute(
            """
            SELECT * FROM clients
            WHERE user_id = ?
            ORDER BY created_at DESC
            LIMIT ? OFFSET ?
            """,
            (user_id, limit, offset),
        ).fetchall()

        clients_data = []
        for row in rows:
            try:
                brand_guidelines = None
                if row["brand_guidelines"]:
                    try:
                        brand_guidelines = json.loads(row["brand_guidelines"])
                        print(
                            f"DEBUG: Parsed brand_guidelines for client {row['id']}: {brand_guidelines}"
                        )
                    except json.JSONDecodeError as e:
                        print(
                            f"ERROR: Failed to parse brand_guidelines for client {row['id']}: {e}"
                        )
                        brand_guidelines = None

                # Safely check for homepage and metadata columns
                # (they might not exist in older production databases)
                homepage = None
                metadata = None

                try:
                    if "homepage" in row.keys() and row["homepage"]:
                        homepage = row["homepage"]
                except KeyError:
                    pass

                try:
                    if "metadata" in row.keys() and row["metadata"]:
                        metadata = json.loads(row["metadata"])
                        print(
                            f"DEBUG: Parsed metadata for client {row['id']}: {metadata}"
                        )
                except (KeyError, json.JSONDecodeError) as e:
                    print(
                        f"DEBUG: Could not parse metadata for client {row['id']}: {e}"
                    )
                    metadata = None

                client_data = {
                    "id": row["id"],
                    "name": row["name"],
                    "description": row["description"],
                    "homepage": homepage,
                    "brandGuidelines": brand_guidelines,
                    "metadata": metadata,
                    "createdAt": row["created_at"],
                    "updatedAt": row["updated_at"],
                }
                clients_data.append(client_data)
                print(f"DEBUG: Processed client {row['id']}: {client_data}")

            except Exception as e:
                print(f"ERROR: Failed to process client {row['id']}: {e}")
                continue

        return clients_data


def update_client(
    client_id: str,
    user_id: int,
    name: Optional[str] = None,
    description: Optional[str] = None,
    homepage: Optional[str] = None,
    brand_guidelines: Optional[Dict[str, Any]] = None,
    metadata: Optional[Dict[str, Any]] = None,
) -> bool:
    """Update a client (partial update)."""
    with get_db() as conn:
        # Build dynamic update query
        update_fields = []
        values = []

        if name is not None:
            update_fields.append("name = ?")
            values.append(name)

        if description is not None:
            update_fields.append("description = ?")
            values.append(description)

        if homepage is not None:
            update_fields.append("homepage = ?")
            values.append(homepage)

        if brand_guidelines is not None:
            update_fields.append("brand_guidelines = ?")
            values.append(json.dumps(brand_guidelines))

        if metadata is not None:
            update_fields.append("metadata = ?")
            values.append(json.dumps(metadata))

        if not update_fields:
            return False  # Nothing to update

        # Add WHERE clause values
        values.extend([client_id, user_id])

        query = f"""
            UPDATE clients
            SET {", ".join(update_fields)}
            WHERE id = ? AND user_id = ?
        """

        cursor = conn.execute(query, values)
        conn.commit()
        return cursor.rowcount > 0


def delete_client(client_id: str, user_id: int) -> bool:
    """Delete a client (cascades to campaigns and assets)."""
    with get_db() as conn:
        cursor = conn.execute(
            "DELETE FROM clients WHERE id = ? AND user_id = ?", (client_id, user_id)
        )
        conn.commit()
        return cursor.rowcount > 0


def get_client_stats(client_id: str, user_id: int) -> Optional[Dict[str, Any]]:
    """Get statistics for a client."""
    with get_db() as conn:
        # Verify ownership
        client = conn.execute(
            "SELECT id FROM clients WHERE id = ? AND user_id = ?", (client_id, user_id)
        ).fetchone()

        if not client:
            return None

        # Get campaign count
        campaign_count_row = conn.execute(
            "SELECT COUNT(*) as count FROM campaigns WHERE client_id = ?", (client_id,)
        ).fetchone()
        campaign_count = campaign_count_row["count"] if campaign_count_row else 0

        # Get video count and total spend
        video_stats_row = conn.execute(
            """
            SELECT
                COUNT(v.id) as video_count,
                COALESCE(SUM(v.actual_cost), 0) as total_spend
            FROM campaigns c
            LEFT JOIN generated_videos v ON v.campaign_id = c.id
            WHERE c.client_id = ?
            """,
            (client_id,),
        ).fetchone()

        video_count = video_stats_row["video_count"] if video_stats_row else 0
        total_spend = video_stats_row["total_spend"] if video_stats_row else 0.0

        return {
            "campaignCount": campaign_count,
            "videoCount": video_count,
            "totalSpend": float(total_spend),
        }


# ============================================================================
# CONSOLIDATED ASSETS CRUD OPERATIONS
# ============================================================================


def create_asset(
    name: str,
    asset_type: str,
    url: str,
    format: str,
    size: Optional[int] = None,
    user_id: Optional[int] = None,
    client_id: Optional[str] = None,
    campaign_id: Optional[str] = None,
    tags: Optional[List[str]] = None,
    width: Optional[int] = None,
    height: Optional[int] = None,
    duration: Optional[int] = None,
    thumbnail_url: Optional[str] = None,
    thumbnail_blob_id: Optional[str] = None,
    waveform_url: Optional[str] = None,
    page_count: Optional[int] = None,
    asset_id: Optional[str] = None,
    blob_data: Optional[bytes] = None,
    blob_id: Optional[str] = None,
    source_url: Optional[str] = None,
) -> str:
    """Create a new asset in the consolidated assets table.

    Args:
        name: Display name of the asset
        asset_type: Type discriminator ('image', 'video', 'audio', 'document')
        url: Full URL to the file in cloud storage
        format: Specific file format ('png', 'mp4', 'mp3', 'pdf', etc.)
        size: File size in bytes
        user_id: Owner user ID (nullable)
        client_id: Associated client ID (nullable)
        campaign_id: Associated campaign ID (nullable)
        tags: Array of text tags
        width: For images and videos
        height: For images and videos
        duration: For videos and audio (in seconds)
        thumbnail_url: For videos and documents
        waveform_url: For audio
        page_count: For documents
        asset_id: Optional pre-generated asset ID (if None, generates new UUID)
        blob_data: Optional binary blob data for storing asset in database
        blob_id: Optional reference to asset_blobs table (for V3 blob storage)
        source_url: Optional original URL where asset was downloaded from

    Returns:
        Asset ID (UUID string)
    """
    if asset_id is None:
        asset_id = str(uuid.uuid4())

    with get_db() as conn:
        conn.execute(
            """
            INSERT INTO assets (
                id, user_id, client_id, campaign_id, name, asset_type, url,
                size, format, tags, width, height, duration, thumbnail_url,
                thumbnail_blob_id, waveform_url, page_count, blob_data, blob_id, source_url
            )
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """,
            (
                asset_id,
                user_id,
                client_id,
                campaign_id,
                name,
                asset_type,
                url,
                size,
                format,
                json.dumps(tags) if tags else None,
                width,
                height,
                duration,
                thumbnail_url,
                thumbnail_blob_id,
                waveform_url,
                page_count,
                blob_data,
                blob_id,
                source_url,
            ),
        )
        conn.commit()
        return asset_id


def get_asset_by_id(asset_id: str, include_blob: bool = False) -> Optional[Asset]:
    """Get an asset by ID and return as Pydantic Asset model.

    Args:
        asset_id: Asset UUID
        include_blob: If True, includes blob_data in the response (default False)

    Returns:
        Asset (ImageAsset | VideoAsset | AudioAsset | DocumentAsset) or None
    """
    with get_db() as conn:
        # Select all columns except blob_data unless specifically requested
        if include_blob:
            query = "SELECT * FROM assets WHERE id = ?"
        else:
            query = """
                SELECT id, user_id, client_id, campaign_id, name, asset_type, url,
                       size, uploaded_at, format, tags, width, height, duration,
                       thumbnail_url, waveform_url, page_count, blob_id, source_url
                FROM assets WHERE id = ?
            """

        row = conn.execute(query, (asset_id,)).fetchone()

        if row:
            return _row_to_asset_model(row)
    return None


def list_assets(
    user_id: Optional[int] = None,
    client_id: Optional[str] = None,
    campaign_id: Optional[str] = None,
    asset_type: Optional[str] = None,
    limit: int = 100,
    offset: int = 0,
) -> List[Asset]:
    """List assets with optional filtering.

    Args:
        user_id: Filter by user
        client_id: Filter by client
        campaign_id: Filter by campaign
        asset_type: Filter by type ('image', 'video', 'audio', 'document')
        limit: Maximum number of results
        offset: Pagination offset

    Returns:
        List of Asset Pydantic models (ImageAsset | VideoAsset | AudioAsset | DocumentAsset)
    """
    print(f"[DEBUG] list_assets called with: user_id={user_id}, client_id={client_id}, campaign_id={campaign_id}, asset_type={asset_type}")

    with get_db() as conn:
        # Build dynamic query
        where_clauses = []
        values = []

        if user_id is not None:
            where_clauses.append("user_id = ?")
            values.append(user_id)

        if client_id is not None:
            where_clauses.append("client_id = ?")
            values.append(client_id)

        if campaign_id is not None:
            where_clauses.append("campaign_id = ?")
            values.append(campaign_id)

        if asset_type is not None:
            where_clauses.append("asset_type = ?")
            values.append(asset_type)

        where_clause = f"WHERE {' AND '.join(where_clauses)}" if where_clauses else ""
        values.extend([limit, offset])

        # Don't include blob_data in list queries for performance
        query = f"""
            SELECT id, user_id, client_id, campaign_id, name, asset_type, url,
                   size, uploaded_at, format, tags, width, height, duration,
                   thumbnail_url, waveform_url, page_count
            FROM assets
            {where_clause}
            ORDER BY uploaded_at DESC
            LIMIT ? OFFSET ?
        """

        print(f"[DEBUG] Executing query: {query}")
        print(f"[DEBUG] With values: {values}")
        rows = conn.execute(query, values).fetchall()
        print(f"[DEBUG] Query returned {len(rows)} rows")
        return [_row_to_asset_model(row) for row in rows]


def update_asset(
    asset_id: str,
    name: Optional[str] = None,
    client_id: Optional[str] = None,
    campaign_id: Optional[str] = None,
    tags: Optional[List[str]] = None,
) -> bool:
    """Update an asset (partial update)."""
    with get_db() as conn:
        update_fields = []
        values = []

        if name is not None:
            update_fields.append("name = ?")
            values.append(name)

        if client_id is not None:
            update_fields.append("client_id = ?")
            values.append(client_id)

        if campaign_id is not None:
            update_fields.append("campaign_id = ?")
            values.append(campaign_id)

        if tags is not None:
            update_fields.append("tags = ?")
            values.append(json.dumps(tags))

        if not update_fields:
            return False

        values.append(asset_id)

        query = f"""
            UPDATE assets
            SET {", ".join(update_fields)}
            WHERE id = ?
        """

        cursor = conn.execute(query, values)
        conn.commit()
        return cursor.rowcount > 0


def delete_asset(asset_id: str, user_id: Optional[int] = None) -> bool:
    """Delete an asset. If user_id is provided, only delete if asset belongs to user."""
    with get_db() as conn:
        if user_id is not None:
            # Check ownership first
            cursor = conn.execute(
                "SELECT id FROM assets WHERE id = ? AND user_id = ?",
                (asset_id, user_id),
            )
            if not cursor.fetchone():
                return False

        cursor = conn.execute("DELETE FROM assets WHERE id = ?", (asset_id,))
        conn.commit()
        return cursor.rowcount > 0


def _row_to_asset_model(row: sqlite3.Row) -> Asset:
    """Convert a database row to an Asset Pydantic model.

    Returns the appropriate asset type (ImageAsset | VideoAsset | AudioAsset | DocumentAsset)
    based on asset_type discriminator.
    """
    # Parse tags from JSON string
    tags_list = None
    if row["tags"]:
        try:
            tags_list = json.loads(row["tags"])
        except:
            pass

    # Common fields for all asset types
    common = {
        "id": row["id"],
        "userId": str(row["user_id"]) if row["user_id"] else "",
        "clientId": row["client_id"],
        "campaignId": row["campaign_id"],
        "name": row["name"],
        "url": row["url"],
        "size": row["size"],
        "uploadedAt": row["uploaded_at"],  # Will be formatted by Pydantic if datetime
        "tags": tags_list,
        "format": row["format"],
    }

    # Create appropriate Asset type based on discriminator
    asset_type = row["asset_type"]

    if asset_type == "image":
        return ImageAsset(
            **common,
            width=row["width"] or 0,
            height=row["height"] or 0,
        )
    elif asset_type == "video":
        return VideoAsset(
            **common,
            width=row["width"] or 0,
            height=row["height"] or 0,
            duration=row["duration"] or 0,
            thumbnailUrl=row["thumbnail_url"] or "",
        )
    elif asset_type == "audio":
        return AudioAsset(
            **common,
            duration=row["duration"] or 0,
            waveformUrl=row["waveform_url"],
        )
    elif asset_type == "document":
        return DocumentAsset(
            **common,
            pageCount=row["page_count"],
            thumbnailUrl=row["thumbnail_url"],
        )
    else:
        raise ValueError(f"Unknown asset type: {asset_type}")


# ============================================================================
# CAMPAIGN CRUD OPERATIONS
# ============================================================================


def create_campaign(
    user_id: int,
    client_id: str,
    name: str,
    goal: str,
    status: str = "draft",
    product_url: Optional[str] = None,
    brief: Optional[Dict[str, Any]] = None,
    metadata: Optional[Dict[str, Any]] = None,
) -> str:
    """Create a new campaign."""
    campaign_id = str(uuid.uuid4())

    with get_db() as conn:
        conn.execute(
            """
            INSERT INTO campaigns (id, client_id, user_id, name, goal, status, product_url, brief, metadata)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            """,
            (
                campaign_id,
                client_id,
                user_id,
                name,
                goal,
                status,
                product_url,
                json.dumps(brief) if brief else None,
                json.dumps(metadata) if metadata else None,
            ),
        )
        conn.commit()
        return campaign_id


def get_campaign_by_id(campaign_id: str, user_id: int) -> Optional[Dict[str, Any]]:
    """Get a campaign by ID (allows cross-user access for API keys)."""
    with get_db() as conn:
        row = conn.execute(
            "SELECT * FROM campaigns WHERE id = ?",
            (campaign_id,),
        ).fetchone()

        if row:
            # Safely handle optional columns (product_url, metadata)
            product_url = None
            metadata = None

            try:
                if "product_url" in row.keys() and row["product_url"]:
                    product_url = row["product_url"]
            except KeyError:
                pass

            try:
                if "metadata" in row.keys() and row["metadata"]:
                    metadata = json.loads(row["metadata"])
            except (KeyError, json.JSONDecodeError):
                pass

            return {
                "id": row["id"],
                "clientId": row["client_id"],
                "name": row["name"],
                "goal": row["goal"],
                "status": row["status"],
                "productUrl": product_url,
                "brief": json.loads(row["brief"]) if row["brief"] else None,
                "metadata": metadata,
                "createdAt": row["created_at"],
                "updatedAt": row["updated_at"],
            }
    return None


def list_campaigns(
    user_id: Optional[int] = None, client_id: Optional[str] = None, limit: int = 100, offset: int = 0
) -> List[Dict[str, Any]]:
    """List campaigns, optionally filtered by user and/or client."""
    with get_db() as conn:
        # Build query dynamically based on filters
        query = "SELECT * FROM campaigns"
        conditions = []
        params = []

        if user_id is not None:
            conditions.append("user_id = ?")
            params.append(user_id)

        if client_id:
            conditions.append("client_id = ?")
            params.append(client_id)

        if conditions:
            query += " WHERE " + " AND ".join(conditions)

        query += " ORDER BY created_at DESC LIMIT ? OFFSET ?"
        params.extend([limit, offset])

        rows = conn.execute(query, params).fetchall()

        campaigns = []
        for row in rows:
            # Safely handle optional columns (product_url, metadata)
            product_url = None
            metadata = None

            try:
                if "product_url" in row.keys() and row["product_url"]:
                    product_url = row["product_url"]
            except KeyError:
                pass

            try:
                if "metadata" in row.keys() and row["metadata"]:
                    metadata = json.loads(row["metadata"])
            except (KeyError, json.JSONDecodeError):
                pass

            campaigns.append({
                "id": row["id"],
                "clientId": row["client_id"],
                "name": row["name"],
                "goal": row["goal"],
                "status": row["status"],
                "productUrl": product_url,
                "brief": json.loads(row["brief"]) if row["brief"] else None,
                "metadata": metadata,
                "createdAt": row["created_at"],
                "updatedAt": row["updated_at"],
            })

        return campaigns


def update_campaign(
    campaign_id: str,
    user_id: int,
    name: Optional[str] = None,
    goal: Optional[str] = None,
    status: Optional[str] = None,
    product_url: Optional[str] = None,
    brief: Optional[Dict[str, Any]] = None,
    metadata: Optional[Dict[str, Any]] = None,
) -> bool:
    """Update a campaign (partial update)."""
    with get_db() as conn:
        # Build dynamic update query
        update_fields = []
        values = []

        if name is not None:
            update_fields.append("name = ?")
            values.append(name)

        if goal is not None:
            update_fields.append("goal = ?")
            values.append(goal)

        if status is not None:
            update_fields.append("status = ?")
            values.append(status)

        if product_url is not None:
            update_fields.append("product_url = ?")
            values.append(product_url)

        if brief is not None:
            update_fields.append("brief = ?")
            values.append(json.dumps(brief))

        if metadata is not None:
            update_fields.append("metadata = ?")
            values.append(json.dumps(metadata))

        if not update_fields:
            return False  # Nothing to update

        # Add WHERE clause values
        values.extend([campaign_id, user_id])

        query = f"""
            UPDATE campaigns
            SET {", ".join(update_fields)}
            WHERE id = ? AND user_id = ?
        """

        cursor = conn.execute(query, values)
        conn.commit()
        return cursor.rowcount > 0


def delete_campaign(campaign_id: str, user_id: int) -> bool:
    """Delete a campaign (cascades to campaign assets)."""
    with get_db() as conn:
        cursor = conn.execute(
            "DELETE FROM campaigns WHERE id = ? AND user_id = ?", (campaign_id, user_id)
        )
        conn.commit()
        return cursor.rowcount > 0


def get_campaign_stats(campaign_id: str, user_id: int) -> Optional[Dict[str, Any]]:
    """Get statistics for a campaign."""
    with get_db() as conn:
        # Verify ownership
        campaign = conn.execute(
            "SELECT id FROM campaigns WHERE id = ? AND user_id = ?",
            (campaign_id, user_id),
        ).fetchone()

        if not campaign:
            return None

        # Get video count, total spend, and average cost
        stats_row = conn.execute(
            """
            SELECT
                COUNT(id) as video_count,
                COALESCE(SUM(actual_cost), 0) as total_spend,
                COALESCE(AVG(actual_cost), 0) as avg_cost
            FROM generated_videos
            WHERE campaign_id = ?
            """,
            (campaign_id,),
        ).fetchone()

        video_count = stats_row["video_count"] if stats_row else 0
        total_spend = stats_row["total_spend"] if stats_row else 0.0
        avg_cost = stats_row["avg_cost"] if stats_row else 0.0

        return {
            "videoCount": video_count,
            "totalSpend": float(total_spend),
            "avgCost": float(avg_cost),
        }


# ============================================================================
# DEPRECATED: Old campaign asset functions (use consolidated assets API)
# ============================================================================
# These functions are kept for backward compatibility but redirect to new assets table


# ============================================================================
# VIDEO OPERATIONS (Enhanced for frontend integration)
# ============================================================================


def update_video_metrics(
    video_id: int,
    views: Optional[int] = None,
    clicks: Optional[int] = None,
    ctr: Optional[float] = None,
    conversions: Optional[int] = None,
) -> bool:
    """Update video performance metrics."""
    with get_db() as conn:
        # Build dynamic update query
        update_fields = []
        values = []

        if views is not None:
            update_fields.append("views = ?")
            values.append(views)

        if clicks is not None:
            update_fields.append("clicks = ?")
            values.append(clicks)

        if ctr is not None:
            update_fields.append("ctr = ?")
            values.append(ctr)

        if conversions is not None:
            update_fields.append("conversions = ?")
            values.append(conversions)

        if not update_fields:
            return False  # Nothing to update

        # Add WHERE clause value
        values.append(video_id)

        query = f"""
            UPDATE generated_videos
            SET {", ".join(update_fields)}
            WHERE id = ?
        """

        cursor = conn.execute(query, values)
        conn.commit()
        return cursor.rowcount > 0


def list_videos_by_campaign(
    campaign_id: str, limit: int = 50, offset: int = 0
) -> List[Dict[str, Any]]:
    """List all videos for a campaign."""
    with get_db() as conn:
        # Exclude video_data BLOB to avoid loading large binary data for gallery listing
        rows = conn.execute(
            """
            SELECT id, prompt, video_url, model_id, parameters, status,
                   created_at, collection, metadata, campaign_id, format,
                   duration, views, clicks, ctr, conversions, actual_cost,
                   updated_at, storyboard_data, progress
            FROM generated_videos
            WHERE campaign_id = ?
            ORDER BY created_at DESC
            LIMIT ? OFFSET ?
            """,
            (campaign_id, limit, offset),
        ).fetchall()

        def safe_get(row, key, default=None):
            try:
                return row[key]
            except (KeyError, IndexError):
                return default

        return [
            {
                "id": row["id"],
                "campaignId": safe_get(row, "campaign_id"),
                "name": row["prompt"][:50]
                if row["prompt"]
                else "Untitled",  # Use prompt as name
                "status": row["status"],
                "format": safe_get(row, "format", "16:9"),
                "duration": safe_get(row, "duration", 30),
                "prompt": row["prompt"],
                "videoUrl": row["video_url"],
                "storyboard": json.loads(safe_get(row, "storyboard_data"))
                if safe_get(row, "storyboard_data")
                else None,
                "generationProgress": json.loads(safe_get(row, "progress"))
                if safe_get(row, "progress")
                else None,
                "metrics": {
                    "views": safe_get(row, "views", 0),
                    "clicks": safe_get(row, "clicks", 0),
                    "ctr": safe_get(row, "ctr", 0.0),
                    "conversions": safe_get(row, "conversions", 0),
                },
                "cost": safe_get(row, "actual_cost", 0.0),
                "createdAt": row["created_at"],
                "updatedAt": safe_get(row, "updated_at", row["created_at"]),
            }
            for row in rows
        ]


# ============================================================================
# Scene Management Functions (Phase 2)
# ============================================================================


def create_job_scene(
    job_id: int,
    scene_number: int,
    duration: float,
    description: str,
    script: Optional[str] = None,
    shot_type: Optional[str] = None,
    transition: Optional[str] = None,
    assets: Optional[List[str]] = None,
    metadata: Optional[Dict[str, Any]] = None,
) -> str:
    """
    Create a scene record for a job.

    Args:
        job_id: The job ID this scene belongs to
        scene_number: Scene number (1-indexed)
        duration: Scene duration in seconds
        description: Scene description
        script: Optional voiceover script
        shot_type: Optional shot type (wide, close-up, etc.)
        transition: Optional transition type (cut, fade, etc.)
        assets: Optional list of asset IDs used in this scene
        metadata: Optional additional scene metadata

    Returns:
        The created scene ID
    """
    scene_id = str(uuid.uuid4())
    assets_json = json.dumps(assets) if assets else None
    metadata_json = json.dumps(metadata) if metadata else None

    with get_db() as conn:
        conn.execute(
            """
            INSERT INTO job_scenes (
                id, job_id, scene_number, duration_seconds, description,
                script, shot_type, transition, assets, metadata
            )
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """,
            (
                scene_id,
                job_id,
                scene_number,
                duration,
                description,
                script,
                shot_type,
                transition,
                assets_json,
                metadata_json,
            ),
        )
        conn.commit()

    return scene_id


def get_scenes_by_job(job_id: int) -> List[Dict[str, Any]]:
    """
    Get all scenes for a job, ordered by scene number.

    Args:
        job_id: The job ID

    Returns:
        List of scene dictionaries
    """
    with get_db() as conn:
        cursor = conn.execute(
            """
            SELECT id, job_id, scene_number, duration_seconds, description,
                   script, shot_type, transition, assets, metadata,
                   created_at, updated_at
            FROM job_scenes
            WHERE job_id = ?
            ORDER BY scene_number ASC
            """,
            (job_id,),
        )
        rows = cursor.fetchall()

        return [
            {
                "id": row["id"],
                "jobId": row["job_id"],
                "sceneNumber": row["scene_number"],
                "duration": row["duration_seconds"],
                "description": row["description"],
                "script": row["script"],
                "shotType": row["shot_type"],
                "transition": row["transition"],
                "assets": json.loads(row["assets"]) if row["assets"] else [],
                "metadata": json.loads(row["metadata"]) if row["metadata"] else {},
                "createdAt": row["created_at"],
                "updatedAt": row["updated_at"],
            }
            for row in rows
        ]


def get_scene_by_id(scene_id: str) -> Optional[Dict[str, Any]]:
    """
    Get a specific scene by ID.

    Args:
        scene_id: The scene UUID

    Returns:
        Scene dictionary or None if not found
    """
    with get_db() as conn:
        cursor = conn.execute(
            """
            SELECT id, job_id, scene_number, duration_seconds, description,
                   script, shot_type, transition, assets, metadata,
                   created_at, updated_at
            FROM job_scenes
            WHERE id = ?
            """,
            (scene_id,),
        )
        row = cursor.fetchone()

        if not row:
            return None

        return {
            "id": row["id"],
            "jobId": row["job_id"],
            "sceneNumber": row["scene_number"],
            "duration": row["duration_seconds"],
            "description": row["description"],
            "script": row["script"],
            "shotType": row["shot_type"],
            "transition": row["transition"],
            "assets": json.loads(row["assets"]) if row["assets"] else [],
            "metadata": json.loads(row["metadata"]) if row["metadata"] else {},
            "createdAt": row["created_at"],
            "updatedAt": row["updated_at"],
        }


def update_job_scene(
    scene_id: str,
    description: Optional[str] = None,
    script: Optional[str] = None,
    shot_type: Optional[str] = None,
    transition: Optional[str] = None,
    duration: Optional[float] = None,
    assets: Optional[List[str]] = None,
    metadata: Optional[Dict[str, Any]] = None,
) -> bool:
    """
    Update a scene record.

    Args:
        scene_id: The scene UUID
        description: Optional new description
        script: Optional new script
        shot_type: Optional new shot type
        transition: Optional new transition
        duration: Optional new duration
        assets: Optional new assets list
        metadata: Optional new metadata

    Returns:
        True if updated successfully
    """
    updates = []
    params = []

    if description is not None:
        updates.append("description = ?")
        params.append(description)
    if script is not None:
        updates.append("script = ?")
        params.append(script)
    if shot_type is not None:
        updates.append("shot_type = ?")
        params.append(shot_type)
    if transition is not None:
        updates.append("transition = ?")
        params.append(transition)
    if duration is not None:
        updates.append("duration_seconds = ?")
        params.append(duration)
    if assets is not None:
        updates.append("assets = ?")
        params.append(json.dumps(assets))
    if metadata is not None:
        updates.append("metadata = ?")
        params.append(json.dumps(metadata))

    if not updates:
        return False

    updates.append("updated_at = CURRENT_TIMESTAMP")
    params.append(scene_id)

    with get_db() as conn:
        cursor = conn.execute(
            f"UPDATE job_scenes SET {', '.join(updates)} WHERE id = ?", params
        )
        conn.commit()
        return cursor.rowcount > 0


def delete_job_scene(scene_id: str) -> bool:
    """
    Delete a scene record.

    Args:
        scene_id: The scene UUID

    Returns:
        True if deleted successfully
    """
    with get_db() as conn:
        cursor = conn.execute("DELETE FROM job_scenes WHERE id = ?", (scene_id,))
        conn.commit()
        return cursor.rowcount > 0


def delete_scenes_by_job(job_id: int) -> int:
    """
    Delete all scenes for a job.

    Args:
        job_id: The job ID

    Returns:
        Number of scenes deleted
    """
    with get_db() as conn:
        cursor = conn.execute("DELETE FROM job_scenes WHERE job_id = ?", (job_id,))
        conn.commit()
        return cursor.rowcount
</file>

<file path="backend/main.py">
from fastapi import (
    FastAPI,
    HTTPException,
    Query,
    BackgroundTasks,
    Depends,
    Response,
    UploadFile,
    File,
    Request,
    Form,
)
from contextlib import asynccontextmanager
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse, JSONResponse
from fastapi.security import OAuth2PasswordRequestForm
from pydantic import BaseModel, validator
from typing import Dict, Optional, List, Any, Union
from datetime import timedelta
from enum import Enum
import uvicorn
import os
import hashlib
import json
import requests
import asyncio
import logging
from dotenv import load_dotenv
from pathlib import Path

# Configure logging
logger = logging.getLogger(__name__)

# Import Asset Pydantic models
from .schemas.assets import (
    Asset,
    ImageAsset,
    VideoAsset,
    AudioAsset,
    DocumentAsset,
)

# Note: replicate package has Python 3.14 compatibility issues
# We only use HTTP API calls via requests library
replicate = None
REPLICATE_AVAILABLE = False

from .config import get_settings
from .database import (
    save_generated_scene,
    get_scene_by_id,
    list_scenes,
    get_scene_count,
    get_models_list,
    delete_scene,
    save_generated_video,
    update_video_status,
    get_video_by_id,
    save_generated_image,
    update_image_status,
    get_image_by_id,
    list_images,
    delete_image,
    save_generated_audio,
    update_audio_status,
    get_audio_by_id,
    list_audio,
    delete_audio,
    create_api_key,
    list_api_keys,
    revoke_api_key,
    # V2 workflow functions
    get_job,
    update_job_progress,
    approve_storyboard,
    # Asset management functions
    save_uploaded_asset,
    get_asset_by_id,
    list_user_assets,
    delete_asset,
    # Video export and refinement functions
    increment_download_count,
    get_download_count,
    refine_scene_in_storyboard,
    reorder_storyboard_scenes,
    get_refinement_count,
    increment_estimated_cost,
    # Client-based generation queries
    get_generated_images_by_client,
    get_generated_videos_by_client,
    # Campaign-based generation queries
    get_generated_images_by_campaign,
    get_generated_videos_by_campaign,
)

# Redis cache layer (optional, gracefully degrades if unavailable)
from .cache import (
    get_job_with_cache,
    update_job_progress_with_cache,
    invalidate_job_cache,
    get_cache_stats,
    redis_available,
)

from .auth import (
    verify_auth,
    get_current_admin_user,
    authenticate_user,
    create_access_token,
    generate_api_key,
    hash_api_key,
    ACCESS_TOKEN_EXPIRE_MINUTES,
)

# Load environment variables from .env file in parent directory
# Try loading .env from backend directory, then parent directory
if not load_dotenv(".env"):
    load_dotenv("../.env")
# import genesis as gs  # Using geometric validation instead

# Initialize centralized settings
settings = get_settings()

# Import limiter for rate limiting
from .prompt_parser_service.core.limiter import limiter
from slowapi.errors import RateLimitExceeded

# Import prompt parser service router
from .prompt_parser_service.api.v1 import parse as parse_api
from .prompt_parser_service.api.v1 import briefs as briefs_api

# Import clients and campaigns router
from .api_routes import router as clients_campaigns_router

# Import v3 API router
from .api.v3.router import router as v3_router

# Import Luigi workflow router
from .workflows.fastapi_integration import luigi_router

# Import logging
import logging

logger = logging.getLogger(__name__)

# Security: Allowed file extensions (prevents path traversal)
ALLOWED_FILE_EXTENSIONS = {
    "png",
    "jpg",
    "jpeg",
    "gif",
    "webp",
    "mp4",
    "mov",
    "mp3",
    "wav",
    "pdf",
}

# Magic bytes for file type validation
MAGIC_BYTES = {
    b"\x89PNG\r\n\x1a\n": "png",
    b"\xff\xd8\xff": "jpg",  # JPEG (various markers)
    b"GIF87a": "gif",
    b"GIF89a": "gif",
    b"RIFF": "webp",  # Also WAV, need to check further
    b"\x00\x00\x00\x18ftypmp42": "mp4",  # MP4
    b"\x00\x00\x00\x1cftypmp42": "mp4",
    b"\x00\x00\x00\x20ftypmp42": "mp4",
    b"\x00\x00\x00\x1cftypisom": "mp4",
    b"ID3": "mp3",
    b"\xff\xfb": "mp3",  # MP3 without ID3
    b"%PDF": "pdf",
}


def validate_and_sanitize_format(format_str: str) -> str:
    """
    Validate and sanitize file format to prevent path traversal attacks.

    Args:
        format_str: File format/extension to validate

    Returns:
        Sanitized format string

    Raises:
        HTTPException: If format is invalid or not allowed
    """
    if not format_str:
        raise HTTPException(status_code=400, detail="File format is required")

    # Remove dots and convert to lowercase
    format_clean = format_str.lower().strip().lstrip(".")

    # Check against whitelist
    if format_clean not in ALLOWED_FILE_EXTENSIONS:
        raise HTTPException(
            status_code=400,
            detail=f"Invalid file format: {format_clean}. Allowed: {', '.join(sorted(ALLOWED_FILE_EXTENSIONS))}",
        )

    return format_clean


def validate_file_type_with_magic_bytes(
    file_contents: bytes, claimed_type: str
) -> bool:
    """
    Validate file type using magic bytes to prevent file type spoofing.

    Args:
        file_contents: First bytes of the file
        claimed_type: The MIME type claimed by the client

    Returns:
        True if validation passes, False otherwise
    """
    # Extract first 32 bytes for magic byte checking
    header = file_contents[:32]

    # Log for debugging
    logger.info(
        f"Validating file type: claimed={claimed_type}, magic_bytes={header[:16].hex()}"
    )

    # Check common image formats
    if claimed_type.startswith("image/"):
        # PNG: starts with \x89PNG
        if header.startswith(b"\x89PNG"):
            return claimed_type in ["image/png", "image/x-png"]
        # JPEG: starts with \xff\xd8\xff
        elif header.startswith(b"\xff\xd8\xff"):
            return claimed_type in ["image/jpeg", "image/jpg", "image/pjpeg"]
        # GIF
        elif header.startswith(b"GIF87a") or header.startswith(b"GIF89a"):
            return claimed_type in ["image/gif"]
        # WebP: RIFF....WEBP
        elif header.startswith(b"RIFF") and b"WEBP" in header[:16]:
            return claimed_type in ["image/webp"]
        # SVG
        elif header.startswith(b"<?xml") or header.startswith(b"<svg"):
            return claimed_type in ["image/svg+xml", "image/svg"]
        else:
            # Log the actual bytes to help debug
            logger.warning(f"Unknown image magic bytes for claimed type {claimed_type}")
            logger.warning(f"  Hex: {header[:16].hex()}")
            logger.warning(f"  ASCII: {header[:16]}")
            return False

    # Check video formats
    elif claimed_type.startswith("video/"):
        if b"ftyp" in header[:32]:  # MP4/MOV container
            return claimed_type in ["video/mp4", "video/quicktime"]
        else:
            logger.warning(f"Unknown video magic bytes for claimed type {claimed_type}")
            return False

    # Check audio formats
    elif claimed_type.startswith("audio/"):
        if (
            header.startswith(b"ID3")
            or header.startswith(b"\xff\xfb")
            or header.startswith(b"\xff\xf3")
        ):
            return claimed_type in ["audio/mpeg", "audio/mp3"]
        elif header.startswith(b"RIFF") and b"WAVE" in header[:16]:
            return claimed_type in ["audio/wav", "audio/wave"]
        else:
            logger.warning(f"Unknown audio magic bytes for claimed type {claimed_type}")
            return False

    # Check document formats
    elif claimed_type == "application/pdf":
        if header.startswith(b"%PDF"):
            return True
        else:
            logger.warning(f"Invalid PDF magic bytes")
            return False

    # Unknown type
    logger.warning(f"Unrecognized content type for validation: {claimed_type}")
    return False


openapi_tags = [
    # V3 API - Primary API (organized logically)
    {
        "name": "v3-clients",
        "description": "🏢 V3 Client Management - Create and manage clients with brand guidelines",
    },
    {
        "name": "v3-campaigns",
        "description": "📢 V3 Campaign Management - Create and manage advertising campaigns",
    },
    {
        "name": "v3-assets",
        "description": "📁 V3 Asset Management - Upload and manage media assets (images, videos, documents)",
    },
    {
        "name": "v3-jobs",
        "description": "⚙️ V3 Job Management - Create and monitor video generation jobs with storyboards",
    },
    {
        "name": "v3-cost",
        "description": "💰 V3 Cost Estimation - Estimate costs before creating jobs",
    },
    # Legacy APIs
    {
        "name": "Core Entities",
        "description": "Client and campaign creation (decoupled from assets).",
    },
    {
        "name": "Asset Management",
        "description": "Upload and retrieve assets with client/campaign association.",
    },
    {
        "name": "clients-campaigns",
        "description": "Legacy client and campaign management endpoints.",
    },
    {"name": "creative", "description": "Creative brief parsing and management."},
    {
        "name": "Database",
        "description": "Database administration and inspection endpoints (admin only).",
    },
]


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Startup and shutdown events for the application."""
    # Startup: Recover orphaned jobs from previous shutdown
    logger.info("Checking for orphaned jobs from previous shutdown...")
    try:
        from .database import get_db
        from .workflows.runner import run_pipeline_async

        with get_db() as conn:
            orphaned = conn.execute("""
                SELECT id, parameters
                FROM generated_videos
                WHERE status IN ('image_pair_selection', 'sub_job_processing', 'sub_job_creation')
                AND created_at > datetime('now', '-2 hours')
            """).fetchall()

            if orphaned:
                logger.warning(f"Found {len(orphaned)} orphaned jobs, re-triggering...")

                for job in orphaned:
                    job_id = job['id']
                    try:
                        params = json.loads(job['parameters']) if isinstance(job['parameters'], str) else job['parameters']
                        campaign_id = params.get('campaign_id')
                        clip_duration = params.get('clip_duration')
                        num_pairs = params.get('num_pairs')

                        logger.info(f"Recovering job {job_id} for campaign {campaign_id}")

                        # Re-trigger in background (don't block startup)
                        asyncio.create_task(run_pipeline_async(
                            job_id=job_id,
                            campaign_id=campaign_id,
                            clip_duration=clip_duration,
                            num_pairs=num_pairs,
                            use_local_scheduler=False,  # Use central scheduler for Luigi web UI visibility
                        ))
                    except Exception as e:
                        logger.error(f"Failed to recover job {job_id}: {e}")
            else:
                logger.info("No orphaned jobs found")
    except Exception as e:
        logger.error(f"Error during startup recovery: {e}")

    yield  # Application runs here

    # Shutdown: Log warning about running pipelines
    logger.info("Shutting down - any running pipelines will be recovered on next startup")


app = FastAPI(
    title="Physics Simulator API",
    version="1.0.0",
    openapi_tags=openapi_tags,
    lifespan=lifespan
)

# CORS middleware (for development)
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:5173",
        "http://127.0.0.1:5173",
        "http://localhost:5175",  # Alternative Vite port
        "http://127.0.0.1:5175",
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# Add rate limiting
@app.exception_handler(RateLimitExceeded)
async def rate_limit_handler(request, exc):
    return JSONResponse({"detail": "Too many requests"}, status_code=429)


app.state.limiter = limiter

# Check if static files exist (production mode)
STATIC_DIR = Path(__file__).parent.parent / "static"
if STATIC_DIR.exists() and STATIC_DIR.is_dir():
    # Mount static files
    app.mount(
        "/assets", StaticFiles(directory=str(STATIC_DIR / "assets")), name="assets"
    )


# Pydantic models
class Vec3(BaseModel):
    x: float
    y: float
    z: float


class Transform(BaseModel):
    position: Vec3
    rotation: Vec3
    scale: Vec3


class PhysicsProperties(BaseModel):
    mass: float
    friction: float
    restitution: float


class VisualProperties(BaseModel):
    color: str
    shape: str  # "Box", "Sphere", "Cylinder"


class PhysicsObject(BaseModel):
    id: str
    transform: Transform
    physicsProperties: PhysicsProperties
    visualProperties: VisualProperties
    description: Optional[str] = None  # Text description for LLM semantic augmentation


class Scene(BaseModel):
    objects: Dict[str, PhysicsObject]
    selectedObject: Optional[str] = None


class GenerateRequest(BaseModel):
    prompt: str
    brief_id: Optional[str] = None  # Optional link to creative brief


class ValidationResult(BaseModel):
    valid: bool
    message: str
    details: Optional[Dict] = None


# AI client initialization
if settings.REPLICATE_API_KEY:
    ai_client = {
        "api_key": settings.REPLICATE_API_KEY,
        "base_url": "https://api.replicate.com/v1",
    }
    replicate_client = (
        replicate.Client(api_token=settings.REPLICATE_API_KEY)
        if REPLICATE_AVAILABLE and replicate
        else None
    )
    print("AI client initialized with Replicate")
else:
    ai_client = None
    replicate_client = None
    print("Warning: Using demo scene generation (REPLICATE_API_KEY not set)")

# Demo video models for fallback
DEMO_VIDEO_MODELS = [
    {
        "id": "demo/video-1",
        "name": "Demo Text-to-Video",
        "description": "Generates video from text prompt (demo mode)",
        "input_schema": None,
    },
    {
        "id": "demo/video-2",
        "name": "Demo Image-to-Video",
        "description": "Generates video from image and prompt (demo mode)",
        "input_schema": None,
    },
]

# Simple in-memory cache (replace with LMDB later)
scene_cache = {}


@app.get("/health")
async def health_check():
    return {"status": "healthy"}


@app.get("/api")
async def api_root():
    return {"message": "Physics Simulator API", "status": "running"}


@app.get("/api/v2/cache/stats")
async def cache_statistics():
    """
    Get cache performance statistics (SQLite-based for POC).

    Returns cache stats including active entries and TTL configuration.
    This endpoint is public (no auth required) for monitoring purposes.
    """
    stats = get_cache_stats()
    return {
        "cache_enabled": True,  # SQLite cache is always available
        "cache_type": "sqlite",
        "statistics": stats,
        "message": "SQLite cache is working normally",
    }


# ============================================================================
# Authentication Endpoints
# ============================================================================


class LoginResponse(BaseModel):
    access_token: str
    token_type: str
    expires_in: int


class CreateAPIKeyRequest(BaseModel):
    name: str
    expires_days: Optional[int] = None


class APIKeyResponse(BaseModel):
    api_key: str  # Only returned on creation
    name: str
    created_at: str


class APIKeyListItem(BaseModel):
    id: int
    name: str
    is_active: bool
    created_at: str
    last_used: Optional[str]
    expires_at: Optional[str]


@app.post("/api/auth/login")
async def login(
    form_data: OAuth2PasswordRequestForm = Depends(), response: Response = None
):
    """Login with username and password. Sets HTTP-only cookie."""
    from fastapi import Response

    user = authenticate_user(form_data.username, form_data.password)
    if not user:
        raise HTTPException(
            status_code=401,
            detail="Incorrect username or password",
            headers={"WWW-Authenticate": "Bearer"},
        )

    access_token_expires = timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    access_token = create_access_token(
        data={"sub": user["username"]}, expires_delta=access_token_expires
    )

    # Create response
    if response is None:
        from fastapi import Response

        response = Response()

    # Set HTTP-only cookie
    response.set_cookie(
        key="access_token",
        value=access_token,
        httponly=True,
        secure=os.getenv("ENVIRONMENT") == "production",  # HTTPS only in production
        samesite="lax",
        max_age=ACCESS_TOKEN_EXPIRE_MINUTES * 60,
        path="/",
    )

    return {"message": "Login successful", "username": user["username"]}


@app.post("/api/auth/logout")
async def logout(response: Response = None):
    """Logout by clearing the authentication cookie."""
    from fastapi import Response

    if response is None:
        response = Response()

    # Clear the cookie
    response.delete_cookie(key="access_token", path="/")

    return {"message": "Logout successful"}


@app.post("/api/auth/api-keys", response_model=APIKeyResponse)
async def create_new_api_key(
    request: CreateAPIKeyRequest, current_user: Dict = Depends(verify_auth)
):
    """Create a new API key for the authenticated user."""
    from datetime import datetime

    # Generate API key
    api_key = generate_api_key()
    key_hash = hash_api_key(api_key)

    # Calculate expiration
    expires_at = None
    if request.expires_days:
        expires_at = (
            datetime.utcnow() + timedelta(days=request.expires_days)
        ).isoformat()

    # Save to database
    key_id = create_api_key(
        key_hash=key_hash,
        name=request.name,
        user_id=current_user["id"],
        expires_at=expires_at,
    )

    return {
        "api_key": api_key,  # Only shown once!
        "name": request.name,
        "created_at": datetime.utcnow().isoformat(),
    }


@app.get("/api/auth/api-keys", response_model=List[APIKeyListItem])
async def get_api_keys(current_user: Dict = Depends(verify_auth)):
    """List all API keys for the authenticated user."""
    keys = list_api_keys(current_user["id"])
    return keys


@app.delete("/api/auth/api-keys/{key_id}")
async def revoke_api_key_endpoint(
    key_id: int, current_user: Dict = Depends(verify_auth)
):
    """Revoke an API key."""
    success = revoke_api_key(key_id, current_user["id"])
    if not success:
        raise HTTPException(status_code=404, detail="API key not found")
    return {"message": "API key revoked successfully"}


# ============================================================================
# Scene Generation Endpoints
# ============================================================================


def generate_scene(prompt: str) -> Scene:
    """Generate a physics scene from a text prompt using AI."""
    print(f"ai_client is None: {ai_client is None}")
    # For demo purposes, return a simple test scene if AI client is not configured
    if not ai_client:
        print("Warning: Using demo scene generation (AI client not configured)")
        return create_demo_scene(prompt)

    # Check cache first
    cache_key = hashlib.sha256(prompt.encode()).hexdigest()
    if cache_key in scene_cache:
        try:
            return Scene.parse_raw(scene_cache[cache_key])
        except Exception:
            pass  # Cache corrupted, regenerate

    # Create prompt template
    system_prompt = """You are a physics scene generator. Create realistic 3D physics scenes based on text descriptions.

Generate scenes with 2-8 objects that can interact physically. Each object should have:
- Realistic physics properties (mass, friction, restitution)
- Appropriate visual properties (color, shape)
- Sensible initial positions and orientations

Supported shapes: "Box", "Sphere", "Cylinder"
Colors should be hex codes like "#ff0000" for red

Return ONLY valid JSON matching this schema:
{
  "objects": {
    "object_id": {
      "id": "object_id",
      "transform": {
        "position": {"x": float, "y": float, "z": float},
        "rotation": {"x": float, "y": float, "z": float},
        "scale": {"x": float, "y": float, "z": float}
      },
      "physicsProperties": {
        "mass": float,
        "friction": float,
        "restitution": float
      },
      "visualProperties": {
        "color": "hex_color",
        "shape": "Box|Sphere|Cylinder"
      }
    }
  }
}

Make scenes physically realistic and interesting to simulate."""

    user_prompt = f"Generate a physics scene for: {prompt}"

    try:
        # Use Claude via Replicate HTTP API
        headers = {
            "Authorization": f"Bearer {ai_client['api_key']}",
            "Content-Type": "application/json",
        }

        payload = {
            "input": {
                "prompt": f"{system_prompt}\n\n{user_prompt}",
                "max_tokens": 2000,
                "temperature": 0.7,
            }
        }

        response = requests.post(
            "https://api.replicate.com/v1/models/anthropic/claude-3.5-sonnet/predictions",
            headers=headers,
            json=payload,
            timeout=60,
        )
        # Log the response for debugging
        print(f"Replicate API response status: {response.status_code}")
        if response.status_code != 200 and response.status_code != 201:
            print(f"Replicate API error response: {response.text}")

        response.raise_for_status()

        result = response.json()

        # Wait for the prediction to complete
        prediction_url = result.get("urls", {}).get("get")
        if not prediction_url:
            print(f"Error: No prediction URL in response: {result}")
            raise HTTPException(status_code=500, detail="No prediction URL returned")

        print(f"Polling prediction at: {prediction_url}")

        # Poll for completion
        import time

        max_attempts = 120  # Increased timeout for Claude
        for attempt in range(max_attempts):
            pred_response = requests.get(prediction_url, headers=headers)
            pred_response.raise_for_status()
            pred_data = pred_response.json()

            status = pred_data.get("status")
            print(f"Attempt {attempt + 1}/{max_attempts}: Status = {status}")

            if status == "succeeded":
                output = pred_data.get("output")
                print(f"Raw output type: {type(output)}")
                print(f"Raw output: {output}")

                if isinstance(output, list):
                    scene_json = "".join(output).strip()
                elif isinstance(output, str):
                    scene_json = output.strip()
                else:
                    print(f"Unexpected output type: {type(output)}, value: {output}")
                    raise HTTPException(
                        status_code=500,
                        detail=f"Unexpected output format: {type(output)}",
                    )

                print(f"Scene JSON (first 200 chars): {scene_json[:200]}")
                break
            elif status in ["failed", "canceled"]:
                error = pred_data.get("error", "Unknown error")
                print(f"Prediction failed: {error}")
                raise HTTPException(
                    status_code=500, detail=f"Prediction failed: {error}"
                )

            time.sleep(2)  # Poll every 2 seconds
        else:
            print(f"Prediction timed out after {max_attempts} attempts")
            raise HTTPException(status_code=500, detail="Prediction timed out")

        # Clean up JSON response (remove markdown code blocks if present)
        if scene_json.startswith("```json"):
            scene_json = scene_json[7:]
        if scene_json.endswith("```"):
            scene_json = scene_json[:-3]
        scene_json = scene_json.strip()

        # Parse and validate the scene
        scene_data = json.loads(scene_json)
        scene = Scene(**scene_data)

        # Skip validation for now - it's too strict
        # validation = validate_with_genesis(scene)
        # if not validation.valid:
        #     raise HTTPException(
        #         status_code=400,
        #         detail=f"Generated scene is not stable: {validation.message}"
        #     )

        # Cache the result
        scene_cache[cache_key] = scene.json()

        return scene

    except json.JSONDecodeError as e:
        raise HTTPException(
            status_code=500, detail=f"Invalid JSON response from AI: {str(e)}"
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"AI generation failed: {str(e)}")


def create_demo_scene(prompt: str) -> Scene:
    """Create a demo scene for testing when AI is not available."""
    # Create a simple demo scene with a few objects
    objects = {
        "box1": PhysicsObject(
            id="box1",
            transform=Transform(
                position=Vec3(x=0, y=5, z=0),
                rotation=Vec3(x=0, y=0, z=0),
                scale=Vec3(x=1, y=1, z=1),
            ),
            physicsProperties=PhysicsProperties(
                mass=1.0, friction=0.5, restitution=0.3
            ),
            visualProperties=VisualProperties(color="#ff0000", shape="Box"),
        ),
        "sphere1": PhysicsObject(
            id="sphere1",
            transform=Transform(
                position=Vec3(x=2, y=8, z=0),
                rotation=Vec3(x=0, y=0, z=0),
                scale=Vec3(x=1, y=1, z=1),
            ),
            physicsProperties=PhysicsProperties(
                mass=0.5, friction=0.2, restitution=0.8
            ),
            visualProperties=VisualProperties(color="#0000ff", shape="Sphere"),
        ),
        "ground": PhysicsObject(
            id="ground",
            transform=Transform(
                position=Vec3(x=0, y=-0.5, z=0),
                rotation=Vec3(x=0, y=0, z=0),
                scale=Vec3(x=10, y=1, z=10),
            ),
            physicsProperties=PhysicsProperties(
                mass=0.0,  # Static ground
                friction=0.8,
                restitution=0.1,
            ),
            visualProperties=VisualProperties(color="#888888", shape="Box"),
        ),
    }

    return Scene(objects=objects)


def validate_with_genesis(scene: Scene) -> ValidationResult:
    """Validate scene stability using geometric analysis."""
    try:
        # Check for unsupported shapes
        for obj_id, obj in scene.objects.items():
            if obj.visualProperties.shape not in ["Box", "Sphere"]:
                return ValidationResult(
                    valid=False,
                    message=f"Unsupported shape: {obj.visualProperties.shape}",
                    details={"unsupported_shape": obj.visualProperties.shape},
                )

        # Check for overlapping objects (simple geometric validation)
        overlapping_pairs = []
        objects_list = list(scene.objects.items())

        for i, (id1, obj1) in enumerate(objects_list):
            for j, (id2, obj2) in enumerate(objects_list[i + 1 :], i + 1):
                # Skip ground objects (mass = 0)
                if obj1.physicsProperties.mass == 0 or obj2.physicsProperties.mass == 0:
                    continue

                # Calculate distance between centers
                dx = obj1.transform.position.x - obj2.transform.position.x
                dy = obj1.transform.position.y - obj2.transform.position.y
                dz = obj1.transform.position.z - obj2.transform.position.z
                distance = (dx**2 + dy**2 + dz**2) ** 0.5

                # Calculate minimum separation needed
                if (
                    obj1.visualProperties.shape == "Box"
                    and obj2.visualProperties.shape == "Box"
                ):
                    # Box-box collision: check if bounding boxes overlap
                    min_sep_x = (obj1.transform.scale.x + obj2.transform.scale.x) / 2
                    min_sep_y = (obj1.transform.scale.y + obj2.transform.scale.y) / 2
                    min_sep_z = (obj1.transform.scale.z + obj2.transform.scale.z) / 2

                    if (
                        abs(dx) < min_sep_x
                        and abs(dy) < min_sep_y
                        and abs(dz) < min_sep_z
                    ):
                        overlapping_pairs.append((id1, id2))

                elif (
                    obj1.visualProperties.shape == "Sphere"
                    and obj2.visualProperties.shape == "Sphere"
                ):
                    # Sphere-sphere collision
                    min_distance = (
                        obj1.transform.scale.x + obj2.transform.scale.x
                    ) / 2  # Assume uniform scale
                    if distance < min_distance:
                        overlapping_pairs.append((id1, id2))

                else:
                    # Mixed sphere-box: approximate with sphere radius
                    sphere_obj = (
                        obj1 if obj1.visualProperties.shape == "Sphere" else obj2
                    )
                    box_obj = obj2 if obj1.visualProperties.shape == "Sphere" else obj1

                    sphere_radius = sphere_obj.transform.scale.x / 2
                    box_half_size = (
                        max(
                            box_obj.transform.scale.x,
                            box_obj.transform.scale.y,
                            box_obj.transform.scale.z,
                        )
                        / 2
                    )

                    min_distance = sphere_radius + box_half_size
                    if distance < min_distance:
                        overlapping_pairs.append((id1, id2))

        # Check for objects too high (likely to fall unstably)
        high_objects = []
        for obj_id, obj in scene.objects.items():
            if obj.physicsProperties.mass > 0 and obj.transform.position.y > 5.0:
                high_objects.append(obj_id)

        # Validate results
        issues = []
        if overlapping_pairs:
            issues.append(f"Overlapping objects: {overlapping_pairs}")
        if high_objects:
            issues.append(f"Objects too high (unstable): {high_objects}")

        if issues:
            return ValidationResult(
                valid=False,
                message="Scene has stability issues: " + "; ".join(issues),
                details={
                    "overlapping_pairs": overlapping_pairs,
                    "high_objects": high_objects,
                    "max_height_threshold": 5.0,
                },
            )
        else:
            return ValidationResult(
                valid=True,
                message="Scene appears geometrically stable",
                details={"checked_objects": len(scene.objects)},
            )

    except Exception as e:
        return ValidationResult(
            valid=False,
            message=f"Validation failed: {str(e)}",
            details={"error": str(e)},
        )


@app.post("/api/generate")
async def api_generate_scene(
    request: GenerateRequest, current_user: Dict = Depends(verify_auth)
):
    """Generate a physics scene from a text prompt. Optionally links to creative brief. Requires authentication."""
    try:
        # If brief_id is provided, validate ownership and use brief context
        brief_context = None
        if request.brief_id:
            from .database import get_creative_brief

            brief = get_creative_brief(request.brief_id, current_user["id"])
            if not brief:
                raise HTTPException(
                    status_code=404, detail="Brief not found or access denied"
                )
            brief_context = brief

        scene = generate_scene(request.prompt)
        scene_dict = scene.dict()

        # Save to database with brief linkage
        metadata = {"source": "generate", "user_id": current_user["id"]}
        if request.brief_id:
            metadata["brief_id"] = request.brief_id

        scene_id = save_generated_scene(
            prompt=request.prompt,
            scene_data=scene_dict,
            model="claude-3.5-sonnet",
            metadata=metadata,
        )

        # Add scene_id to response
        scene_dict["_id"] = scene_id
        scene_dict["_brief_id"] = request.brief_id

        return scene_dict
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Scene generation failed: {str(e)}"
        )


@app.post("/api/validate")
async def api_validate_scene(scene: Scene, current_user: Dict = Depends(verify_auth)):
    """Validate a physics scene for stability using Genesis simulation. Requires authentication."""
    try:
        result = validate_with_genesis(scene)
        return result.dict()
    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Scene validation failed: {str(e)}"
        )


class RefineRequest(BaseModel):
    scene: Scene
    prompt: str


class VideoModel(BaseModel):
    id: str
    name: str
    description: str
    input_schema: Optional[Dict] = None


class RunVideoRequest(BaseModel):
    model_id: str
    input: Dict[str, Any]  # Accepts strings, numbers, bools, etc.
    collection: Optional[str] = None
    version: Optional[str] = None  # Model version ID for reliable predictions
    brief_id: Optional[str] = None  # Link to creative brief for context


class RunImageRequest(BaseModel):
    model_id: str
    input: Dict[str, Any]  # Accepts strings, numbers, bools, etc.
    collection: Optional[str] = None
    version: Optional[str] = None  # Model version ID for reliable predictions
    brief_id: Optional[str] = None  # Link to creative brief for context


class RunAudioRequest(BaseModel):
    model_id: str
    input: Dict[str, Any]  # Accepts strings, numbers, bools, etc.
    collection: Optional[str] = None
    version: Optional[str] = None  # Model version ID for reliable predictions
    brief_id: Optional[str] = None  # Link to creative brief for context


class ImageGenerationRequest(BaseModel):
    prompt: Optional[str] = None
    asset_id: Optional[str] = None  # Reference to uploaded asset
    image_id: Optional[int] = None  # Reference to generated image
    video_id: Optional[int] = None  # Reference to generated video (use thumbnail)
    client_id: Optional[str] = None  # For tracking ownership
    campaign_id: str  # Required - link to campaign

    @validator("prompt", "asset_id", "image_id", "video_id", always=True)
    def check_at_least_one(cls, v, values):
        # Check if at least one of the required fields is provided
        has_prompt = values.get("prompt") or v
        has_asset = values.get("asset_id")
        has_image = values.get("image_id")
        has_video = values.get("video_id")

        if not any([has_prompt, has_asset, has_image, has_video]):
            raise ValueError(
                "At least one of prompt, asset_id, image_id, or video_id must be provided"
            )
        return v


class VideoModel(str, Enum):
    SEEDANCE = "bytedance/seedance-1-lite"
    KLING = "kwaivgi/kling-v2.1"


class AudioModel(str, Enum):
    MUSICGEN = "meta/musicgen"
    RIFFUSION = "riffusion/riffusion"


class VideoGenerationRequest(BaseModel):
    prompt: Optional[str] = None
    asset_id: Optional[str] = None  # Reference to uploaded asset
    image_id: Optional[int] = None  # Reference to generated image
    video_id: Optional[int] = None  # Reference to generated video (use thumbnail)
    client_id: Optional[str] = None  # For tracking ownership
    campaign_id: str  # Required - link to campaign
    model: VideoModel = VideoModel.SEEDANCE  # Default to seedance-1-lite

    @validator("prompt", "asset_id", "image_id", "video_id", always=True)
    def check_at_least_one(cls, v, values):
        # Check if at least one of the required fields is provided
        has_prompt = values.get("prompt") or v
        has_asset = values.get("asset_id")
        has_image = values.get("image_id")
        has_video = values.get("video_id")

        if not any([has_prompt, has_asset, has_image, has_video]):
            raise ValueError(
                "At least one of prompt, asset_id, image_id, or video_id must be provided"
            )
        return v


class AudioGenerationRequest(BaseModel):
    prompt: str  # Required for audio generation
    client_id: Optional[str] = None  # For tracking ownership
    campaign_id: str  # Required - link to campaign
    model: AudioModel = AudioModel.MUSICGEN  # Default to MusicGen
    duration: Optional[int] = 8  # Duration in seconds (default 8)


class GenesisRenderRequest(BaseModel):
    scene: Scene
    duration: float = 5.0
    fps: int = 60
    resolution: tuple[int, int] = (1920, 1080)
    quality: str = "high"  # "draft", "high", "ultra"
    camera_config: Optional[Dict] = None
    scene_context: Optional[str] = None


def refine_scene(scene: Scene, prompt: str) -> Scene:
    """Refine an existing physics scene based on a text prompt using AI."""
    print(f"Refining scene with prompt: {prompt}")
    # For demo purposes, return the original scene if AI client is not configured
    if not ai_client:
        print("Warning: Using demo scene refinement (AI client not configured)")
        return scene

    # Create cache key from scene and prompt
    scene_str = scene.json()
    cache_key = hashlib.sha256(f"{scene_str}:{prompt}".encode()).hexdigest()

    if cache_key in scene_cache:
        try:
            return Scene.parse_raw(scene_cache[cache_key])
        except Exception:
            pass  # Cache corrupted, regenerate

    # Create prompt template for refinement
    system_prompt = """You are a physics scene refiner. Modify existing 3D physics scenes based on text instructions.

Given an existing scene JSON and a refinement prompt, modify the scene accordingly. You can:
- Change object colors, positions, scales, rotations
- Add new objects
- Remove objects
- Modify physics properties (mass, friction, restitution)
- Change object shapes

Return ONLY valid JSON matching the scene schema. Preserve the structure and only make the requested changes.

Scene schema:
{
  "objects": {
    "object_id": {
      "id": "object_id",
      "transform": {
        "position": {"x": float, "y": float, "z": float},
        "rotation": {"x": float, "y": float, "z": float},
        "scale": {"x": float, "y": float, "z": float}
      },
      "physicsProperties": {
        "mass": float,
        "friction": float,
        "restitution": float
      },
      "visualProperties": {
        "color": "hex_color",
        "shape": "Box|Sphere|Cylinder"
      }
    }
  }
}

Make minimal, targeted changes based on the prompt."""

    user_prompt = f"Original scene: {scene_str}\n\nRefinement request: {prompt}\n\nReturn the modified scene JSON:"

    try:
        # Use Claude via Replicate HTTP API
        headers = {
            "Authorization": f"Bearer {ai_client['api_key']}",
            "Content-Type": "application/json",
        }

        payload = {
            "input": {
                "prompt": f"{system_prompt}\n\n{user_prompt}",
                "max_tokens": 2000,
                "temperature": 0.7,
            }
        }

        response = requests.post(
            "https://api.replicate.com/v1/models/anthropic/claude-3.5-sonnet/predictions",
            headers=headers,
            json=payload,
            timeout=60,
        )
        # Log the response for debugging
        print(f"Replicate API response status: {response.status_code}")
        if response.status_code != 200 and response.status_code != 201:
            print(f"Replicate API error response: {response.text}")

        response.raise_for_status()

        result = response.json()

        # Wait for the prediction to complete
        prediction_url = result.get("urls", {}).get("get")
        if not prediction_url:
            raise HTTPException(status_code=500, detail="No prediction URL returned")

        # Poll for completion
        import time

        max_attempts = 120  # Increased timeout for Claude
        for attempt in range(max_attempts):
            pred_response = requests.get(prediction_url, headers=headers)
            pred_response.raise_for_status()
            pred_data = pred_response.json()

            status = pred_data.get("status")
            print(f"Refine attempt {attempt + 1}/{max_attempts}: Status = {status}")

            if status == "succeeded":
                output = pred_data.get("output")
                print(f"Raw output type: {type(output)}")

                if isinstance(output, list):
                    refined_scene_json = "".join(output).strip()
                elif isinstance(output, str):
                    refined_scene_json = output.strip()
                else:
                    print(f"Unexpected output type: {type(output)}, value: {output}")
                    raise HTTPException(
                        status_code=500,
                        detail=f"Unexpected output format: {type(output)}",
                    )

                print(
                    f"Refined scene JSON (first 200 chars): {refined_scene_json[:200]}"
                )
                break
            elif status in ["failed", "canceled"]:
                error = pred_data.get("error", "Unknown error")
                print(f"Prediction failed: {error}")
                raise HTTPException(
                    status_code=500, detail=f"Prediction failed: {error}"
                )

            time.sleep(2)  # Poll every 2 seconds
        else:
            print(f"Prediction timed out after {max_attempts} attempts")
            raise HTTPException(status_code=500, detail="Prediction timed out")

        # Clean up JSON response
        if refined_scene_json.startswith("```json"):
            refined_scene_json = refined_scene_json[7:]
        if refined_scene_json.endswith("```"):
            refined_scene_json = refined_scene_json[:-3]
        refined_scene_json = refined_scene_json.strip()

        # Parse and validate the refined scene
        refined_scene_data = json.loads(refined_scene_json)
        refined_scene = Scene(**refined_scene_data)

        # Skip validation for now - it's too strict
        # validation = validate_with_genesis(refined_scene)
        # if not validation.valid:
        #     raise HTTPException(
        #         status_code=400,
        #         detail=f"Refined scene is not stable: {validation.message}"
        #     )

        # Cache the result
        scene_cache[cache_key] = refined_scene.json()

        return refined_scene

    except json.JSONDecodeError as e:
        raise HTTPException(
            status_code=500, detail=f"Invalid JSON response from AI: {str(e)}"
        )
    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Scene refinement failed: {str(e)}"
        )


@app.post("/api/refine")
async def api_refine_scene(
    request: RefineRequest, current_user: Dict = Depends(verify_auth)
):
    """Refine an existing physics scene based on a text prompt. Requires authentication."""
    try:
        refined_scene = refine_scene(request.scene, request.prompt)
        return refined_scene.dict()
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Scene refinement failed: {str(e)}"
        )


# Scene history endpoints
@app.get("/api/scenes")
async def api_list_scenes(
    limit: int = Query(50, ge=1, le=100),
    offset: int = Query(0, ge=0),
    model: Optional[str] = Query(None),
    current_user: Dict = Depends(verify_auth),
):
    """List generated scenes with pagination and optional model filter. Requires authentication."""
    try:
        scenes = list_scenes(limit=limit, offset=offset, model=model)
        total = get_scene_count(model=model)
        return {"scenes": scenes, "total": total, "limit": limit, "offset": offset}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to list scenes: {str(e)}")


@app.get("/api/scenes/{scene_id}")
async def api_get_scene(scene_id: int, current_user: Dict = Depends(verify_auth)):
    """Get a specific scene by ID. Requires authentication."""
    try:
        scene = get_scene_by_id(scene_id)
        if not scene:
            raise HTTPException(status_code=404, detail="Scene not found")
        return scene
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get scene: {str(e)}")


@app.delete("/api/scenes/{scene_id}")
async def api_delete_scene(scene_id: int, current_user: Dict = Depends(verify_auth)):
    """Delete a scene by ID. Requires authentication."""
    try:
        deleted = delete_scene(scene_id)
        if not deleted:
            raise HTTPException(status_code=404, detail="Scene not found")
        return {"success": True, "message": "Scene deleted"}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to delete scene: {str(e)}")


@app.get("/api/models")
async def api_get_models():
    """Get list of models that have generated scenes."""
    try:
        models = get_models_list()
        return {"models": models}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get models: {str(e)}")


@app.get("/api/replicate-models")
async def api_get_replicate_models(
    query: Optional[str] = Query(None, description="Search query"),
    cursor: Optional[str] = Query(None, description="Pagination cursor"),
):
    """Get list of available models from Replicate."""
    try:
        if not ai_client:
            return {"results": DEMO_VIDEO_MODELS, "next": None}

        headers = {
            "Authorization": f"Bearer {ai_client['api_key']}",
            "Content-Type": "application/json",
        }

        # Build URL with query params
        params = []
        if cursor:
            params.append(f"cursor={cursor}")
        if query:
            params.append(f"query={query}")

        # Use Replicate's models API
        url = "https://api.replicate.com/v1/models"
        if params:
            url += "?" + "&".join(params)

        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        data = response.json()

        # Format the response
        models = []
        results = data.get("results", [])

        for model_data in results:
            models.append(
                {
                    "owner": model_data.get("owner"),
                    "name": model_data.get("name"),
                    "description": model_data.get("description"),
                    "url": model_data.get("url"),
                    "cover_image_url": model_data.get("cover_image_url"),
                    "latest_version": model_data.get("latest_version", {}).get("id")
                    if model_data.get("latest_version")
                    else None,
                    "run_count": model_data.get("run_count", 0),
                }
            )

        return {"results": models, "next": data.get("next")}

    except Exception as e:
        print(f"Error fetching models from Replicate: {str(e)}")
        import traceback

        traceback.print_exc()
        # Fallback to demo models
        return {"results": DEMO_VIDEO_MODELS, "next": None}


@app.get("/api/video-models")
async def api_get_video_models(
    collection: Optional[str] = Query(
        "text-to-video",
        description="Collection slug: text-to-video, image-to-video, etc.",
    ),
):
    """Get video generation models from Replicate collections API."""
    try:
        if not ai_client:
            # Fallback to demo models if no API key
            return {"models": [model for model in DEMO_VIDEO_MODELS]}

        headers = {
            "Authorization": f"Bearer {ai_client['api_key']}",
            "Content-Type": "application/json",
        }

        # Use collections API with the specified collection slug
        url = f"https://api.replicate.com/v1/collections/{collection}"
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        data = response.json()

        # Format the models from the collection
        models = []
        for model_data in data.get("models", []):
            model_id = f"{model_data.get('owner')}/{model_data.get('name')}"
            models.append(
                {
                    "id": model_id,
                    "name": model_data.get("name", ""),
                    "owner": model_data.get("owner", ""),
                    "description": model_data.get("description"),
                    "cover_image_url": model_data.get("cover_image_url"),
                    "latest_version": model_data.get("latest_version", {}).get("id")
                    if model_data.get("latest_version")
                    else None,
                    "run_count": model_data.get("run_count", 0),
                    "input_schema": None,  # Will be fetched when model is selected
                }
            )

        return {"models": models}
    except Exception as e:
        print(f"Error fetching video models from collection '{collection}': {str(e)}")
        import traceback

        traceback.print_exc()
        # Fallback to demo models
        return {"models": [model for model in DEMO_VIDEO_MODELS]}


@app.get("/api/video-models/{model_owner}/{model_name}/schema")
async def api_get_model_schema(model_owner: str, model_name: str):
    """Get the input schema for a specific model."""
    try:
        if not ai_client:
            return {"input_schema": {"prompt": {"type": "string"}}}

        headers = {
            "Authorization": f"Bearer {ai_client['api_key']}",
            "Content-Type": "application/json",
        }

        # Fetch model details including schema
        url = f"https://api.replicate.com/v1/models/{model_owner}/{model_name}"
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        data = response.json()

        # Extract input schema from latest version
        latest_version = data.get("latest_version") or {}
        version_id = latest_version.get("id")
        openapi_schema = latest_version.get("openapi_schema") or {}
        input_schema = (
            openapi_schema.get("components", {}).get("schemas", {}).get("Input", {})
        )

        # Extract properties and required fields
        properties = input_schema.get("properties", {})
        required_fields = input_schema.get("required", [])

        # Return schema with version ID for reliable predictions
        return {
            "input_schema": properties,
            "required": required_fields,
            "version": version_id,  # Include version ID for predictions
        }
    except Exception as e:
        print(f"Error fetching model schema: {str(e)}")
        import traceback

        traceback.print_exc()
        return {"input_schema": {"prompt": {"type": "string"}}}


def process_video_to_text_background(
    video_id: int,
    prediction_url: str,
    api_key: str,
    model_id: str,
    input_params: dict,
    collection: str,
):
    """Background task to poll Replicate for video-to-text completion."""
    import time

    headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}

    max_attempts = 120  # 4 minutes (2 seconds * 120)

    try:
        for attempt in range(max_attempts):
            pred_response = requests.get(prediction_url, headers=headers)
            pred_response.raise_for_status()
            pred_data = pred_response.json()

            status = pred_data.get("status")

            if status == "succeeded":
                # For video-to-text, output is text, not a URL
                output = pred_data.get("output", "")
                if isinstance(output, list):
                    output = output[0] if output else ""

                if output:
                    # Store text output in metadata
                    metadata = {
                        "replicate_id": pred_data.get("id"),
                        "prediction_url": prediction_url,
                        "text_output": output,
                    }

                    # Update database with completed text generation
                    update_video_status(
                        video_id=video_id,
                        status="completed",
                        video_url="",  # No video URL for text output
                        metadata=metadata,
                    )
                    print(
                        f"Video-to-text {video_id} completed successfully: {output[:100]}..."
                    )
                    return
                else:
                    # No text output in response
                    metadata = {
                        "replicate_id": pred_data.get("id"),
                        "prediction_url": prediction_url,
                        "error": "No text output in Replicate response",
                    }
                    update_video_status(
                        video_id=video_id, status="failed", metadata=metadata
                    )
                    print(f"Video-to-text {video_id} failed: no output")
                    return

            elif status in ["failed", "canceled"]:
                error = pred_data.get("error", "Unknown error")
                metadata = {"error": error, "replicate_id": pred_data.get("id")}

                # Update database with failure
                update_video_status(video_id=video_id, status=status, metadata=metadata)
                print(f"Video-to-text {video_id} {status}: {error}")
                return

            time.sleep(2)

        # Timed out waiting for completion
        print(f"Video-to-text {video_id} timed out after {max_attempts * 2} seconds")
        update_video_status(
            video_id=video_id,
            status="failed",
            metadata={"error": "Timeout waiting for Replicate completion"},
        )

    except Exception as e:
        print(f"Error polling video-to-text {video_id}: {str(e)}")
        import traceback

        traceback.print_exc()
        update_video_status(
            video_id=video_id,
            status="failed",
            metadata={"error": f"Polling error: {str(e)}"},
        )


def process_video_generation_background(
    video_id: int,
    prediction_url: str,
    api_key: str,
    model_id: str,
    input_params: dict,
    collection: str,
):
    """Background task to poll Replicate for video generation completion."""
    import time

    headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}

    prompt = input_params.get("prompt", "")
    max_attempts = 120  # 4 minutes (2 seconds * 120)

    try:
        for attempt in range(max_attempts):
            pred_response = requests.get(prediction_url, headers=headers)
            pred_response.raise_for_status()
            pred_data = pred_response.json()

            status = pred_data.get("status")

            if status == "succeeded":
                # Check if this is a video-to-text collection
                # Video-to-text should be handled by process_video_to_text_background or webhook
                if collection == "video-to-text":
                    print(
                        f"Video {video_id} is video-to-text, skipping video generation background task (will be handled by video-to-text task or webhook)"
                    )
                    return

                output = pred_data.get("output", [])
                if isinstance(output, str):
                    output = [output]

                video_url = output[0] if output else ""

                if video_url:
                    # Prevent race condition: check if download already attempted
                    from .database import mark_download_attempted, mark_download_failed

                    if not mark_download_attempted(video_id):
                        print(
                            f"Video {video_id} download already attempted by another process, skipping"
                        )
                        return

                    # Download and save video to database
                    try:
                        db_url = download_and_save_video(video_url, video_id)
                        metadata = {
                            "replicate_id": pred_data.get("id"),
                            "prediction_url": prediction_url,
                            "original_url": video_url,
                        }

                        # Update database with completed video
                        update_video_status(
                            video_id=video_id,
                            status="completed",
                            video_url=db_url,
                            metadata=metadata,
                        )
                        print(f"Video {video_id} completed successfully")
                        return

                    except Exception as e:
                        # Download failed after all retries - mark as permanently failed
                        error_msg = f"Failed to download video after retries: {str(e)}"
                        print(error_msg)
                        mark_download_failed(video_id, error_msg)
                        return
                else:
                    # No video URL in response
                    metadata = {
                        "replicate_id": pred_data.get("id"),
                        "prediction_url": prediction_url,
                        "error": "No video URL in Replicate response",
                    }
                    update_video_status(
                        video_id=video_id, status="failed", metadata=metadata
                    )
                    print(f"Video {video_id} failed: no output URL")
                    return

            elif status in ["failed", "canceled"]:
                error = pred_data.get("error", "Unknown error")
                metadata = {"error": error, "replicate_id": pred_data.get("id")}

                # Update database with failure
                update_video_status(video_id=video_id, status=status, metadata=metadata)
                print(f"Video {video_id} {status}: {error}")
                return

            time.sleep(2)

        # Timeout
        update_video_status(
            video_id=video_id,
            status="timeout",
            metadata={"error": "Video generation timed out"},
        )
        print(f"Video {video_id} timed out")

    except Exception as e:
        print(f"Error processing video {video_id}: {str(e)}")
        import traceback

        traceback.print_exc()

        # Update database with error
        update_video_status(
            video_id=video_id, status="failed", metadata={"error": str(e)}
        )


@app.post("/api/run-video-model")
async def api_run_video_model(
    request: RunVideoRequest,
    background_tasks: BackgroundTasks,
    current_user: Dict = Depends(verify_auth),
):
    """Initiate video generation and return immediately with a video ID. Requires authentication.

    Note: Input validation is handled by the frontend (Elm) which validates required fields
    against the model schema before submission. Replicate API also validates and will return
    clear error messages if inputs are invalid.
    """
    try:
        if not ai_client:
            # Demo response - create a pending video
            video_id = save_generated_video(
                prompt=request.input.get("prompt", ""),
                video_url="",
                model_id=request.model_id,
                parameters=request.input,
                collection=request.collection,
                status="processing",
                brief_id=request.brief_id,
            )
            return {"video_id": video_id, "status": "processing"}

        # Basic validation: ensure we have at least a prompt or image parameter
        if not request.input.get("prompt") and not any(
            k for k in request.input.keys() if "image" in k.lower()
        ):
            raise HTTPException(
                status_code=400,
                detail="Missing required input: must provide either 'prompt' or an image parameter",
            )

        headers = {
            "Authorization": f"Bearer {ai_client['api_key']}",
            "Content-Type": "application/json",
        }

        # Convert parameter types
        converted_input = {}
        for key, value in request.input.items():
            if isinstance(value, str):
                # Try to convert to int
                try:
                    converted_input[key] = int(value)
                    continue
                except ValueError:
                    pass

                # Try to convert to float
                try:
                    converted_input[key] = float(value)
                    continue
                except ValueError:
                    pass

                # Keep as string
                converted_input[key] = value
            else:
                converted_input[key] = value

        # Get the base URL for webhooks
        # In production, this should be the actual deployed URL
        base_url = settings.BASE_URL

        # Only use webhooks if we have an HTTPS URL (production)
        use_webhooks = base_url.startswith("https://")

        # Create prediction using HTTP API
        # Use version-based endpoint if version provided (more reliable)
        if request.version:
            payload = {
                "version": request.version,
                "input": converted_input,
            }
            if use_webhooks:
                payload["webhook"] = f"{base_url}/api/webhooks/replicate"
                payload["webhook_events_filter"] = ["completed"]
            url = "https://api.replicate.com/v1/predictions"
            print(f"DEBUG: Sending to Replicate API (version-based):")
            print(f"  Model: {request.model_id}")
            print(f"  Version: {request.version}")
        else:
            payload = {
                "input": converted_input,
            }
            if use_webhooks:
                payload["webhook"] = f"{base_url}/api/webhooks/replicate"
                payload["webhook_events_filter"] = ["completed"]
            url = f"https://api.replicate.com/v1/models/{request.model_id}/predictions"
            print(f"DEBUG: Sending to Replicate API (model-based):")
            print(f"  Model: {request.model_id}")

        print(
            f"  Input types: {[(k, type(v).__name__, v) for k, v in converted_input.items()]}"
        )
        if use_webhooks:
            print(f"  Webhook URL: {base_url}/api/webhooks/replicate")
        else:
            print(f"  Webhook: Disabled (local development - using polling only)")
        response = requests.post(url, headers=headers, json=payload, timeout=60)

        # Log the detailed error if request fails
        if response.status_code != 201:
            error_detail = response.text
            print(f"Replicate API Error ({response.status_code}): {error_detail}")

            try:
                error_json = response.json()
                error_msg = error_json.get("detail", error_detail)
            except:
                error_msg = error_detail

            raise HTTPException(
                status_code=400, detail=f"Replicate API error: {error_msg}"
            )

        result = response.json()

        # Get the prediction URL
        prediction_url = result.get("urls", {}).get("get")
        if not prediction_url:
            raise HTTPException(
                status_code=500, detail="No prediction URL returned from Replicate"
            )

        # Enhance prompt with brief context if provided
        enhanced_prompt = request.input.get("prompt", "")
        metadata = {"replicate_id": result.get("id"), "prediction_url": prediction_url}

        if request.brief_id:
            try:
                from .database import get_creative_brief

                brief = get_creative_brief(request.brief_id, current_user["id"])
                if brief:
                    # Add brief context to prompt
                    brief_context = f" [Style: {brief.get('creative_direction', {}).get('style', 'cinematic')}]"
                    enhanced_prompt += brief_context
                    metadata["brief_id"] = request.brief_id
                    print(f"Enhanced video prompt with brief context: {brief_context}")
            except Exception as e:
                print(f"Failed to enhance video prompt with brief context: {e}")

        # Create video record with "processing" status
        video_id = save_generated_video(
            prompt=enhanced_prompt,
            video_url="",  # Will be filled in when complete and downloaded
            model_id=request.model_id,
            parameters=request.input,
            collection=request.collection,
            status="processing",
            metadata=metadata,
            brief_id=request.brief_id,
        )

        # Start background task to poll for completion (fallback if webhook fails)
        background_tasks.add_task(
            process_video_generation_background,
            video_id=video_id,
            prediction_url=prediction_url,
            api_key=ai_client["api_key"],
            model_id=request.model_id,
            input_params=request.input,
            collection=request.collection,
        )

        # Return immediately with video ID
        return {"video_id": video_id, "status": "processing"}

    except HTTPException:
        raise
    except Exception as e:
        print(f"Error initiating video generation: {str(e)}")
        import traceback

        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Internal error: {str(e)}")


def resolve_image_reference(
    asset_id: Optional[str] = None,
    image_id: Optional[int] = None,
    video_id: Optional[int] = None,
) -> str:
    """
    Resolve an image reference (asset_id, image_id, or video_id) to a public URL for Replicate.

    Args:
        asset_id: Optional asset UUID
        image_id: Optional generated image ID
        video_id: Optional generated video ID

    Returns:
        Public URL to the image

    Raises:
        HTTPException: If reference is invalid, not found, or multiple references provided
    """
    # Count how many references are provided
    refs_provided = sum(
        [asset_id is not None, image_id is not None, video_id is not None]
    )

    if refs_provided == 0:
        raise HTTPException(status_code=400, detail="No image reference provided")

    if refs_provided > 1:
        raise HTTPException(
            status_code=400,
            detail="Provide exactly one image reference (asset_id, image_id, or video_id)",
        )

    base_url = settings.BASE_URL
    if not base_url:
        raise HTTPException(
            status_code=500,
            detail="BASE_URL not configured - cannot generate public URLs for Replicate",
        )

    # Handle asset_id
    if asset_id:
        asset = get_asset_by_id(asset_id)
        if not asset:
            raise HTTPException(status_code=404, detail=f"Asset {asset_id} not found")

        # Check if it's an image or video asset
        if asset.get("type") not in ["image", "video"]:
            raise HTTPException(
                status_code=400,
                detail=f"Asset must be an image or video, got {asset.get('type')}",
            )

        # For images, use data endpoint; for videos, use thumbnail if available
        if asset.get("type") == "video":
            # Use thumbnail for videos
            return f"{base_url}/api/v2/assets/{asset_id}/thumbnail"
        else:
            return f"{base_url}/api/v2/assets/{asset_id}/data"

    # Handle image_id
    if image_id:
        image = get_image_by_id(image_id)
        if not image:
            raise HTTPException(status_code=404, detail=f"Image {image_id} not found")

        # Check if image is completed
        if image.get("status") != "completed":
            raise HTTPException(
                status_code=400,
                detail=f"Image {image_id} is not completed (status: {image.get('status')})",
            )

        # Prefer Replicate's original URL (publicly accessible) over localhost
        # This allows external services like Replicate to fetch the image
        import json

        metadata = image.get("metadata")
        if metadata:
            if isinstance(metadata, str):
                try:
                    metadata = json.loads(metadata)
                except:
                    pass

            if isinstance(metadata, dict):
                original_url = metadata.get("original_url")
                if original_url:
                    return original_url

        # Fallback to local URL (only works if BASE_URL is publicly accessible)
        return f"{base_url}/api/images/{image_id}/data"

    # Handle video_id
    if video_id:
        video = get_video_by_id(video_id)
        if not video:
            raise HTTPException(status_code=404, detail=f"Video {video_id} not found")

        # Check if video is completed
        if video.get("status") != "completed":
            raise HTTPException(
                status_code=400,
                detail=f"Video {video_id} is not completed (status: {video.get('status')})",
            )

        # For videos, we'll use the thumbnail endpoint (images)
        # Since videos don't have thumbnails in the blob, we'll just use the data endpoint
        # and let Replicate handle it (it can extract frames from videos)
        return f"{base_url}/api/videos/{video_id}/data"

    # Should never reach here
    raise HTTPException(
        status_code=500, detail="Internal error resolving image reference"
    )


def download_and_save_video(video_url: str, video_id: int, max_retries: int = 3) -> str:
    """
    Download a video from Replicate and save it locally with retry logic and validation.

    Args:
        video_url: URL of the video to download
        video_id: ID of the video in the database
        max_retries: Maximum number of download attempts (default: 3)

    Returns:
        str: Local file path of the downloaded video

    Raises:
        Exception: If download fails after all retries
    """
    import uuid
    import time
    from pathlib import Path

    # Create videos directory if it doesn't exist
    videos_dir = Path(__file__).parent / "DATA" / "videos"
    videos_dir.mkdir(parents=True, exist_ok=True)

    # Generate unique filename
    file_ext = ".mp4"  # Default to mp4
    if video_url:
        url_ext = video_url.split(".")[-1].split("?")[0].lower()  # Remove query params
        if url_ext in ["mp4", "mov", "avi", "webm"]:
            file_ext = f".{url_ext}"

    filename = f"video_{video_id}_{uuid.uuid4().hex[:8]}{file_ext}"
    file_path = videos_dir / filename

    last_error = None

    for attempt in range(1, max_retries + 1):
        try:
            print(
                f"Downloading video (attempt {attempt}/{max_retries}) from {video_url} to {file_path}"
            )

            # Download with timeout
            response = requests.get(video_url, stream=True, timeout=300)
            response.raise_for_status()

            # Write to temporary file first
            temp_path = file_path.with_suffix(file_path.suffix + ".tmp")
            bytes_downloaded = 0

            with open(temp_path, "wb") as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        bytes_downloaded += len(chunk)

            # Validate download
            if bytes_downloaded == 0:
                raise ValueError("Downloaded file is empty (0 bytes)")

            if bytes_downloaded < 1024:  # Less than 1KB is suspicious
                raise ValueError(
                    f"Downloaded file is too small ({bytes_downloaded} bytes)"
                )

            # Validate file is a video by checking magic bytes
            with open(temp_path, "rb") as f:
                header = f.read(12)
                is_video = False

                # Check common video file signatures
                if (
                    header.startswith(b"\x00\x00\x00\x18ftypmp4")
                    or header.startswith(b"\x00\x00\x00\x1cftypisom")
                    or header.startswith(b"\x00\x00\x00\x14ftyp")
                    or header[4:8] == b"ftyp"
                ):  # Generic MP4/MOV
                    is_video = True
                elif header.startswith(b"RIFF") and header[8:12] == b"AVI ":  # AVI
                    is_video = True
                elif header.startswith(b"\x1a\x45\xdf\xa3"):  # WebM/MKV
                    is_video = True

                if not is_video:
                    raise ValueError(
                        f"Downloaded file does not appear to be a valid video (header: {header.hex()})"
                    )

            # Read the video data from temp file
            with open(temp_path, "rb") as f:
                video_binary_data = f.read()

            # Generate thumbnail from video data
            thumbnail_data = None
            try:
                import subprocess
                import os

                thumb_temp_path = file_path.with_suffix(".jpg")

                # Extract frame at 1 second using ffmpeg
                cmd = [
                    "ffmpeg",
                    "-i",
                    str(temp_path),
                    "-ss",
                    "1.0",  # Seek to 1 second
                    "-vframes",
                    "1",  # Extract 1 frame
                    "-vf",
                    "scale=400:-1",  # Resize to 400px width, maintain aspect ratio
                    "-q:v",
                    "2",  # High quality JPEG
                    "-y",
                    str(thumb_temp_path),
                ]

                result = subprocess.run(cmd, capture_output=True, timeout=10)

                if result.returncode == 0:
                    # Read thumbnail
                    with open(thumb_temp_path, "rb") as f:
                        thumbnail_data = f.read()
                    print(f"Generated thumbnail: {len(thumbnail_data)} bytes")
                else:
                    print(f"Warning: Failed to generate thumbnail for video {video_id}")

                # Clean up thumbnail temp file
                try:
                    thumb_temp_path.unlink()
                except:
                    pass

            except Exception as e:
                print(f"Warning: Error generating thumbnail for video {video_id}: {e}")

            # Store binary data in database (video + thumbnail)
            from .database import get_db

            with get_db() as conn:
                if thumbnail_data:
                    conn.execute(
                        "UPDATE generated_videos SET video_data = ?, thumbnail_data = ? WHERE id = ?",
                        (video_binary_data, thumbnail_data, video_id),
                    )
                else:
                    conn.execute(
                        "UPDATE generated_videos SET video_data = ? WHERE id = ?",
                        (video_binary_data, video_id),
                    )
                conn.commit()

            # Delete temp file - we only store in database now
            temp_path.unlink()

            print(
                f"Video downloaded successfully: {bytes_downloaded} bytes stored in DB (video_id={video_id})"
            )
            # Return a database URL instead of file path
            return f"/api/videos/{video_id}/data"

        except Exception as e:
            last_error = e
            print(f"Download attempt {attempt} failed: {e}")

            # Clean up temp file if it exists
            temp_path = file_path.with_suffix(file_path.suffix + ".tmp")
            if temp_path.exists():
                temp_path.unlink()

            if attempt < max_retries:
                # Exponential backoff: 2, 4, 8 seconds
                wait_time = 2**attempt
                print(f"Retrying in {wait_time} seconds...")
                time.sleep(wait_time)
            else:
                print(
                    f"All {max_retries} download attempts failed for video {video_id}"
                )

    # All retries failed
    raise Exception(
        f"Failed to download video after {max_retries} attempts: {last_error}"
    )


@app.post("/api/webhooks/replicate")
async def replicate_webhook(request: dict, background_tasks: BackgroundTasks):
    """Handle webhook from Replicate when a prediction completes."""
    try:
        print(f"Received Replicate webhook: {json.dumps(request, indent=2)}")

        replicate_id = request.get("id")
        status = request.get("status")
        output = request.get("output")

        if not replicate_id:
            print("No replicate_id in webhook")
            return {"status": "ignored"}

        # Find the video, image, or audio by replicate_id in metadata
        from .database import get_db

        video_id = None
        image_id = None
        audio_id = None

        with get_db() as conn:
            # Check videos first
            row = conn.execute(
                """
                SELECT id, collection FROM generated_videos
                WHERE json_extract(metadata, '$.replicate_id') = ?
                """,
                (replicate_id,),
            ).fetchone()

            if row:
                video_id = row["id"]
                video_collection = row["collection"]
            else:
                # Check images
                row = conn.execute(
                    """
                    SELECT id FROM generated_images
                    WHERE json_extract(metadata, '$.replicate_id') = ?
                    """,
                    (replicate_id,),
                ).fetchone()

                if row:
                    image_id = row["id"]
                else:
                    # Check audio
                    row = conn.execute(
                        """
                        SELECT id FROM generated_audio
                        WHERE json_extract(metadata, '$.replicate_id') = ?
                        """,
                        (replicate_id,),
                    ).fetchone()

                    if row:
                        audio_id = row["id"]

            if not video_id and not image_id and not audio_id:
                print(
                    f"No video, image, or audio found for replicate_id: {replicate_id}"
                )
                return {"status": "ignored"}

        if video_id:
            print(
                f"Found video_id: {video_id} for replicate_id: {replicate_id}, collection: {video_collection}"
            )

            if status == "succeeded" and output:
                # Check if this is a video-to-text collection
                if video_collection == "video-to-text":
                    # For video-to-text, output is text, not a URL
                    text_output = output[0] if isinstance(output, list) else output

                    # Store text output in metadata
                    metadata = {
                        "replicate_id": replicate_id,
                        "text_output": text_output,
                    }

                    # Update database with text output
                    update_video_status(
                        video_id=video_id,
                        status="completed",
                        video_url="",  # No video URL for text output
                        metadata=metadata,
                    )
                    print(
                        f"Video-to-text {video_id} completed via webhook: {text_output[:100]}..."
                    )
                else:
                    # Regular video generation - download video
                    video_url = output[0] if isinstance(output, list) else output

                    if video_url:
                        # Download and save video in background with race condition prevention
                        def download_video_task():
                            from .database import (
                                mark_download_attempted,
                                mark_download_failed,
                            )

                            # Prevent race condition: check if download already attempted
                            if not mark_download_attempted(video_id):
                                print(
                                    f"Video {video_id} download already attempted by another process (webhook), skipping"
                                )
                                return

                            try:
                                db_url = download_and_save_video(video_url, video_id)
                                # Update database with DB URL
                                update_video_status(
                                    video_id=video_id,
                                    status="completed",
                                    video_url=db_url,
                                    metadata={
                                        "replicate_id": replicate_id,
                                        "original_url": video_url,
                                    },
                                )
                                print(f"Video {video_id} saved to database via webhook")
                            except Exception as e:
                                # Download failed after all retries - mark as permanently failed
                                error_msg = (
                                    f"Failed to download video after retries: {str(e)}"
                                )
                                print(f"Webhook: {error_msg}")
                                mark_download_failed(video_id, error_msg)

                        background_tasks.add_task(download_video_task)

            elif status in ["failed", "canceled"]:
                error = request.get("error", "Unknown error")
                update_video_status(
                    video_id=video_id,
                    status=status,
                    metadata={"error": error, "replicate_id": replicate_id},
                )

        elif image_id:
            print(f"Found image_id: {image_id} for replicate_id: {replicate_id}")

            if status == "succeeded" and output:
                # Get image URL from output
                image_url = output[0] if isinstance(output, list) else output

                if image_url:
                    # Download and save image in background with race condition prevention
                    def download_image_task():
                        from .database import (
                            mark_image_download_attempted,
                            mark_image_download_failed,
                        )

                        # Prevent race condition: check if download already attempted
                        if not mark_image_download_attempted(image_id):
                            print(
                                f"Image {image_id} download already attempted by another process (webhook), skipping"
                            )
                            return

                        try:
                            db_url = download_and_save_image(image_url, image_id)
                            # Update database with DB URL
                            update_image_status(
                                image_id=image_id,
                                status="completed",
                                image_url=db_url,
                                metadata={
                                    "replicate_id": replicate_id,
                                    "original_url": image_url,
                                },
                            )
                            print(f"Image {image_id} saved to database via webhook")
                        except Exception as e:
                            # Download failed after all retries - mark as permanently failed
                            error_msg = (
                                f"Failed to download image after retries: {str(e)}"
                            )
                            print(f"Webhook: {error_msg}")
                            mark_image_download_failed(image_id, error_msg)

                    background_tasks.add_task(download_image_task)

            elif status in ["failed", "canceled"]:
                error = request.get("error", "Unknown error")
                update_image_status(
                    image_id=image_id,
                    status=status,
                    metadata={"error": error, "replicate_id": replicate_id},
                )

        elif audio_id:
            print(f"Found audio_id: {audio_id} for replicate_id: {replicate_id}")

            if status == "succeeded" and output:
                # Get audio URL from output (handle different formats)
                audio_url = None
                if isinstance(output, str):
                    audio_url = output
                elif isinstance(output, list) and len(output) > 0:
                    audio_url = output[0]
                elif isinstance(output, dict):
                    audio_url = (
                        output.get("audio")
                        or output.get("file")
                        or output.get("output")
                    )

                if audio_url:
                    # Download and save audio in background with race condition prevention
                    def download_audio_task():
                        from .database import (
                            mark_audio_download_attempted,
                            mark_audio_download_failed,
                        )

                        # Prevent race condition: check if download already attempted
                        if not mark_audio_download_attempted(audio_id):
                            print(
                                f"Audio {audio_id} download already attempted by another process (webhook), skipping"
                            )
                            return

                        try:
                            db_url = download_and_save_audio(audio_url, audio_id)
                            # Update database with DB URL
                            update_audio_status(
                                audio_id=audio_id,
                                status="completed",
                                audio_url=db_url,
                                metadata={
                                    "replicate_id": replicate_id,
                                    "original_url": audio_url,
                                },
                            )
                            print(f"Audio {audio_id} saved to database via webhook")
                        except Exception as e:
                            # Download failed after all retries - mark as permanently failed
                            error_msg = (
                                f"Failed to download audio after retries: {str(e)}"
                            )
                            print(f"Webhook: {error_msg}")
                            mark_audio_download_failed(audio_id, error_msg)

                    background_tasks.add_task(download_audio_task)

            elif status in ["failed", "canceled"]:
                error = request.get("error", "Unknown error")
                update_audio_status(
                    audio_id=audio_id,
                    status=status,
                    metadata={"error": error, "replicate_id": replicate_id},
                )

        return {"status": "processed"}

    except Exception as e:
        print(f"Error processing webhook: {e}")
        import traceback

        traceback.print_exc()
        return {"status": "error", "message": str(e)}


@app.get("/api/videos")
async def api_list_videos(
    background_tasks: BackgroundTasks,
    limit: int = Query(50, ge=1, le=100),
    offset: int = Query(0, ge=0),
    model_id: Optional[str] = Query(None),
    collection: Optional[str] = Query(None),
    current_user: Dict = Depends(verify_auth),
):
    """
    List generated videos from the database. Requires authentication.

    Automatically retries fetching videos stuck in 'processing' status when gallery refreshes.
    """
    from .database import list_videos, count_videos
    from datetime import datetime, timedelta

    videos = list_videos(
        limit=limit, offset=offset, model_id=model_id, collection=collection
    )
    total = count_videos(model_id=model_id, collection=collection)

    # Auto-retry any videos in 'processing' status on gallery refresh
    for video in videos:
        if video.get("status") != "processing":
            continue

        # Get metadata
        metadata = video.get("metadata", {})
        if isinstance(metadata, str):
            try:
                import json

                metadata = json.loads(metadata)
            except:
                metadata = {}

        prediction_url = metadata.get("prediction_url")
        replicate_id = metadata.get("replicate_id")

        if not prediction_url and not replicate_id:
            continue

        # Construct prediction URL if we only have ID
        if not prediction_url and replicate_id:
            prediction_url = f"https://api.replicate.com/v1/predictions/{replicate_id}"

        video_id = video["id"]

        # Auto-retry in background
        def auto_retry_task(vid_id, pred_url):
            import requests

            api_key = settings.REPLICATE_API_KEY
            if not api_key:
                return

            headers = {
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json",
            }

            try:
                # Check prediction status
                response = requests.get(pred_url, headers=headers, timeout=10)
                response.raise_for_status()
                pred_data = response.json()

                status = pred_data.get("status")

                if status == "succeeded":
                    output = pred_data.get("output", [])
                    if isinstance(output, str):
                        output = [output]

                    video_url = output[0] if output else ""

                    if video_url:
                        # Download and save
                        db_url = download_and_save_video(video_url, vid_id)
                        update_video_status(
                            video_id=vid_id,
                            status="completed",
                            video_url=db_url,
                            metadata={
                                "replicate_id": pred_data.get("id"),
                                "prediction_url": pred_url,
                                "original_url": video_url,
                                "auto_retried": True,
                            },
                        )
                        print(f"Auto-retry: Video {vid_id} completed")
                elif status in ["failed", "canceled"]:
                    error = pred_data.get("error", "Unknown error")
                    update_video_status(
                        video_id=vid_id,
                        status=status,
                        metadata={
                            "error": error,
                            "replicate_id": pred_data.get("id"),
                            "auto_retried": True,
                        },
                    )
                    print(f"Auto-retry: Video {vid_id} {status}")
                # If still processing, leave as-is

            except Exception as e:
                print(f"Auto-retry error for video {vid_id}: {e}")

        background_tasks.add_task(auto_retry_task, video_id, prediction_url)

    # For video-to-text collection, extract text_output from metadata
    if collection == "video-to-text":
        for video in videos:
            metadata = video.get("metadata", {})
            if isinstance(metadata, str):
                try:
                    import json

                    metadata = json.loads(metadata)
                except:
                    metadata = {}

            # Extract text_output from metadata and add it as a top-level field
            text_output = metadata.get("text_output", "")
            video["output_text"] = text_output

    return {"videos": videos, "total": total}


@app.get("/api/videos/{video_id}")
async def api_get_video(video_id: int, current_user: Dict = Depends(verify_auth)):
    """Get a specific video by ID (used for polling video status). Requires authentication."""
    video = get_video_by_id(video_id)
    if not video:
        raise HTTPException(status_code=404, detail=f"Video {video_id} not found")
    return video


# ============================================================================
# Image Generation Endpoints
# ============================================================================


@app.get("/api/image-models")
async def api_get_image_models(
    collection: Optional[str] = Query(
        "text-to-image",
        description="Collection slug: text-to-image, super-resolution, etc.",
    ),
):
    """Get image generation models from Replicate collections API."""
    try:
        if not ai_client:
            # Fallback to demo models if no API key
            return {"models": []}

        headers = {
            "Authorization": f"Bearer {ai_client['api_key']}",
            "Content-Type": "application/json",
        }

        # Special handling for curated image-editing collection
        if collection == "image-editing":
            return {
                "models": [
                    {
                        "id": "reve/create",
                        "name": "Reve Create",
                        "owner": "reve",
                        "description": "Natural language image editing - remove objects, change backgrounds, swap seasons, restyle elements, or make any adjustment you can describe in words",
                        "cover_image_url": None,
                        "latest_version": None,
                        "run_count": 0,
                        "input_schema": None,
                    }
                ]
            }

        # Use collections API with the specified collection slug
        url = f"https://api.replicate.com/v1/collections/{collection}"
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        data = response.json()

        # Format the models from the collection
        models = []
        for model_data in data.get("models", []):
            model_id = f"{model_data.get('owner')}/{model_data.get('name')}"
            models.append(
                {
                    "id": model_id,
                    "name": model_data.get("name", ""),
                    "owner": model_data.get("owner", ""),
                    "description": model_data.get("description"),
                    "cover_image_url": model_data.get("cover_image_url"),
                    "latest_version": model_data.get("latest_version", {}).get("id")
                    if model_data.get("latest_version")
                    else None,
                    "run_count": model_data.get("run_count", 0),
                    "input_schema": None,  # Will be fetched when model is selected
                }
            )

        # If the collection is super-resolution, ensure the configured upscaler is included
        # This allows flexibility to change upscaler models via UPSCALER_MODEL env variable
        if collection == "super-resolution":
            upscaler_model_id = settings.UPSCALER_MODEL
            # Check if it's already in the list
            if not any(m["id"] == upscaler_model_id for m in models):
                # Add it manually with dynamic name based on model ID
                owner, name = upscaler_model_id.split("/")
                display_name = name.replace("-", " ").replace("_", " ").title()
                models.insert(
                    0,
                    {
                        "id": upscaler_model_id,
                        "name": display_name,
                        "owner": owner,
                        "description": "High-resolution AI image upscaler with stunning detail and quality",
                        "cover_image_url": None,
                        "latest_version": None,
                        "run_count": 0,
                        "input_schema": None,
                    },
                )

        return {"models": models}
    except Exception as e:
        print(f"Error fetching image models from collection '{collection}': {str(e)}")
        import traceback

        traceback.print_exc()
        # Fallback to empty list
        return {"models": []}


@app.get("/api/image-models/{model_owner}/{model_name}/schema")
async def api_get_image_model_schema(model_owner: str, model_name: str):
    """Get the input schema for a specific image model."""
    try:
        if not ai_client:
            return {"input_schema": {"prompt": {"type": "string"}}}

        headers = {
            "Authorization": f"Bearer {ai_client['api_key']}",
            "Content-Type": "application/json",
        }

        # Fetch model details including schema
        url = f"https://api.replicate.com/v1/models/{model_owner}/{model_name}"
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        data = response.json()

        # Extract input schema from latest version
        latest_version = data.get("latest_version") or {}
        version_id = latest_version.get("id")
        openapi_schema = latest_version.get("openapi_schema") or {}
        input_schema = (
            openapi_schema.get("components", {}).get("schemas", {}).get("Input", {})
        )

        # Extract properties and required fields
        properties = input_schema.get("properties", {})
        required_fields = input_schema.get("required", [])

        # Return schema with version ID for reliable predictions
        return {
            "input_schema": properties,
            "required": required_fields,
            "version": version_id,  # Include version ID for predictions
        }
    except Exception as e:
        print(f"Error fetching image model schema: {str(e)}")
        import traceback

        traceback.print_exc()
        return {"input_schema": {"prompt": {"type": "string"}}}


def process_image_generation_background(
    image_id: int,
    prediction_url: str,
    api_key: str,
    model_id: str,
    input_params: dict,
    collection: str,
):
    """Background task to poll Replicate for image generation completion."""
    import time

    headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}

    prompt = input_params.get("prompt", "")
    max_attempts = 120  # 4 minutes (2 seconds * 120)

    try:
        for attempt in range(max_attempts):
            pred_response = requests.get(prediction_url, headers=headers)
            pred_response.raise_for_status()
            pred_data = pred_response.json()

            status = pred_data.get("status")

            if status == "succeeded":
                output = pred_data.get("output", [])
                if isinstance(output, str):
                    output = [output]

                image_url = output[0] if output else ""

                if image_url:
                    # Prevent race condition: check if download already attempted
                    from .database import (
                        mark_image_download_attempted,
                        mark_image_download_failed,
                    )

                    if not mark_image_download_attempted(image_id):
                        print(
                            f"Image {image_id} download already attempted by another process, skipping"
                        )
                        return

                    # Download and save image to database
                    try:
                        db_url = download_and_save_image(image_url, image_id)
                        metadata = {
                            "replicate_id": pred_data.get("id"),
                            "prediction_url": prediction_url,
                            "original_url": image_url,
                        }

                        # Update database with completed image
                        update_image_status(
                            image_id=image_id,
                            status="completed",
                            image_url=db_url,
                            metadata=metadata,
                        )
                        print(f"Image {image_id} completed successfully")
                        return

                    except Exception as e:
                        # Download failed after all retries - mark as permanently failed
                        error_msg = f"Failed to download image after retries: {str(e)}"
                        print(error_msg)
                        mark_image_download_failed(image_id, error_msg)
                        return
                else:
                    # No image URL in response
                    metadata = {
                        "replicate_id": pred_data.get("id"),
                        "prediction_url": prediction_url,
                        "error": "No image URL in Replicate response",
                    }
                    update_image_status(
                        image_id=image_id, status="failed", metadata=metadata
                    )
                    print(f"Image {image_id} failed: no output URL")
                    return

            elif status in ["failed", "canceled"]:
                error = pred_data.get("error", "Unknown error")
                metadata = {"error": error, "replicate_id": pred_data.get("id")}

                # Update database with failure
                update_image_status(image_id=image_id, status=status, metadata=metadata)
                print(f"Image {image_id} {status}: {error}")
                return

            time.sleep(2)

        # Timeout
        update_image_status(
            image_id=image_id,
            status="timeout",
            metadata={"error": "Image generation timed out"},
        )
        print(f"Image {image_id} timed out")

    except Exception as e:
        print(f"Error processing image {image_id}: {str(e)}")
        import traceback

        traceback.print_exc()

        # Update database with error
        update_image_status(
            image_id=image_id, status="failed", metadata={"error": str(e)}
        )


def process_audio_generation_background(
    audio_id: int,
    prediction_url: str,
    api_key: str,
    model_id: str,
    input_params: dict,
    collection: str,
):
    """Background task to poll Replicate for audio generation completion."""
    import time

    headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}

    prompt = input_params.get("prompt", "")
    max_attempts = 180  # 6 minutes (2 seconds * 180) - audio generation can take longer

    try:
        for attempt in range(max_attempts):
            pred_response = requests.get(prediction_url, headers=headers)
            pred_response.raise_for_status()
            pred_data = pred_response.json()

            status = pred_data.get("status")

            if status == "succeeded":
                output = pred_data.get("output")

                # Handle different output formats
                audio_url = None
                if isinstance(output, str):
                    audio_url = output
                elif isinstance(output, list) and len(output) > 0:
                    audio_url = output[0]
                elif isinstance(output, dict):
                    # Some models return output as dict with 'audio' or 'file' key
                    audio_url = (
                        output.get("audio")
                        or output.get("file")
                        or output.get("output")
                    )

                if audio_url:
                    # Prevent race condition: check if download already attempted
                    from .database import (
                        mark_audio_download_attempted,
                        mark_audio_download_failed,
                    )

                    if not mark_audio_download_attempted(audio_id):
                        print(
                            f"Audio {audio_id} download already attempted by another process, skipping"
                        )
                        return

                    # Download and save audio to database
                    try:
                        db_url = download_and_save_audio(audio_url, audio_id)

                        # Extract duration if available
                        duration = pred_data.get("metrics", {}).get("predict_time")

                        metadata = {
                            "replicate_id": pred_data.get("id"),
                            "prediction_url": prediction_url,
                            "original_url": audio_url,
                            "metrics": pred_data.get("metrics", {}),
                        }

                        # Update database with completed audio
                        update_audio_status(
                            audio_id=audio_id,
                            status="completed",
                            audio_url=db_url,
                            metadata=metadata,
                        )
                        print(f"Audio {audio_id} completed successfully")
                        return

                    except Exception as e:
                        # Download failed after all retries - mark as permanently failed
                        error_msg = f"Failed to download audio after retries: {str(e)}"
                        print(error_msg)
                        mark_audio_download_failed(audio_id, error_msg)
                        return
                else:
                    # No audio URL in response
                    metadata = {
                        "replicate_id": pred_data.get("id"),
                        "prediction_url": prediction_url,
                        "error": "No audio URL in Replicate response",
                    }
                    update_audio_status(
                        audio_id=audio_id, status="failed", metadata=metadata
                    )
                    print(f"Audio {audio_id} failed: no output URL")
                    return

            elif status in ["failed", "canceled"]:
                error = pred_data.get("error", "Unknown error")
                metadata = {"error": error, "replicate_id": pred_data.get("id")}

                # Update database with failure
                update_audio_status(audio_id=audio_id, status=status, metadata=metadata)
                print(f"Audio {audio_id} {status}: {error}")
                return

            time.sleep(2)

        # Timeout
        update_audio_status(
            audio_id=audio_id,
            status="timeout",
            metadata={"error": "Audio generation timed out"},
        )
        print(f"Audio {audio_id} timed out")

    except Exception as e:
        print(f"Error processing audio {audio_id}: {str(e)}")
        import traceback

        traceback.print_exc()

        # Update database with error
        update_audio_status(
            audio_id=audio_id, status="failed", metadata={"error": str(e)}
        )


@app.post("/api/run-image-model")
async def api_run_image_model(
    request: RunImageRequest,
    background_tasks: BackgroundTasks,
    current_user: Dict = Depends(verify_auth),
):
    """Initiate image generation and return immediately with an image ID. Requires authentication.

    This endpoint handles all image generation models including:
    - Text-to-image models (prompt -> image)
    - Image-to-image models (image + prompt -> image)
    - Super-resolution/upscaler models (image + scale/dynamic/sharpen -> upscaled image)

    The upscaling functionality is integrated into the standard image generation workflow.
    Upscaler models from the 'super-resolution' collection accept an 'image' input parameter
    along with upscaling-specific parameters instead of just 'prompt'.

    Note: Input validation is handled by the frontend (Elm) which validates required fields
    against the model schema before submission. Additional validation for super-resolution
    models is performed below. Replicate API also validates and will return clear error messages
    if inputs are invalid.
    """
    try:
        if not ai_client:
            # Demo response - create a pending image
            image_id = save_generated_image(
                prompt=request.input.get("prompt", ""),
                image_url="",
                model_id=request.model_id,
                parameters=request.input,
                collection=request.collection,
                status="processing",
            )
            return {"image_id": image_id, "status": "processing"}

        # Basic validation: ensure we have at least a prompt or image parameter
        if not request.input.get("prompt") and not any(
            k for k in request.input.keys() if "image" in k.lower()
        ):
            raise HTTPException(
                status_code=400,
                detail="Missing required input: must provide either 'prompt' or an image parameter",
            )

        headers = {
            "Authorization": f"Bearer {ai_client['api_key']}",
            "Content-Type": "application/json",
        }

        # Convert parameter types
        converted_input = {}
        for key, value in request.input.items():
            if isinstance(value, str):
                # Try to convert to int
                try:
                    converted_input[key] = int(value)
                    continue
                except ValueError:
                    pass

                # Try to convert to float
                try:
                    converted_input[key] = float(value)
                    continue
                except ValueError:
                    pass

                # Keep as string
                converted_input[key] = value
            else:
                converted_input[key] = value

        # Validate parameters for super-resolution models
        # Super-resolution models (upscalers) have specific parameter constraints
        if request.collection == "super-resolution":
            # Validate scale parameter (if provided)
            if "scale" in converted_input:
                scale = converted_input["scale"]
                if not isinstance(scale, (int, float)) or not (1 <= scale <= 4):
                    raise HTTPException(
                        status_code=400,
                        detail="Scale parameter must be between 1 and 4",
                    )

            # Validate dynamic/HDR parameter (if provided)
            if "dynamic" in converted_input:
                dynamic = converted_input["dynamic"]
                if not isinstance(dynamic, (int, float)) or not (3 <= dynamic <= 9):
                    raise HTTPException(
                        status_code=400,
                        detail="Dynamic (HDR) parameter must be between 3 and 9",
                    )

            # Validate sharpen parameter (if provided)
            if "sharpen" in converted_input:
                sharpen = converted_input["sharpen"]
                if not isinstance(sharpen, (int, float)) or not (0 <= sharpen <= 10):
                    raise HTTPException(
                        status_code=400,
                        detail="Sharpen parameter must be between 0 and 10",
                    )

            # Validate image URL parameter (required for upscaling)
            if "image" in converted_input:
                image_url = converted_input["image"]
                if not isinstance(image_url, str) or not image_url.startswith(
                    ("http://", "https://")
                ):
                    raise HTTPException(
                        status_code=400,
                        detail="Image parameter must be a valid HTTP/HTTPS URL",
                    )

        # Get the base URL for webhooks
        base_url = settings.BASE_URL

        # Only use webhooks if we have an HTTPS URL (production)
        use_webhooks = base_url.startswith("https://")

        # Create prediction using HTTP API
        if request.version:
            payload = {
                "version": request.version,
                "input": converted_input,
            }
            if use_webhooks:
                payload["webhook"] = f"{base_url}/api/webhooks/replicate"
                payload["webhook_events_filter"] = ["completed"]
            url = "https://api.replicate.com/v1/predictions"
            print(f"DEBUG: Sending to Replicate API (version-based) for image:")
            print(f"  Model: {request.model_id}")
            print(f"  Version: {request.version}")
        else:
            payload = {
                "input": converted_input,
            }
            if use_webhooks:
                payload["webhook"] = f"{base_url}/api/webhooks/replicate"
                payload["webhook_events_filter"] = ["completed"]
            url = f"https://api.replicate.com/v1/models/{request.model_id}/predictions"
            print(f"DEBUG: Sending to Replicate API (model-based) for image:")
            print(f"  Model: {request.model_id}")

        print(
            f"  Input types: {[(k, type(v).__name__, v) for k, v in converted_input.items()]}"
        )
        if use_webhooks:
            print(f"  Webhook URL: {base_url}/api/webhooks/replicate")
        else:
            print(f"  Webhook: Disabled (local development - using polling only)")
        response = requests.post(url, headers=headers, json=payload, timeout=60)

        # Log the detailed error if request fails
        if response.status_code != 201:
            error_detail = response.text
            print(f"Replicate API Error ({response.status_code}): {error_detail}")

            try:
                error_json = response.json()
                error_msg = error_json.get("detail", error_detail)
            except:
                error_msg = error_detail

            raise HTTPException(
                status_code=400, detail=f"Replicate API error: {error_msg}"
            )

        result = response.json()

        # Get the prediction URL
        prediction_url = result.get("urls", {}).get("get")
        if not prediction_url:
            raise HTTPException(
                status_code=500, detail="No prediction URL returned from Replicate"
            )

        # Enhance prompt with brief context if provided
        enhanced_prompt = request.input.get("prompt", "")
        metadata = {"replicate_id": result.get("id"), "prediction_url": prediction_url}

        if request.brief_id:
            try:
                from .database import get_creative_brief

                brief = get_creative_brief(request.brief_id, current_user["id"])
                if brief:
                    # Add brief context to prompt
                    brief_context = f" [Style: {brief.get('creative_direction', {}).get('style', 'modern')}]"
                    enhanced_prompt += brief_context
                    metadata["brief_id"] = request.brief_id
                    print(f"Enhanced prompt with brief context: {brief_context}")
            except Exception as e:
                print(f"Failed to enhance prompt with brief context: {e}")

        # Create image record with "processing" status
        image_id = save_generated_image(
            prompt=enhanced_prompt,
            image_url="",  # Will be filled in when complete and downloaded
            model_id=request.model_id,
            parameters=request.input,
            collection=request.collection,
            status="processing",
            metadata=metadata,
            brief_id=request.brief_id,
        )

        # Start background task to poll for completion (fallback if webhook fails)
        background_tasks.add_task(
            process_image_generation_background,
            image_id=image_id,
            prediction_url=prediction_url,
            api_key=ai_client["api_key"],
            model_id=request.model_id,
            input_params=request.input,
            collection=request.collection,
        )

        # Return immediately with image ID
        return {"image_id": image_id, "status": "processing"}

    except HTTPException:
        raise
    except Exception as e:
        print(f"Error initiating image generation: {str(e)}")
        import traceback

        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Internal error: {str(e)}")


@app.post("/api/run-audio-model")
async def api_run_audio_model(
    request: RunAudioRequest,
    background_tasks: BackgroundTasks,
    current_user: Dict = Depends(verify_auth),
):
    """Initiate audio generation and return immediately with an audio ID. Requires authentication.

    This endpoint handles all audio generation models from Replicate collections:
    - ai-music-generation (music generation models)
    - text-to-speech (TTS models)

    Note: Input validation is handled by the frontend (Elm) which validates required fields
    against the model schema before submission. Replicate API also validates and will return
    clear error messages if inputs are invalid.
    """
    try:
        if not ai_client:
            # Demo response - create a pending audio
            audio_id = save_generated_audio(
                prompt=request.input.get("prompt", ""),
                audio_url="",
                model_id=request.model_id,
                parameters=request.input,
                collection=request.collection,
                status="processing",
            )
            return {"audio_id": audio_id, "status": "processing"}

        # Basic validation: ensure we have at least a prompt parameter
        if not request.input.get("prompt") and not request.input.get("text"):
            raise HTTPException(
                status_code=400,
                detail="Missing required input: must provide either 'prompt' or 'text'",
            )

        # Validate continuation parameters if continuation is enabled
        if request.input.get("continuation"):
            continuation_start = request.input.get("continuation_start")
            continuation_end = request.input.get("continuation_end")

            if continuation_start is not None and continuation_end is not None:
                try:
                    start_val = float(continuation_start)
                    end_val = float(continuation_end)

                    if start_val >= end_val:
                        raise HTTPException(
                            status_code=400,
                            detail="Continuation start time must be less than end time",
                        )
                    if start_val < 0:
                        raise HTTPException(
                            status_code=400,
                            detail="Continuation start time must be positive",
                        )
                    if end_val <= 0:
                        raise HTTPException(
                            status_code=400,
                            detail="Continuation end time must be greater than 0",
                        )
                except (ValueError, TypeError):
                    raise HTTPException(
                        status_code=400,
                        detail="Continuation start and end times must be valid numbers",
                    )

        headers = {
            "Authorization": f"Bearer {ai_client['api_key']}",
            "Content-Type": "application/json",
        }

        # Convert parameter types (string to int/float where appropriate)
        converted_input = {}
        for key, value in request.input.items():
            if isinstance(value, str):
                # Skip empty strings
                if not value.strip():
                    continue

                # Try to convert to int
                try:
                    converted_input[key] = int(value)
                    continue
                except ValueError:
                    pass

                # Try to convert to float
                try:
                    converted_input[key] = float(value)
                    continue
                except ValueError:
                    pass

                # Keep as string
                converted_input[key] = value
            else:
                converted_input[key] = value

        # Get the base URL for webhooks
        base_url = settings.BASE_URL

        # Only use webhooks if we have an HTTPS URL (production)
        use_webhooks = base_url.startswith("https://")

        # Create prediction using HTTP API
        if request.version:
            payload = {
                "version": request.version,
                "input": converted_input,
            }
            if use_webhooks:
                payload["webhook"] = f"{base_url}/api/webhooks/replicate"
                payload["webhook_events_filter"] = ["completed"]
            url = "https://api.replicate.com/v1/predictions"
            print(f"DEBUG: Sending to Replicate API (version-based) for audio:")
            print(f"  Model: {request.model_id}")
            print(f"  Version: {request.version}")
        else:
            payload = {
                "input": converted_input,
            }
            if use_webhooks:
                payload["webhook"] = f"{base_url}/api/webhooks/replicate"
                payload["webhook_events_filter"] = ["completed"]
            url = f"https://api.replicate.com/v1/models/{request.model_id}/predictions"
            print(f"DEBUG: Sending to Replicate API (model-based) for audio:")
            print(f"  Model: {request.model_id}")

        print(
            f"  Input types: {[(k, type(v).__name__, v) for k, v in converted_input.items()]}"
        )
        if use_webhooks:
            print(f"  Webhook URL: {base_url}/api/webhooks/replicate")
        else:
            print(f"  Webhook: Disabled (local development - using polling only)")

        response = requests.post(url, headers=headers, json=payload, timeout=60)

        # Log the detailed error if request fails
        if response.status_code != 201:
            error_detail = response.text
            print(f"Replicate API Error ({response.status_code}): {error_detail}")

            try:
                error_json = response.json()
                error_msg = error_json.get("detail", error_detail)
            except:
                error_msg = error_detail

            raise HTTPException(
                status_code=400, detail=f"Replicate API error: {error_msg}"
            )

        result = response.json()

        # Get the prediction URL
        prediction_url = result.get("urls", {}).get("get")
        if not prediction_url:
            raise HTTPException(
                status_code=500, detail="No prediction URL returned from Replicate"
            )

        # Get prompt from input
        prompt = request.input.get("prompt") or request.input.get("text", "")
        metadata = {"replicate_id": result.get("id"), "prediction_url": prediction_url}

        if request.brief_id:
            metadata["brief_id"] = request.brief_id

        # Create audio record with "processing" status
        audio_id = save_generated_audio(
            prompt=prompt,
            audio_url="",  # Will be filled in when complete and downloaded
            model_id=request.model_id,
            parameters=request.input,
            collection=request.collection,
            status="processing",
            metadata=metadata,
            brief_id=request.brief_id,
        )

        # Start background task to poll for completion (fallback if webhook fails)
        background_tasks.add_task(
            process_audio_generation_background,
            audio_id=audio_id,
            prediction_url=prediction_url,
            api_key=ai_client["api_key"],
            model_id=request.model_id,
            input_params=request.input,
            collection=request.collection,
        )

        # Return immediately with audio ID
        return {"audio_id": audio_id, "status": "processing"}

    except HTTPException:
        raise
    except Exception as e:
        print(f"Error initiating audio generation: {str(e)}")
        import traceback

        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Internal error: {str(e)}")


@app.post("/api/run-video-to-text-model")
async def api_run_video_to_text_model(
    request: RunAudioRequest,
    background_tasks: BackgroundTasks,
    current_user: Dict = Depends(verify_auth),
):
    """Initiate video-to-text generation and return immediately with a video ID. Requires authentication.

    This endpoint handles all video-to-text models from Replicate collections.
    The output is stored as text in the video record's metadata.

    Note: Input validation is handled by the frontend (Elm) which validates required fields
    against the model schema before submission. Replicate API also validates and will return
    clear error messages if inputs are invalid.
    """
    try:
        if not ai_client:
            # Demo response - create a pending video-to-text result
            video_id = save_generated_video(
                prompt="Video-to-text processing",
                video_url="",
                model_id=request.model_id,
                parameters=request.input,
                collection=request.collection,
                status="processing",
            )
            return {"video_id": video_id, "status": "processing"}

        headers = {
            "Authorization": f"Bearer {ai_client['api_key']}",
            "Content-Type": "application/json",
        }

        # Convert parameter types (string to int/float where appropriate)
        converted_input = {}
        for key, value in request.input.items():
            if isinstance(value, str):
                # Skip empty strings
                if not value.strip():
                    continue

                # Try to convert to int
                try:
                    converted_input[key] = int(value)
                    continue
                except ValueError:
                    pass

                # Try to convert to float
                try:
                    converted_input[key] = float(value)
                    continue
                except ValueError:
                    pass

                # Keep as string
                converted_input[key] = value
            else:
                converted_input[key] = value

        # Get the base URL for webhooks
        base_url = settings.BASE_URL

        # Only use webhooks if we have an HTTPS URL (production)
        use_webhooks = base_url.startswith("https://")

        # Create prediction using HTTP API
        if request.version:
            payload = {
                "version": request.version,
                "input": converted_input,
            }
            if use_webhooks:
                payload["webhook"] = f"{base_url}/api/webhooks/replicate"
                payload["webhook_events_filter"] = ["completed"]
            url = "https://api.replicate.com/v1/predictions"
            print(f"DEBUG: Sending to Replicate API (version-based) for video-to-text:")
            print(f"  Model: {request.model_id}")
            print(f"  Version: {request.version}")
        else:
            payload = {
                "input": converted_input,
            }
            if use_webhooks:
                payload["webhook"] = f"{base_url}/api/webhooks/replicate"
                payload["webhook_events_filter"] = ["completed"]
            url = f"https://api.replicate.com/v1/models/{request.model_id}/predictions"
            print(f"DEBUG: Sending to Replicate API (model-based) for video-to-text:")
            print(f"  Model: {request.model_id}")

        print(
            f"  Input types: {[(k, type(v).__name__, v) for k, v in converted_input.items()]}"
        )
        if use_webhooks:
            print(f"  Webhook URL: {base_url}/api/webhooks/replicate")
        else:
            print(f"  Webhook: Disabled (local development - using polling only)")

        response = requests.post(url, headers=headers, json=payload, timeout=60)

        # Log the detailed error if request fails
        if response.status_code != 201:
            error_detail = response.text
            print(f"Replicate API Error ({response.status_code}): {error_detail}")

            try:
                error_json = response.json()
                error_msg = error_json.get("detail", error_detail)
            except:
                error_msg = error_detail

            raise HTTPException(
                status_code=400, detail=f"Replicate API error: {error_msg}"
            )

        result = response.json()

        # Get the prediction URL
        prediction_url = result.get("urls", {}).get("get")
        if not prediction_url:
            raise HTTPException(
                status_code=500, detail="No prediction URL returned from Replicate"
            )

        # Get prompt from input (video_url or video parameter)
        prompt = f"Video-to-text: {request.model_id}"
        metadata = {"replicate_id": result.get("id"), "prediction_url": prediction_url}

        # Debug: log the collection value
        print(f"DEBUG: Video-to-text request collection = '{request.collection}'")

        if request.brief_id:
            metadata["brief_id"] = request.brief_id

        # Create video record with "processing" status
        # The text output will be stored in metadata when the prediction completes
        video_id = save_generated_video(
            prompt=prompt,
            video_url="",  # No video output - text output will be in metadata
            model_id=request.model_id,
            parameters=request.input,
            collection=request.collection,
            status="processing",
            metadata=metadata,
            brief_id=request.brief_id,
        )

        # Start background task to poll for completion (fallback if webhook fails)
        background_tasks.add_task(
            process_video_to_text_background,
            video_id=video_id,
            prediction_url=prediction_url,
            api_key=ai_client["api_key"],
            model_id=request.model_id,
            input_params=request.input,
            collection=request.collection,
        )

        # Return immediately with video ID
        return {"video_id": video_id, "status": "processing"}

    except HTTPException:
        raise
    except Exception as e:
        print(f"Error initiating video-to-text generation: {str(e)}")
        import traceback

        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Internal error: {str(e)}")


@app.post("/api/generate-images-from-brief")
async def api_generate_images_from_brief(
    request: Dict,
    background_tasks: BackgroundTasks,
    current_user: Dict = Depends(verify_auth),
):
    """Generate images from all scenes in a creative brief.

    Expects:
        - briefId: The creative brief ID
        - modelName: The image generation model to use (e.g., "flux-schnell")

    Returns:
        - imageIds: List of created image IDs
    """
    try:
        brief_id = request.get("briefId")
        model_name = request.get("modelName")

        if not brief_id:
            raise HTTPException(status_code=400, detail="Missing briefId")
        if not model_name:
            raise HTTPException(status_code=400, detail="Missing modelName")

        # Fetch the brief
        from database import get_brief

        brief = get_brief(brief_id, current_user["id"])

        if not brief:
            raise HTTPException(status_code=404, detail="Brief not found")

        # Extract scenes from brief
        scenes = brief.get("scenes", [])
        if not scenes:
            raise HTTPException(status_code=400, detail="No scenes found in brief")

        # Find the model to get owner/name
        # Model name comes as just the name, need to look it up from image models
        image_ids = []

        # For each scene, extract the generation prompt and create an image
        for scene in scenes:
            visual = scene.get("visual", {})
            if not visual:
                continue

            generation_prompt = visual.get("generation_prompt")
            if not generation_prompt:
                continue

            # Model name should now be in format "owner/model"
            # If not, skip this scene
            if "/" not in model_name:
                print(
                    f"Warning: Invalid model format '{model_name}', expected 'owner/model', skipping scene"
                )
                continue

            model_id = model_name

            # Create image generation request
            image_request = RunImageRequest(
                model_id=model_id,
                input={"prompt": generation_prompt},
                collection="text-to-image",
                version=None,
                brief_id=brief_id,
            )

            # Call the existing image generation endpoint logic
            try:
                result = await api_run_image_model(
                    image_request, background_tasks, current_user
                )
                image_ids.append(result["image_id"])
                print(
                    f"Created image {result['image_id']} for scene prompt: {generation_prompt[:50]}..."
                )
            except Exception as e:
                print(f"Error generating image for scene: {str(e)}")
                # Continue with other scenes even if one fails

        if not image_ids:
            raise HTTPException(
                status_code=500, detail="Failed to generate any images from brief"
            )

        return {"imageIds": image_ids, "count": len(image_ids)}

    except HTTPException:
        raise
    except Exception as e:
        print(f"Error generating images from brief: {str(e)}")
        import traceback

        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Internal error: {str(e)}")


def download_and_save_image(image_url: str, image_id: int, max_retries: int = 3) -> str:
    """
    Download an image from Replicate and save it locally with retry logic.

    Args:
        image_url: URL of the image to download
        image_id: ID of the image in the database
        max_retries: Maximum number of download attempts (default: 3)

    Returns:
        str: Local file path of the downloaded image
    """
    import time
    from .database import increment_image_download_retries

    images_dir = Path(__file__).parent / "DATA" / "images"
    images_dir.mkdir(parents=True, exist_ok=True)

    # Determine file extension from URL
    ext = ".png"  # Default extension
    if "." in image_url:
        url_ext = image_url.split(".")[-1].split("?")[0].lower()
        if url_ext in ["jpg", "jpeg", "png", "gif", "webp"]:
            ext = f".{url_ext}"

    filename = f"image_{image_id}{ext}"
    file_path = images_dir / filename

    last_error = None
    for attempt in range(max_retries):
        try:
            print(
                f"Downloading image {image_id} (attempt {attempt + 1}/{max_retries}): {image_url}"
            )

            # Download with timeout
            response = requests.get(image_url, timeout=60, stream=True)
            response.raise_for_status()

            # Verify it's an image
            content_type = response.headers.get("content-type", "")
            if not content_type.startswith("image/"):
                raise ValueError(
                    f"Invalid content type: {content_type}, expected image/*"
                )

            # Download to temp file and collect binary data
            image_binary_data = bytearray()
            with open(file_path, "wb") as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
                    image_binary_data.extend(chunk)

            # Verify file was created and has content
            if not file_path.exists():
                raise FileNotFoundError(f"File was not created: {file_path}")

            file_size = file_path.stat().st_size
            if file_size == 0:
                raise ValueError("Downloaded image file is empty")

            # Store binary data in database
            from .database import get_db

            with get_db() as conn:
                conn.execute(
                    "UPDATE generated_images SET image_data = ?, status = 'completed' WHERE id = ?",
                    (bytes(image_binary_data), image_id),
                )
                conn.commit()

            # Delete temp file - we only store in database now
            file_path.unlink()

            print(
                f"Image {image_id} downloaded successfully: {file_size} bytes stored in DB"
            )
            # Return a database URL instead of file path
            # Use NGROK_URL for external services like Replicate, fall back to BASE_URL
            ngrok_url = os.getenv("NGROK_URL", "").strip()
            base_url = os.getenv("BASE_URL", "").strip()

            # Prefer ngrok URL for external accessibility
            public_url = ngrok_url if ngrok_url else base_url

            if public_url:
                return f"{public_url}/api/images/{image_id}/data"
            else:
                return f"/api/images/{image_id}/data"

        except Exception as e:
            last_error = e
            print(f"Image {image_id} download attempt {attempt + 1} failed: {str(e)}")

            # Clean up partial file if it exists
            if file_path.exists():
                file_path.unlink()

            # Increment retry counter in database
            retry_count = increment_image_download_retries(image_id)
            print(f"Image {image_id} retry count: {retry_count}")

            # Wait before retrying (exponential backoff)
            if attempt < max_retries - 1:
                wait_time = 2**attempt  # 1s, 2s, 4s, etc.
                print(f"Waiting {wait_time}s before retry...")
                time.sleep(wait_time)

    # All retries failed
    raise Exception(
        f"Failed to download image after {max_retries} attempts. Last error: {str(last_error)}"
    )


def download_and_save_audio(audio_url: str, audio_id: int, max_retries: int = 3) -> str:
    """
    Download an audio file from Replicate and save it locally with retry logic.

    Args:
        audio_url: URL of the audio file to download
        audio_id: ID of the audio in the database
        max_retries: Maximum number of download attempts (default: 3)

    Returns:
        str: Local database URL of the downloaded audio
    """
    import time
    from .database import increment_audio_download_retries

    audio_dir = Path(__file__).parent / "DATA" / "audio"
    audio_dir.mkdir(parents=True, exist_ok=True)

    # Determine file extension from URL
    ext = ".mp3"  # Default extension
    if "." in audio_url:
        url_ext = audio_url.split(".")[-1].split("?")[0].lower()
        if url_ext in ["mp3", "wav", "ogg", "m4a", "flac"]:
            ext = f".{url_ext}"

    filename = f"audio_{audio_id}{ext}"
    file_path = audio_dir / filename

    last_error = None
    for attempt in range(max_retries):
        try:
            print(
                f"Downloading audio {audio_id} (attempt {attempt + 1}/{max_retries}): {audio_url}"
            )

            # Download with timeout
            response = requests.get(audio_url, timeout=120, stream=True)
            response.raise_for_status()

            # Verify it's an audio file
            content_type = response.headers.get("content-type", "")
            if not (
                content_type.startswith("audio/")
                or content_type == "application/octet-stream"
            ):
                raise ValueError(
                    f"Invalid content type: {content_type}, expected audio/*"
                )

            # Download to temp file and collect binary data
            audio_binary_data = bytearray()
            with open(file_path, "wb") as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
                    audio_binary_data.extend(chunk)

            # Verify file was created and has content
            if not file_path.exists():
                raise FileNotFoundError(f"File was not created: {file_path}")

            file_size = file_path.stat().st_size
            if file_size == 0:
                raise ValueError("Downloaded audio file is empty")

            # Store binary data in database
            from .database import get_db

            with get_db() as conn:
                conn.execute(
                    "UPDATE generated_audio SET audio_data = ?, status = 'completed' WHERE id = ?",
                    (bytes(audio_binary_data), audio_id),
                )
                conn.commit()

            # Delete temp file - we only store in database now
            file_path.unlink()

            print(
                f"Audio {audio_id} downloaded successfully: {file_size} bytes stored in DB"
            )
            # Return a database URL instead of file path
            base_url = os.getenv("BASE_URL", "").strip()
            if base_url:
                return f"{base_url}/api/audio/{audio_id}/data"
            else:
                return f"/api/audio/{audio_id}/data"

        except Exception as e:
            last_error = e
            print(f"Audio {audio_id} download attempt {attempt + 1} failed: {str(e)}")

            # Clean up partial file if it exists
            if file_path.exists():
                file_path.unlink()

            # Increment retry counter in database
            retry_count = increment_audio_download_retries(audio_id)
            print(f"Audio {audio_id} retry count: {retry_count}")

            # Wait before retrying (exponential backoff)
            if attempt < max_retries - 1:
                wait_time = 2**attempt  # 1s, 2s, 4s, etc.
                print(f"Waiting {wait_time}s before retry...")
                time.sleep(wait_time)

    # All retries failed
    raise Exception(
        f"Failed to download audio after {max_retries} attempts. Last error: {str(last_error)}"
    )


@app.get("/api/images")
async def api_get_images(
    background_tasks: BackgroundTasks,
    limit: int = Query(50, ge=1, le=100),
    offset: int = Query(0, ge=0),
    model_id: Optional[str] = None,
    collection: Optional[str] = None,
    current_user: Dict = Depends(verify_auth),
):
    """
    Get generated images. Requires authentication.

    Automatically retries fetching images stuck in 'processing' status when gallery refreshes.
    """
    images = list_images(
        limit=limit, offset=offset, model_id=model_id, collection=collection
    )

    # Auto-retry any images in 'processing' status on gallery refresh
    for image in images:
        if image.get("status") != "processing":
            continue

        # Get metadata
        metadata = image.get("metadata", {})
        if isinstance(metadata, str):
            try:
                import json

                metadata = json.loads(metadata)
            except:
                metadata = {}

        prediction_url = metadata.get("prediction_url")
        replicate_id = metadata.get("replicate_id")

        if not prediction_url and not replicate_id:
            continue

        # Construct prediction URL if we only have ID
        if not prediction_url and replicate_id:
            prediction_url = f"https://api.replicate.com/v1/predictions/{replicate_id}"

        image_id = image["id"]

        # Auto-retry in background
        def auto_retry_image_task(img_id, pred_url):
            import requests

            api_key = settings.REPLICATE_API_KEY
            if not api_key:
                return

            headers = {
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json",
            }

            try:
                # Check prediction status
                response = requests.get(pred_url, headers=headers, timeout=10)
                response.raise_for_status()
                pred_data = response.json()

                status = pred_data.get("status")

                if status == "succeeded":
                    output = pred_data.get("output", [])
                    if isinstance(output, str):
                        output = [output]

                    image_url = output[0] if output else ""

                    if image_url:
                        # Download and save
                        db_url = download_and_save_image(image_url, img_id)
                        update_image_status(
                            image_id=img_id,
                            status="completed",
                            image_url=db_url,
                            metadata={
                                "replicate_id": pred_data.get("id"),
                                "prediction_url": pred_url,
                                "original_url": image_url,
                                "auto_retried": True,
                            },
                        )
                        print(f"Auto-retry: Image {img_id} completed")
                elif status in ["failed", "canceled"]:
                    error = pred_data.get("error", "Unknown error")
                    update_image_status(
                        image_id=img_id,
                        status=status,
                        metadata={
                            "error": error,
                            "replicate_id": pred_data.get("id"),
                            "auto_retried": True,
                        },
                    )
                    print(f"Auto-retry: Image {img_id} {status}")
                # If still processing, leave as-is

            except Exception as e:
                print(f"Auto-retry error for image {img_id}: {e}")

        background_tasks.add_task(auto_retry_image_task, image_id, prediction_url)

    return {"images": images}


@app.get("/api/images/{image_id}")
async def api_get_image(image_id: int, current_user: Dict = Depends(verify_auth)):
    """Get a specific image by ID (used for polling image status). Requires authentication."""
    image = get_image_by_id(image_id)
    if not image:
        raise HTTPException(status_code=404, detail=f"Image {image_id} not found")
    return image


@app.get("/api/audio")
async def api_get_audio(
    limit: int = Query(50, ge=1, le=100),
    offset: int = Query(0, ge=0),
    model_id: Optional[str] = None,
    collection: Optional[str] = None,
    client_id: Optional[str] = None,
    campaign_id: Optional[str] = None,
    status: Optional[str] = None,
    current_user: Dict = Depends(verify_auth),
):
    """
    Get generated audio/music. Requires authentication.

    Query parameters:
    - limit: Maximum number of audio files to return (1-100, default: 50)
    - offset: Number of audio files to skip (default: 0)
    - model_id: Filter by model ID (e.g., 'meta/musicgen', 'riffusion/riffusion')
    - collection: Filter by collection name
    - client_id: Filter by client ID
    - campaign_id: Filter by campaign ID
    - status: Filter by status (processing, completed, failed, canceled)
    """
    audio_files = list_audio(
        limit=limit,
        offset=offset,
        collection=collection,
        status=status,
        client_id=client_id,
        campaign_id=campaign_id,
    )

    # Additional model_id filtering (since list_audio doesn't support it yet)
    if model_id:
        audio_files = [a for a in audio_files if a.get("model_id") == model_id]

    return {"audio": audio_files, "count": len(audio_files)}


@app.get("/api/audio/{audio_id}")
async def api_get_audio_by_id(audio_id: int, current_user: Dict = Depends(verify_auth)):
    """Get a specific audio by ID. Requires authentication."""
    audio = get_audio_by_id(audio_id)
    if not audio:
        raise HTTPException(status_code=404, detail=f"Audio {audio_id} not found")
    return audio


@app.delete("/api/audio/{audio_id}")
async def api_delete_audio(audio_id: int, current_user: Dict = Depends(verify_auth)):
    """Delete an audio by ID. Requires authentication."""
    if delete_audio(audio_id):
        return {"success": True, "message": f"Audio {audio_id} deleted"}
    else:
        raise HTTPException(
            status_code=500, detail="Failed to delete audio from database"
        )


@app.get("/api/audio/{audio_id}/data")
async def api_get_audio_data(audio_id: int):
    """Get the binary audio data from database. Public endpoint for audio playback."""
    from .database import get_db

    with get_db() as conn:
        row = conn.execute(
            "SELECT audio_data, model_id FROM generated_audio WHERE id = ?", (audio_id,)
        ).fetchone()

        if not row or not row["audio_data"]:
            raise HTTPException(
                status_code=404, detail=f"Audio data not found for ID {audio_id}"
            )

        # Determine media type from model or default to mp3
        media_type = "audio/mpeg"  # Default to MP3
        model_id = row["model_id"] if row["model_id"] else ""

        # Riffusion might output WAV, MusicGen outputs MP3
        if "riffusion" in model_id.lower():
            media_type = "audio/wav"

        # Return binary audio data
        from fastapi.responses import Response

        return Response(
            content=row["audio_data"],
            media_type=media_type,
            headers={
                "Content-Disposition": f"inline; filename=audio_{audio_id}.mp3",
                "Accept-Ranges": "bytes",
            },
        )


@app.get("/api/audio-models")
async def api_get_audio_models(
    collection: Optional[str] = Query(
        "ai-music-generation",
        description="Collection slug: ai-music-generation, text-to-speech, etc.",
    ),
):
    """Get audio generation models from Replicate collections API."""
    try:
        if not ai_client:
            # Fallback to demo models if no API key
            return {"models": []}

        headers = {
            "Authorization": f"Bearer {ai_client['api_key']}",
            "Content-Type": "application/json",
        }

        # Use collections API with the specified collection slug
        url = f"https://api.replicate.com/v1/collections/{collection}"
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        data = response.json()

        # Format the models from the collection
        models = []
        for model_data in data.get("models", []):
            model_id = f"{model_data.get('owner')}/{model_data.get('name')}"
            models.append(
                {
                    "id": model_id,
                    "name": model_data.get("name", ""),
                    "owner": model_data.get("owner", ""),
                    "description": model_data.get("description"),
                    "cover_image_url": model_data.get("cover_image_url"),
                    "latest_version": model_data.get("latest_version", {}).get("id")
                    if model_data.get("latest_version")
                    else None,
                    "run_count": model_data.get("run_count", 0),
                    "input_schema": None,  # Will be fetched when model is selected
                }
            )

        return {"models": models}
    except Exception as e:
        print(f"Error fetching audio models from collection '{collection}': {str(e)}")
        import traceback

        traceback.print_exc()
        # Fallback to empty list
        return {"models": []}


@app.get("/api/audio-models/{model_owner}/{model_name}/schema")
async def api_get_audio_model_schema(model_owner: str, model_name: str):
    """Get the input schema for a specific audio model."""
    try:
        if not ai_client:
            return {"input_schema": {"prompt": {"type": "string"}}}

        headers = {
            "Authorization": f"Bearer {ai_client['api_key']}",
            "Content-Type": "application/json",
        }

        # Fetch model details including schema
        url = f"https://api.replicate.com/v1/models/{model_owner}/{model_name}"
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        data = response.json()

        # Extract input schema from latest version
        latest_version = data.get("latest_version") or {}
        version_id = latest_version.get("id")
        openapi_schema = latest_version.get("openapi_schema") or {}
        input_schema = (
            openapi_schema.get("components", {}).get("schemas", {}).get("Input", {})
        )

        # Extract properties and required fields
        properties = input_schema.get("properties", {})
        required = input_schema.get("required", [])

        # Return schema with version ID for reliable predictions (matching video endpoint format)
        return {
            "input_schema": properties,
            "required": required,
            "version": version_id,  # Include version ID for predictions
        }
    except Exception as e:
        print(
            f"Error fetching schema for audio model {model_owner}/{model_name}: {str(e)}"
        )
        import traceback

        traceback.print_exc()
        return {"input_schema": {"prompt": {"type": "string"}}}


@app.get("/api/images/{image_id}/data")
async def api_get_image_data(image_id: int):
    """Get the binary image data from database. Public endpoint for external services."""
    from .database import get_db

    with get_db() as conn:
        row = conn.execute(
            "SELECT image_data FROM generated_images WHERE id = ?", (image_id,)
        ).fetchone()

        if not row or not row["image_data"]:
            raise HTTPException(
                status_code=404, detail=f"Image data not found for ID {image_id}"
            )

        # Return binary image data
        from fastapi.responses import Response

        return Response(content=row["image_data"], media_type="image/png")


@app.get("/api/images/{image_id}/thumbnail")
async def api_get_image_thumbnail(image_id: int):
    """Get a thumbnail (400px width) of the image for gallery display. Public endpoint."""
    from .database import get_db
    from PIL import Image
    import io

    with get_db() as conn:
        row = conn.execute(
            "SELECT image_data FROM generated_images WHERE id = ?", (image_id,)
        ).fetchone()

        if not row or not row["image_data"]:
            raise HTTPException(
                status_code=404, detail=f"Image data not found for ID {image_id}"
            )

        # Load image and create thumbnail
        image = Image.open(io.BytesIO(row["image_data"]))

        # Resize to max width of 400px, maintaining aspect ratio
        max_width = 400
        if image.width > max_width:
            ratio = max_width / image.width
            new_height = int(image.height * ratio)
            image = image.resize((max_width, new_height), Image.Resampling.LANCZOS)

        # Convert to bytes
        output = io.BytesIO()
        image.save(output, format="JPEG", quality=85, optimize=True)
        thumbnail_data = output.getvalue()

        # Return thumbnail
        from fastapi.responses import Response

        return Response(content=thumbnail_data, media_type="image/jpeg")


@app.get("/api/videos/{video_id}/thumbnail")
async def api_get_video_thumbnail(video_id: int):
    """Return cached thumbnail or generate on-the-fly if not cached. Public endpoint for gallery."""
    import tempfile
    import subprocess
    import asyncio
    from .database import get_db

    with get_db() as conn:
        row = conn.execute(
            "SELECT video_data, thumbnail_data FROM generated_videos WHERE id = ?",
            (video_id,),
        ).fetchone()

        if not row:
            raise HTTPException(
                status_code=404, detail=f"Video not found for ID {video_id}"
            )

        # Return cached thumbnail if available
        if row["thumbnail_data"]:
            from fastapi.responses import Response

            return Response(content=row["thumbnail_data"], media_type="image/jpeg")

        # Fallback: generate thumbnail on-the-fly if not cached
        if not row["video_data"]:
            raise HTTPException(
                status_code=404, detail=f"Video data not found for ID {video_id}"
            )

        video_data = row["video_data"]

    # Generate thumbnail using ffmpeg (run in thread pool for concurrency)
    def generate_thumbnail():
        """Blocking thumbnail generation - runs in thread pool"""
        import os

        with tempfile.NamedTemporaryFile(suffix=".mp4", delete=False) as video_file:
            video_file.write(video_data)
            video_path = video_file.name

        with tempfile.NamedTemporaryFile(suffix=".jpg", delete=False) as thumb_file:
            thumb_path = thumb_file.name

        try:
            # Extract frame at 1 second using ffmpeg
            cmd = [
                "ffmpeg",
                "-i",
                video_path,
                "-ss",
                "1.0",  # Seek to 1 second
                "-vframes",
                "1",  # Extract 1 frame
                "-vf",
                "scale=400:-1",  # Resize to 400px width, maintain aspect ratio
                "-q:v",
                "2",  # High quality JPEG
                "-y",
                thumb_path,
            ]

            result = subprocess.run(cmd, capture_output=True, timeout=10)

            if result.returncode != 0:
                raise Exception("FFmpeg failed to generate thumbnail")

            # Read thumbnail
            with open(thumb_path, "rb") as f:
                thumbnail_bytes = f.read()

            # Cache the generated thumbnail in the database for future requests
            with get_db() as conn:
                conn.execute(
                    "UPDATE generated_videos SET thumbnail_data = ? WHERE id = ?",
                    (thumbnail_bytes, video_id),
                )
                conn.commit()

            return thumbnail_bytes

        finally:
            # Clean up temp files
            try:
                os.unlink(video_path)
            except:
                pass
            try:
                os.unlink(thumb_path)
            except:
                pass

    try:
        # Run thumbnail generation in thread pool to avoid blocking event loop
        thumbnail_data = await asyncio.to_thread(generate_thumbnail)

        from fastapi.responses import Response

        return Response(content=thumbnail_data, media_type="image/jpeg")

    except subprocess.TimeoutExpired:
        raise HTTPException(status_code=500, detail="Thumbnail generation timed out")
    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Error generating thumbnail: {str(e)}"
        )


@app.get("/api/videos/{video_id}/data")
@app.get("/api/videos/{video_id}/data.mp4")
async def api_get_video_data(video_id: int, request: Request):
    """Get the binary video data from database with HTTP Range support for streaming."""
    from .database import get_db

    with get_db() as conn:
        row = conn.execute(
            "SELECT video_data FROM generated_videos WHERE id = ?", (video_id,)
        ).fetchone()

        if not row or not row["video_data"]:
            raise HTTPException(
                status_code=404, detail=f"Video data not found for ID {video_id}"
            )

        video_data = row["video_data"]
        file_size = len(video_data)

        # Parse Range header if present
        range_header = request.headers.get("range")

        if range_header:
            # Parse range header (format: "bytes=start-end")
            try:
                range_match = range_header.replace("bytes=", "").split("-")
                start = int(range_match[0]) if range_match[0] else 0
                end = int(range_match[1]) if range_match[1] else file_size - 1

                # Validate range
                if start >= file_size or end >= file_size or start > end:
                    raise HTTPException(
                        status_code=416, detail="Requested range not satisfiable"
                    )

                # Extract requested chunk
                chunk = video_data[start : end + 1]
                chunk_size = len(chunk)

                # Return 206 Partial Content with range headers
                from fastapi.responses import Response

                return Response(
                    content=chunk,
                    status_code=206,
                    media_type="video/mp4",
                    headers={
                        "Content-Range": f"bytes {start}-{end}/{file_size}",
                        "Accept-Ranges": "bytes",
                        "Content-Length": str(chunk_size),
                    },
                )
            except (ValueError, IndexError):
                # Invalid range format, fall through to full file response
                pass

        # Return full video data (200 OK) if no range or invalid range
        from fastapi.responses import Response

        return Response(
            content=video_data,
            media_type="video/mp4",
            headers={
                "Accept-Ranges": "bytes",
                "Content-Length": str(file_size),
            },
        )


def process_video_combination_background(video_id: int, source_video_ids: List[int]):
    """Background task to combine videos using ffmpeg and store in database."""
    import tempfile
    import subprocess
    from .database import get_db

    try:
        # Create temp directory for processing
        with tempfile.TemporaryDirectory() as temp_dir:
            temp_path = Path(temp_dir)
            video_files = []

            # Fetch each video from database blob storage
            with get_db() as conn:
                for idx, source_id in enumerate(source_video_ids):
                    # Get video data from database
                    row = conn.execute(
                        "SELECT video_data FROM generated_videos WHERE id = ?",
                        (source_id,),
                    ).fetchone()

                    if not row or not row["video_data"]:
                        # Mark as failed
                        conn.execute(
                            "UPDATE generated_videos SET status = 'failed', error_message = ? WHERE id = ?",
                            (
                                f"Source video {source_id} not found in blob storage",
                                video_id,
                            ),
                        )
                        conn.commit()
                        print(f"Failed: Source video {source_id} not found")
                        return

                    # Write video data to temp file
                    video_file = temp_path / f"video_{idx}.mp4"
                    video_file.write_bytes(row["video_data"])
                    video_files.append(video_file)
                    print(
                        f"Loaded video {source_id} from blob storage ({len(row['video_data'])} bytes)"
                    )

            # Create concat file for ffmpeg
            concat_file = temp_path / "concat.txt"
            with open(concat_file, "w") as f:
                for video_file in video_files:
                    f.write(f"file '{video_file.absolute()}'\n")

            # Combine videos with ffmpeg (server-side)
            output_file = temp_path / "combined.mp4"
            cmd = [
                "ffmpeg",
                "-f",
                "concat",
                "-safe",
                "0",
                "-i",
                str(concat_file),
                "-c",
                "copy",
                str(output_file),
            ]

            print(f"Running ffmpeg on server: {' '.join(cmd)}")
            result = subprocess.run(cmd, capture_output=True, text=True)

            if result.returncode != 0:
                print(f"FFmpeg error: {result.stderr}")
                # Mark as failed
                with get_db() as conn:
                    conn.execute(
                        "UPDATE generated_videos SET status = 'failed', error_message = ? WHERE id = ?",
                        (f"FFmpeg failed: {result.stderr[:500]}", video_id),
                    )
                    conn.commit()
                return

            # Read combined video
            combined_data = output_file.read_bytes()
            print(f"Combined video created: {len(combined_data)} bytes")

            # Store in database and mark as completed
            with get_db() as conn:
                conn.execute(
                    """UPDATE generated_videos
                       SET status = 'completed',
                           video_data = ?,
                           video_url = ?
                       WHERE id = ?""",
                    (combined_data, f"/api/videos/{video_id}/data", video_id),
                )
                conn.commit()
                print(f"Video {video_id} completed and stored in database")

    except Exception as e:
        print(f"Error combining videos: {str(e)}")
        import traceback

        traceback.print_exc()
        # Mark as failed
        try:
            with get_db() as conn:
                conn.execute(
                    "UPDATE generated_videos SET status = 'failed', error_message = ? WHERE id = ?",
                    (str(e)[:500], video_id),
                )
                conn.commit()
        except:
            pass


@app.post("/api/videos/combine")
async def api_combine_videos(
    video_ids: List[int],
    background_tasks: BackgroundTasks,
    current_user: Dict = Depends(verify_auth),
):
    """Combine multiple videos into one using ffmpeg server-side with blob storage.

    Creates a new video entry immediately with status='processing',
    combines videos in background, and adds to gallery when complete.
    """
    from .database import get_db, save_generated_video

    if not video_ids or len(video_ids) < 2:
        raise HTTPException(status_code=400, detail="Need at least 2 videos to combine")

    # Verify all source videos exist
    with get_db() as conn:
        for vid_id in video_ids:
            row = conn.execute(
                "SELECT id FROM generated_videos WHERE id = ?", (vid_id,)
            ).fetchone()
            if not row:
                raise HTTPException(status_code=404, detail=f"Video {vid_id} not found")

    # Create database entry immediately with status='processing'
    prompt = f"Combined video from sources: {', '.join(map(str, video_ids))}"
    new_video_id = save_generated_video(
        prompt=prompt,
        video_url="",  # Will be set when processing completes
        model_id="ffmpeg-concat",
        parameters={"source_video_ids": video_ids},
        status="processing",
        metadata={"source_videos": video_ids, "combination_type": "concat"},
    )

    # Process in background
    background_tasks.add_task(
        process_video_combination_background, new_video_id, video_ids
    )

    # Return the new video ID immediately
    return {
        "id": new_video_id,
        "status": "processing",
        "message": "Video combination started. Check status at /api/videos/{id}",
        "video_url": f"/api/videos/{new_video_id}/data",
        "source_videos": video_ids,
    }


@app.get("/api/admin/storage/stats")
async def api_get_storage_stats(current_user: Dict = Depends(get_current_admin_user)):
    """Get video storage statistics. Admin only."""
    from pathlib import Path
    import os

    videos_dir = Path(__file__).parent / "DATA" / "videos"

    if not videos_dir.exists():
        return {
            "total_videos": 0,
            "total_size_bytes": 0,
            "total_size_mb": 0,
            "total_size_gb": 0,
            "videos_directory": str(videos_dir),
            "directory_exists": False,
        }

    # Count files and calculate total size
    video_files = (
        list(videos_dir.glob("*.mp4"))
        + list(videos_dir.glob("*.mov"))
        + list(videos_dir.glob("*.avi"))
        + list(videos_dir.glob("*.webm"))
    )

    total_size = sum(f.stat().st_size for f in video_files if f.is_file())

    return {
        "total_videos": len(video_files),
        "total_size_bytes": total_size,
        "total_size_mb": round(total_size / (1024 * 1024), 2),
        "total_size_gb": round(total_size / (1024 * 1024 * 1024), 2),
        "videos_directory": str(videos_dir),
        "directory_exists": True,
        "files": [
            {
                "filename": f.name,
                "size_bytes": f.stat().st_size,
                "size_mb": round(f.stat().st_size / (1024 * 1024), 2),
                "created": f.stat().st_ctime,
            }
            for f in sorted(video_files, key=lambda x: x.stat().st_ctime, reverse=True)[
                :20
            ]
        ],
    }


@app.delete("/api/admin/storage/videos/{video_id}")
async def api_delete_video_file(
    video_id: int, current_user: Dict = Depends(get_current_admin_user)
):
    """Delete a video file and database record. Admin only."""
    from pathlib import Path
    import os

    # Get video from database
    video = get_video_by_id(video_id)
    if not video:
        raise HTTPException(status_code=404, detail=f"Video {video_id} not found")

    # Delete file if it exists
    video_url = video.get("video_url", "")
    if video_url and video_url.startswith("/data/videos/"):
        filename = video_url.split("/")[-1]
        videos_dir = Path(__file__).parent / "DATA" / "videos"
        file_path = videos_dir / filename

        if file_path.exists():
            file_path.unlink()
            print(f"Deleted video file: {file_path}")

    # Delete database record
    from .database import delete_video

    if delete_video(video_id):
        return {"success": True, "message": f"Video {video_id} deleted"}
    else:
        raise HTTPException(
            status_code=500, detail="Failed to delete video from database"
        )


@app.post("/api/upload-image")
async def upload_image(
    file: UploadFile = File(...), current_user: Dict = Depends(verify_auth)
):
    """Upload an image file and return its URL. Requires authentication."""
    import uuid
    from pathlib import Path

    # Validate file type
    allowed_types = ["image/jpeg", "image/jpg", "image/png", "image/gif", "image/webp"]
    if file.content_type not in allowed_types:
        raise HTTPException(
            status_code=400,
            detail=f"Invalid file type: {file.content_type}. Allowed types: {', '.join(allowed_types)}",
        )

    # Create uploads directory
    uploads_dir = Path(__file__).parent / "DATA" / "uploads"
    uploads_dir.mkdir(parents=True, exist_ok=True)

    # Generate unique filename
    file_ext = Path(file.filename).suffix.lower()
    if not file_ext:
        file_ext = ".jpg"  # Default extension

    unique_filename = f"upload_{uuid.uuid4().hex[:12]}{file_ext}"
    file_path = uploads_dir / unique_filename

    # Save file
    try:
        contents = await file.read()

        # Validate file size (max 10MB)
        max_size = 10 * 1024 * 1024  # 10MB
        if len(contents) > max_size:
            raise HTTPException(
                status_code=400,
                detail=f"File too large. Maximum size is {max_size / (1024 * 1024)}MB",
            )

        with open(file_path, "wb") as f:
            f.write(contents)

        # Return full URL (required for Replicate API)
        base_url = settings.BASE_URL

        # For local development, return data URL since Replicate can't access localhost
        # For production (HTTPS), return HTTP URL
        if base_url.startswith("http://localhost") or base_url.startswith(
            "http://127.0.0.1"
        ):
            import base64

            # Create data URL for Replicate API (works in local dev)
            data_url = (
                f"data:{file.content_type};base64,{base64.b64encode(contents).decode()}"
            )
            print(
                f"Uploaded image (local dev): {file_path} -> data URL ({len(contents)} bytes)"
            )
            return {"success": True, "url": data_url, "filename": unique_filename}
        else:
            # Production: return HTTP URL
            image_url = f"{base_url}/data/uploads/{unique_filename}"
            print(f"Uploaded image: {file_path} -> {image_url}")
            return {"success": True, "url": image_url, "filename": unique_filename}

    except Exception as e:
        # Clean up file if it was created
        if file_path.exists():
            file_path.unlink()
        raise HTTPException(status_code=500, detail=f"Failed to upload image: {str(e)}")


@app.post("/api/upload-video")
async def upload_video(
    file: UploadFile = File(...), current_user: Dict = Depends(verify_auth)
):
    """Upload a video file, store in database as blob, and return its URL. Requires authentication."""
    from .database import get_db

    # Validate file type
    allowed_types = [
        "video/mp4",
        "video/mpeg",
        "video/quicktime",
        "video/x-msvideo",
        "video/x-matroska",
        "video/webm",
    ]
    if file.content_type not in allowed_types:
        raise HTTPException(
            status_code=400,
            detail=f"Invalid file type: {file.content_type}. Allowed types: {', '.join(allowed_types)}",
        )

    # Read file contents
    try:
        contents = await file.read()

        # Validate file size (max 100MB for videos)
        max_size = 100 * 1024 * 1024  # 100MB
        if len(contents) > max_size:
            raise HTTPException(
                status_code=400,
                detail=f"File too large. Maximum size is {max_size / (1024 * 1024)}MB",
            )

        # Store video in database as blob with collection "upload"
        # This creates a temporary video record that we can serve via /api/videos/{id}/data
        with get_db() as conn:
            cursor = conn.execute(
                """
                INSERT INTO generated_videos
                (prompt, video_url, model_id, parameters, collection, status, video_data)
                VALUES (?, ?, ?, ?, ?, ?, ?)
                """,
                (
                    f"Uploaded video: {file.filename}",
                    "",  # Will be set after we get the ID
                    "upload",
                    "{}",
                    "upload",  # Special collection for uploaded videos
                    "completed",
                    contents,
                ),
            )
            upload_id = cursor.lastrowid

            # Update video_url to point to the blob endpoint with .mp4 extension
            # This helps external services identify the file type
            video_url = f"/api/videos/{upload_id}/data.mp4"
            conn.execute(
                "UPDATE generated_videos SET video_url = ? WHERE id = ?",
                (video_url, upload_id),
            )
            conn.commit()

        # Return full URL (required for Replicate API)
        base_url = settings.BASE_URL

        # For local development, return data URL since Replicate can't access localhost
        # For production (HTTPS), return HTTP URL that serves from database
        if base_url.startswith("http://localhost") or base_url.startswith(
            "http://127.0.0.1"
        ):
            import base64

            # Create data URL for Replicate API (works in local dev)
            data_url = (
                f"data:{file.content_type};base64,{base64.b64encode(contents).decode()}"
            )
            print(
                f"Uploaded video (local dev, blob ID {upload_id}): data URL ({len(contents)} bytes)"
            )
            return {"success": True, "url": data_url, "id": upload_id}
        else:
            # Production: return HTTP URL that serves blob from database (with .mp4 extension)
            full_video_url = f"{base_url}{video_url}"
            print(
                f"Uploaded video to database (blob ID {upload_id}): {full_video_url} ({len(contents)} bytes)"
            )
            return {"success": True, "url": full_video_url, "id": upload_id}

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to upload video: {str(e)}")


# ============================================================================
# V2 Asset Upload Endpoints
# ============================================================================


@app.post(
    "/api/v2/upload-asset",
    tags=["Asset Management"],
    response_model=Union[ImageAsset, VideoAsset, AudioAsset, DocumentAsset],
)
@limiter.limit("10/minute")
async def upload_asset_v2(
    request: Request,
    file: UploadFile = File(...),
    clientId: str = Form(
        ...
    ),  # REQUIRED - every asset must be associated with a client
    campaignId: Optional[str] = Form(None),
    name: Optional[str] = Form(None),
    type: Optional[str] = Form(
        None
    ),  # Optional: "image", "video", "audio", or "document" - if empty, inferred from filetype
    tags: Optional[str] = Form(
        None
    ),  # Optional: JSON array string of tags, e.g., '["brand", "logo"]'
    current_user: Dict = Depends(verify_auth),
) -> Asset:
    """
    Upload a media asset (image, video, audio, or document).

    Form-data parameters:
    - file: The binary file (required)
    - clientId: Associate with a client (REQUIRED - every asset must have a client)
    - campaignId: Associate with a campaign (optional)
    - name: Custom display name (optional, defaults to filename)
    - type: Asset type override (optional) - one of: "image", "video", "audio", "document"
      If not provided, type is inferred from file content type (fallback: "document")
    - tags: JSON array of tags as string (optional) - e.g., '["brand", "product"]'

    Supports: jpg, jpeg, png, gif, webp, mp4, mov, mp3, wav, pdf
    Max file size: 50MB
    Rate limit: 10 uploads per minute per user

    Returns: Full Asset object with discriminated union type
    """
    import uuid
    import mimetypes
    import json
    from pathlib import Path
    from backend.database_helpers import create_asset
    from backend.asset_metadata import extract_file_metadata, generate_video_thumbnail

    # Validate optional type parameter if provided
    if type:
        allowed_asset_types = {"image", "video", "audio", "document"}
        if type not in allowed_asset_types:
            raise HTTPException(
                status_code=400,
                detail=f"Invalid type: {type}. Must be one of: {', '.join(allowed_asset_types)}",
            )

    # Parse tags if provided
    parsed_tags = None
    if tags:
        try:
            parsed_tags = json.loads(tags)
            if not isinstance(parsed_tags, list):
                raise HTTPException(
                    status_code=400,
                    detail='Tags must be a JSON array of strings, e.g., \'["brand", "logo"]\'',
                )
            # Validate all tags are strings
            if not all(isinstance(tag, str) for tag in parsed_tags):
                raise HTTPException(status_code=400, detail="All tags must be strings")
        except json.JSONDecodeError:
            raise HTTPException(
                status_code=400,
                detail='Invalid tags format. Must be valid JSON array, e.g., \'["brand", "logo"]\'',
            )

    # Validate file type
    allowed_types = {
        "image/jpeg",
        "image/jpg",
        "image/png",
        "image/gif",
        "image/webp",
        "video/mp4",
        "video/quicktime",
        "audio/mpeg",
        "audio/mp3",
        "audio/wav",
        "application/pdf",
    }

    if file.content_type not in allowed_types:
        raise HTTPException(
            status_code=400,
            detail=f"Invalid file type: {file.content_type}. Allowed: images (jpg, png, gif, webp), videos (mp4, mov), audio (mp3, wav), documents (pdf)",
        )

    # Read file contents
    try:
        contents = await file.read()
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"Failed to read file: {str(e)}")

    # Log upload details for debugging
    logger.info(
        f"Upload: filename={file.filename}, content_type={file.content_type}, size={len(contents)} bytes"
    )

    # Validate file size (max 50MB)
    max_size = 50 * 1024 * 1024  # 50MB
    size_bytes = len(contents)
    if size_bytes > max_size:
        raise HTTPException(
            status_code=400,
            detail=f"File too large. Maximum size is {max_size / (1024 * 1024)}MB",
        )

    # Determine file extension
    file_ext = Path(file.filename).suffix.lower() if file.filename else ""
    if not file_ext:
        # Guess from content type
        ext_map = {
            "image/jpeg": ".jpg",
            "image/jpg": ".jpg",
            "image/png": ".png",
            "image/gif": ".gif",
            "image/webp": ".webp",
            "video/mp4": ".mp4",
            "video/quicktime": ".mov",
            "audio/mpeg": ".mp3",
            "audio/mp3": ".mp3",
            "audio/wav": ".wav",
            "application/pdf": ".pdf",
        }
        file_ext = ext_map.get(file.content_type, ".bin")

    # Generate unique asset ID
    asset_id = str(uuid.uuid4())
    user_id = current_user["id"]

    logger.info(
        f"Asset upload started: {asset_id} ({size_bytes} bytes) by user {user_id}"
    )

    # Extract metadata from bytes (use temp file only for video/audio which need ffprobe)
    metadata = {}
    temp_file_path = None

    try:
        # Determine asset type from content type
        from backend.asset_metadata import determine_asset_type, get_file_format

        file_format = get_file_format("", file.content_type)
        if file_ext:
            file_format = file_ext.lstrip(".")

        inferred_asset_type = determine_asset_type(file.content_type, file_format)

        metadata = {
            "asset_type": inferred_asset_type,
            "format": file_format,
            "size": size_bytes,
        }

        # Extract type-specific metadata
        if inferred_asset_type == "image":
            # Extract image metadata from bytes using PIL
            try:
                from PIL import Image
                from io import BytesIO

                img = Image.open(BytesIO(contents))
                width, height = img.size
                metadata["width"] = width
                metadata["height"] = height
                img.close()
                logger.info(f"Extracted image metadata: {width}x{height}")
            except Exception as e:
                logger.warning(f"Failed to extract image metadata: {e}")

        elif inferred_asset_type in ["video", "audio"]:
            # For video/audio, we need a temp file for ffprobe
            # Create temp file, extract metadata, then delete
            import tempfile

            try:
                with tempfile.NamedTemporaryFile(
                    suffix=file_ext, delete=False
                ) as temp_file:
                    temp_file.write(contents)
                    temp_file_path = temp_file.name

                # Extract metadata using the temp file
                if inferred_asset_type == "video":
                    from backend.asset_metadata import extract_video_metadata

                    video_meta = extract_video_metadata(temp_file_path)
                    metadata.update(video_meta)
                    logger.info(f"Extracted video metadata: {video_meta}")
                else:  # audio
                    from backend.asset_metadata import extract_audio_metadata

                    audio_meta = extract_audio_metadata(temp_file_path)
                    metadata.update(audio_meta)
                    logger.info(f"Extracted audio metadata: {audio_meta}")

            except Exception as e:
                logger.warning(f"Failed to extract {inferred_asset_type} metadata: {e}")
            finally:
                # Clean up temp file
                if temp_file_path and Path(temp_file_path).exists():
                    Path(temp_file_path).unlink()
                    temp_file_path = None

    except Exception as e:
        logger.warning(f"Metadata extraction failed: {e}")
        metadata = {
            "asset_type": "document",
            "format": file_ext.lstrip("."),
            "size": size_bytes,
        }

    # Use provided type parameter if given, otherwise use inferred type (fallback to 'document')
    asset_type = type if type else metadata.get("asset_type", "document")

    # Note: Thumbnail generation for videos is skipped in blob-only mode
    # Videos are stored entirely in database, thumbnails would require complex temp file handling
    # TODO: Implement thumbnail extraction from blob_data if needed
    thumbnail_url = None

    # Determine display name
    display_name = name or file.filename or (f"{asset_id}{file_ext}")

    # Save to database with blob storage (NO filesystem storage)
    base_url = settings.BASE_URL
    asset_url = f"{base_url}/api/v2/assets/{asset_id}/data"  # Serve from blob endpoint

    try:
        db_asset_id = create_asset(
            name=display_name,
            asset_type=asset_type,  # Use the determined asset_type (from param or inferred)
            url=asset_url,
            format=metadata.get("format", file_ext.lstrip(".")),
            size=size_bytes,
            user_id=user_id,
            client_id=clientId,
            campaign_id=campaignId,
            tags=parsed_tags,  # Pass the parsed tags array
            width=metadata.get("width"),
            height=metadata.get("height"),
            duration=metadata.get("duration"),
            thumbnail_url=thumbnail_url,
            waveform_url=None,  # TODO: Implement waveform generation for audio
            page_count=metadata.get("pageCount"),
            asset_id=asset_id,  # Pass the pre-generated asset_id
            blob_data=contents,  # CRITICAL: Store file contents in database BLOB
        )

        logger.info(
            f"Asset saved to database: {asset_id} (blob: {len(contents)} bytes)"
        )
    except Exception as e:
        # No filesystem cleanup needed - everything is in memory/database
        logger.error(f"Failed to save asset to database: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to save asset: {str(e)}")

    # Fetch the created asset to return full discriminated union object
    from backend.database_helpers import get_asset_by_id as get_asset_by_id_helper

    created_asset = get_asset_by_id_helper(db_asset_id)

    if not created_asset:
        raise HTTPException(status_code=500, detail="Failed to retrieve created asset")

    return created_asset


@app.get("/api/v2/assets/{asset_id}/data", tags=["Asset Management"])
async def get_asset_data_v2(asset_id: str, current_user: Dict = Depends(verify_auth)):
    """
    Serve uploaded asset binary data from database blob storage.

    This endpoint serves assets stored entirely in the database (blob_data column).
    NO filesystem storage - all assets are stored as BLOBs.

    Requires authentication - only asset owner can access.
    """
    from fastapi.responses import Response
    from backend.database_helpers import get_asset_by_id as get_asset_helper

    # Get asset metadata AND blob data from database
    asset = get_asset_helper(asset_id, include_blob=True)
    if not asset:
        raise HTTPException(status_code=404, detail="Asset not found")

    # No ownership check - all dev team members can access all assets

    # Get blob data from database
    from backend.database import get_db

    with get_db() as conn:
        row = conn.execute(
            "SELECT blob_data FROM assets WHERE id = ?", (asset_id,)
        ).fetchone()

        if not row or not row["blob_data"]:
            raise HTTPException(
                status_code=404,
                detail="Asset binary data not found in database. Asset may have been uploaded before blob storage migration.",
            )

        blob_data = row["blob_data"]

    # Determine media type from asset type and format
    type_map = {
        "image": "image",
        "video": "video",
        "audio": "audio",
        "document": "application",
    }
    base_type = type_map.get(asset.type, "application")

    # Format-specific media types
    format_lower = asset.format.lower()
    media_type_map = {
        "jpg": "image/jpeg",
        "jpeg": "image/jpeg",
        "png": "image/png",
        "gif": "image/gif",
        "webp": "image/webp",
        "mp4": "video/mp4",
        "mov": "video/quicktime",
        "mp3": "audio/mpeg",
        "wav": "audio/wav",
        "pdf": "application/pdf",
    }

    media_type = media_type_map.get(format_lower, f"{base_type}/{format_lower}")

    # Return binary data with appropriate headers
    return Response(
        content=blob_data,
        media_type=media_type,
        headers={
            "Content-Disposition": f'inline; filename="{asset.name}"',
            "Cache-Control": "public, max-age=31536000",  # Cache for 1 year
        },
    )


@app.get("/api/v2/assets/{asset_id}/thumbnail", tags=["Asset Management"])
async def get_asset_thumbnail_v2(
    asset_id: str, current_user: Dict = Depends(verify_auth)
):
    """
    Serve the thumbnail for a video or document asset.
    Requires authentication - only asset owner can access.
    """
    from fastapi.responses import FileResponse
    from backend.database_helpers import get_asset_by_id as get_asset_helper
    import re

    # Get asset metadata
    asset = get_asset_helper(asset_id)
    if not asset:
        raise HTTPException(status_code=404, detail="Asset not found")

    # Security: Verify ownership
    if asset.userId != str(current_user["id"]):
        raise HTTPException(status_code=403, detail="Access denied")

    # Check if asset has a thumbnail (only VideoAsset and DocumentAsset have thumbnailUrl)
    if not hasattr(asset, "thumbnailUrl") or not asset.thumbnailUrl:
        raise HTTPException(status_code=404, detail="Asset does not have a thumbnail")

    # Security: Validate asset_id is a valid UUID (prevents path traversal)
    uuid_pattern = re.compile(
        r"^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$", re.IGNORECASE
    )
    if not uuid_pattern.match(asset_id):
        raise HTTPException(status_code=400, detail="Invalid asset ID format")

    # Thumbnail is stored as {asset_id}_thumb.jpg
    uploads_base = Path(__file__).parent / "DATA" / "assets"
    thumbnail_path = uploads_base / f"{asset_id}_thumb.jpg"

    # Security: Verify the resolved path is still within uploads_base
    try:
        thumbnail_path = thumbnail_path.resolve()
        uploads_base_resolved = uploads_base.resolve()
        if not str(thumbnail_path).startswith(str(uploads_base_resolved)):
            raise HTTPException(status_code=403, detail="Access denied")
    except Exception as e:
        logger.error(f"Path resolution error: {e}")
        raise HTTPException(status_code=403, detail="Access denied")

    if not thumbnail_path.exists():
        raise HTTPException(status_code=404, detail="Thumbnail file not found on disk")

    return FileResponse(
        path=str(thumbnail_path),
        media_type="image/jpeg",
        filename=f"{asset.name}_thumbnail.jpg",
    )


@app.delete("/api/v2/assets/{asset_id}", tags=["Asset Management"])
async def delete_asset_v2(asset_id: str, current_user: Dict = Depends(verify_auth)):
    """
    Delete an uploaded asset.
    Only the owner can delete their assets.
    """
    import os
    import re
    from backend.database_helpers import (
        get_asset_by_id as get_asset_helper,
        delete_asset as delete_asset_helper,
    )

    # Get asset metadata to verify ownership
    asset = get_asset_helper(asset_id)
    if not asset:
        raise HTTPException(status_code=404, detail="Asset not found")

    # Verify ownership
    if asset.userId != str(current_user["id"]):
        raise HTTPException(
            status_code=403, detail="You don't have permission to delete this asset"
        )

    # Security: Validate and sanitize file format (prevents path traversal)
    format_clean = validate_and_sanitize_format(asset.format)

    # Security: Validate asset_id is a valid UUID (prevents path traversal)
    uuid_pattern = re.compile(
        r"^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$", re.IGNORECASE
    )
    if not uuid_pattern.match(asset_id):
        raise HTTPException(status_code=400, detail="Invalid asset ID format")

    # Delete from database first
    deleted = delete_asset_helper(asset_id)
    if not deleted:
        raise HTTPException(
            status_code=500, detail="Failed to delete asset from database"
        )

    # Delete file from disk with sanitized path
    uploads_base = Path(__file__).parent / "DATA" / "assets"
    file_path = uploads_base / f"{asset_id}.{format_clean}"

    # Security: Verify the resolved path is still within uploads_base
    try:
        file_path_resolved = file_path.resolve()
        uploads_base_resolved = uploads_base.resolve()
        if not str(file_path_resolved).startswith(str(uploads_base_resolved)):
            logger.error(f"Attempted path traversal: {file_path}")
            raise HTTPException(status_code=403, detail="Access denied")
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Path resolution error during deletion: {e}")
        raise HTTPException(status_code=500, detail="Failed to delete asset file")

    if file_path.exists():
        try:
            os.remove(file_path)
            logger.info(f"Deleted asset file: {file_path}")
        except Exception as e:
            logger.warning(f"Failed to delete asset file {file_path}: {e}")

    # Delete thumbnail if it exists
    thumbnail_path = uploads_base / f"{asset_id}_thumb.jpg"
    try:
        thumbnail_path_resolved = thumbnail_path.resolve()
        if (
            str(thumbnail_path_resolved).startswith(str(uploads_base_resolved))
            and thumbnail_path.exists()
        ):
            os.remove(thumbnail_path)
            logger.info(f"Deleted thumbnail: {thumbnail_path}")
    except Exception as e:
        logger.warning(f"Failed to delete thumbnail {thumbnail_path}: {e}")

    return {"success": True, "message": f"Asset {asset_id} deleted successfully"}


@app.get(
    "/api/v2/assets",
    tags=["Asset Management"],
    response_model=List[Union[ImageAsset, VideoAsset, AudioAsset, DocumentAsset]],
)
async def list_assets_v2(
    current_user: Dict = Depends(verify_auth),
    clientId: Optional[str] = Query(None),
    campaignId: Optional[str] = Query(None),
    asset_type: Optional[str] = Query(None),
    limit: int = Query(50, ge=1, le=100),
    offset: int = Query(0, ge=0),
) -> List[Asset]:
    """
    List assets with optional filtering.

    Query parameters:
    - clientId: Filter by client ID
    - campaignId: Filter by campaign ID
    - asset_type: Filter by type ('image', 'video', 'audio', 'document')
    - limit: Maximum results (default: 50, max: 100)
    - offset: Pagination offset (default: 0)

    Returns: Array of Asset objects (discriminated union)
    """
    from backend.database_helpers import list_assets as list_assets_helper

    # Get filtered assets (no user filter - all dev team can access)
    assets = list_assets_helper(
        client_id=clientId,
        campaign_id=campaignId,
        asset_type=asset_type,
        limit=limit,
        offset=offset,
    )

    return assets


@app.get(
    "/api/v2/clients/{client_id}/assets",
    tags=["Asset Management"],
    response_model=List[Union[ImageAsset, VideoAsset, AudioAsset, DocumentAsset]],
)
async def get_client_assets(
    client_id: str,
    current_user: Dict = Depends(verify_auth),
    asset_type: Optional[str] = Query(
        None, description="Filter by type: image, video, audio, document"
    ),
    limit: int = Query(50, ge=1, le=100, description="Maximum results"),
    offset: int = Query(0, ge=0, description="Pagination offset"),
) -> List[Asset]:
    """
    Get all assets associated with a specific client.

    Every asset MUST be associated with a client, so this endpoint
    returns all assets for the given client.

    Query parameters:
    - asset_type: Optional filter by type ('image', 'video', 'audio', 'document')
    - limit: Maximum results (default: 50, max: 100)
    - offset: Pagination offset (default: 0)

    Returns: Array of Asset objects (discriminated union)
    """
    from backend.database_helpers import list_assets as list_assets_helper

    # Get all assets for this client (no user filter - all dev team can access)
    assets = list_assets_helper(
        client_id=client_id,
        campaign_id=None,  # Get all assets for client regardless of campaign
        asset_type=asset_type,
        limit=limit,
        offset=offset,
    )

    return assets


@app.get(
    "/api/v2/campaigns/{campaign_id}/assets",
    tags=["Asset Management"],
    response_model=List[Union[ImageAsset, VideoAsset, AudioAsset, DocumentAsset]],
)
async def get_campaign_assets(
    campaign_id: str,
    current_user: Dict = Depends(verify_auth),
    asset_type: Optional[str] = Query(
        None, description="Filter by type: image, video, audio, document"
    ),
    limit: int = Query(50, ge=1, le=100, description="Maximum results"),
    offset: int = Query(0, ge=0, description="Pagination offset"),
) -> List[Asset]:
    """
    Get all assets associated with a specific campaign.

    Assets may optionally be associated with a campaign, so this endpoint
    returns only assets that have been tagged to the given campaign.

    Query parameters:
    - asset_type: Optional filter by type ('image', 'video', 'audio', 'document')
    - limit: Maximum results (default: 50, max: 100)
    - offset: Pagination offset (default: 0)

    Returns: Array of Asset objects (discriminated union)
    """
    from backend.database_helpers import list_assets as list_assets_helper

    # Get all assets for this campaign (no user filter - all dev team can access)
    assets = list_assets_helper(
        client_id=None,  # Don't filter by client (campaign may have assets from different clients)
        campaign_id=campaign_id,
        asset_type=asset_type,
        limit=limit,
        offset=offset,
    )

    return assets


@app.post("/api/genesis/render")
async def api_genesis_render(
    request: GenesisRenderRequest, current_user: Dict = Depends(verify_auth)
):
    """
    Render a scene using Genesis photorealistic ray-tracer with LLM semantic augmentation.
    Requires authentication.

    This endpoint:
    1. Takes scene data with simple shapes and text descriptions
    2. Uses LLM to augment objects with photorealistic properties
    3. Renders using Genesis ray-tracer
    4. Returns path to rendered video
    """
    try:
        from genesis_renderer import create_renderer

        # Convert scene to dict with description field
        scene_data = request.scene.dict()

        # Ensure each object has a description field (can be empty)
        for obj_id, obj in scene_data.get("objects", {}).items():
            if "description" not in obj:
                obj["description"] = ""

        # Create renderer with specified quality
        renderer = create_renderer(
            quality=request.quality, output_dir="./backend/DATA/genesis_videos"
        )

        # Render the scene
        video_path = await renderer.render_scene(
            scene_data=scene_data,
            duration=request.duration,
            fps=request.fps,
            resolution=request.resolution,
            camera_config=request.camera_config,
            scene_context=request.scene_context,
        )

        # Clean up
        renderer.cleanup()

        # Extract object descriptions for database
        object_descriptions = {}
        for obj_id, obj in scene_data.get("objects", {}).items():
            if obj.get("description"):
                object_descriptions[obj_id] = obj.get("description")

        # Save to database
        from .database import save_genesis_video

        video_id = save_genesis_video(
            scene_data=scene_data,
            video_path=video_path,
            quality=request.quality,
            duration=request.duration,
            fps=request.fps,
            resolution=request.resolution,
            scene_context=request.scene_context,
            object_descriptions=object_descriptions if object_descriptions else None,
            metadata={
                "camera_config": request.camera_config,
                "renderer": "Genesis Rasterizer",  # or RayTracer when available
            },
        )

        # Return video URL (relative to backend)
        video_url = video_path.replace("./backend/DATA/", "/data/")

        return {
            "success": True,
            "video_id": video_id,
            "video_path": video_path,
            "video_url": video_url,
            "quality": request.quality,
            "duration": request.duration,
            "fps": request.fps,
        }

    except ImportError as e:
        raise HTTPException(
            status_code=503,
            detail=f"Genesis not available. Install with: pip install genesis-world==0.3.7. Error: {str(e)}",
        )
    except Exception as e:
        print(f"Genesis rendering error: {str(e)}")
        import traceback

        traceback.print_exc()
        raise HTTPException(
            status_code=500, detail=f"Genesis rendering failed: {str(e)}"
        )


@app.get("/api/genesis/videos")
async def list_genesis_videos_endpoint(
    limit: int = 50,
    offset: int = 0,
    quality: Optional[str] = None,
    current_user: Dict = Depends(verify_auth),
):
    """List Genesis-rendered videos from the database. Requires authentication."""
    try:
        from .database import list_genesis_videos, get_genesis_video_count

        videos = list_genesis_videos(limit=limit, offset=offset, quality=quality)
        total = get_genesis_video_count(quality=quality)

        return {
            "success": True,
            "videos": videos,
            "total": total,
            "limit": limit,
            "offset": offset,
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to list videos: {e}")


@app.get("/api/genesis/videos/{video_id}")
async def get_genesis_video_endpoint(
    video_id: int, current_user: Dict = Depends(verify_auth)
):
    """Get a specific Genesis video by ID. Requires authentication."""
    try:
        from .database import get_genesis_video_by_id

        video = get_genesis_video_by_id(video_id)
        if not video:
            raise HTTPException(status_code=404, detail=f"Video {video_id} not found")

        return {"success": True, "video": video}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get video: {e}")


@app.delete("/api/genesis/videos/{video_id}")
async def delete_genesis_video_endpoint(
    video_id: int, current_user: Dict = Depends(verify_auth)
):
    """Delete a Genesis video by ID. Requires authentication."""
    try:
        from .database import delete_genesis_video
        import os
        from pathlib import Path

        # Get video info first to delete the file
        from .database import get_genesis_video_by_id

        video = get_genesis_video_by_id(video_id)

        if not video:
            raise HTTPException(status_code=404, detail=f"Video {video_id} not found")

        # Delete from database
        deleted = delete_genesis_video(video_id)

        # Delete video file if it exists
        if deleted and video.get("video_path"):
            video_path = Path(video["video_path"])
            if video_path.exists():
                os.remove(video_path)

        return {"success": True, "deleted": deleted}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to delete video: {e}")


# Serve rendered videos
from fastapi.staticfiles import StaticFiles

GENESIS_VIDEO_DIR = Path(__file__).parent / "DATA" / "genesis_videos"
if GENESIS_VIDEO_DIR.exists():
    app.mount(
        "/data/genesis_videos",
        StaticFiles(directory=str(GENESIS_VIDEO_DIR)),
        name="genesis_videos",
    )

# Serve generated videos from Replicate
VIDEOS_DIR = Path(__file__).parent / "DATA" / "videos"
VIDEOS_DIR.mkdir(parents=True, exist_ok=True)
app.mount("/data/videos", StaticFiles(directory=str(VIDEOS_DIR)), name="videos")

# Serve uploaded images
UPLOADS_DIR = Path(__file__).parent / "DATA" / "uploads"
UPLOADS_DIR.mkdir(parents=True, exist_ok=True)
app.mount("/data/uploads", StaticFiles(directory=str(UPLOADS_DIR)), name="uploads")

# Serve generated images from Replicate
IMAGES_DIR = Path(__file__).parent / "DATA" / "images"
IMAGES_DIR.mkdir(parents=True, exist_ok=True)
app.mount("/data/images", StaticFiles(directory=str(IMAGES_DIR)), name="images")

# ============================================================================
# V2 Video Generation Endpoints
# ============================================================================

from .models.video_generation import (
    GenerationRequest,
    JobResponse,
    VideoProgress,
    VideoStatus,
    StoryboardEntry,
    Scene,
)
from .database import create_video_job, update_storyboard_data, get_jobs_by_client
from .services.replicate_client import ReplicateClient
import logging

logger = logging.getLogger(__name__)


def db_job_to_response(job: Dict[str, Any]) -> JobResponse:
    """Convert database job record to JobResponse model."""
    # Parse progress
    progress_data = job.get("progress", {})
    if isinstance(progress_data, str):
        try:
            progress_data = json.loads(progress_data)
        except:
            progress_data = {}

    progress = VideoProgress(
        current_stage=VideoStatus(job.get("status", "pending")),
        scenes_total=progress_data.get("scenes_total", 0),
        scenes_completed=progress_data.get("scenes_completed", 0),
        current_scene=progress_data.get("current_scene"),
        estimated_completion_seconds=progress_data.get("estimated_completion_seconds"),
        message=progress_data.get("message"),
    )

    # Parse storyboard
    storyboard = None
    storyboard_data = job.get("storyboard_data")
    if storyboard_data:
        if isinstance(storyboard_data, str):
            try:
                storyboard_data = json.loads(storyboard_data)
            except:
                storyboard_data = None

        if storyboard_data and isinstance(storyboard_data, list):
            storyboard = [StoryboardEntry(**entry) for entry in storyboard_data]

    # Handle datetime fields
    from datetime import datetime

    created_at = job["created_at"]
    if isinstance(created_at, str):
        created_at = datetime.fromisoformat(created_at.replace("Z", "+00:00"))

    updated_at = job.get("updated_at") or job["created_at"]
    if isinstance(updated_at, str):
        updated_at = datetime.fromisoformat(updated_at.replace("Z", "+00:00"))

    return JobResponse(
        job_id=job["id"],
        status=VideoStatus(job.get("status", "pending")),
        progress=progress,
        storyboard=storyboard,
        video_url=job.get("video_url") if job.get("video_url") else None,
        estimated_cost=job.get("estimated_cost", 0.0),
        actual_cost=job.get("actual_cost"),
        created_at=created_at,
        updated_at=updated_at,
        approved=job.get("approved", False),
        error_message=job.get("error_message"),
    )


@app.post("/api/v2/generate", response_model=JobResponse)
@limiter.limit("5/minute")
async def create_generation_job(
    request: Request,
    gen_request: GenerationRequest,
    background_tasks: BackgroundTasks,
    current_user: Dict = Depends(verify_auth),
):
    """
    Create a new video generation job.

    This endpoint initiates the v2 video generation workflow:
    1. Creates a job record with 'pending' status
    2. Estimates the cost based on duration and scene count
    3. Queues a background task for storyboard generation
    4. Returns the job ID and initial status

    Rate limit: 5 requests per minute per user
    """
    try:
        # Estimate number of scenes (roughly 1 scene per 5 seconds)
        estimated_scenes = max(1, gen_request.duration // 5)

        # Estimate cost (use ReplicateClient if available, otherwise mock for POC)
        try:
            replicate_client = ReplicateClient()
            estimated_cost = replicate_client.estimate_cost(
                num_images=estimated_scenes, video_duration=gen_request.duration
            )
        except ValueError as e:
            # Replicate API key not set - use mock cost for POC
            logger.warning(f"Replicate not available: {e}. Using mock cost estimation.")
            estimated_cost = (estimated_scenes * 0.003) + (gen_request.duration * 0.10)

        # Get client_id from request or user
        client_id = gen_request.client_id or current_user.get("username")

        # Create job in database
        job_id = create_video_job(
            prompt=gen_request.prompt,
            model_id="v2-workflow",
            parameters={
                "duration": gen_request.duration,
                "style": gen_request.style,
                "aspect_ratio": gen_request.aspect_ratio,
                "brand_guidelines": gen_request.brand_guidelines,
            },
            estimated_cost=estimated_cost,
            client_id=client_id,
            status="pending",
        )

        if not job_id:
            raise HTTPException(status_code=500, detail="Failed to create job")

        # Queue background task for storyboard generation (placeholder)
        # TODO: Implement actual storyboard generation
        logger.info(f"Job {job_id} created, queuing storyboard generation")

        # Fetch and return job
        job = get_job(job_id)
        if not job:
            raise HTTPException(status_code=500, detail="Job created but not found")

        return db_job_to_response(job)

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error creating generation job: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to create job: {str(e)}")


@app.get("/api/v2/jobs/{job_id}", response_model=JobResponse)
async def get_job_status(job_id: int):
    """
    Get the current status and progress of a video generation job.

    This is a public endpoint (no authentication required) to allow
    clients to check job status using just the job ID.

    Uses Redis cache to reduce database load from frequent polling.
    """
    try:
        # Use cache if available, falls back to database automatically
        job = get_job_with_cache(job_id)
        if not job:
            raise HTTPException(status_code=404, detail=f"Job {job_id} not found")

        return db_job_to_response(job)

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error fetching job {job_id}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to fetch job: {str(e)}")


@app.get("/api/v2/jobs", response_model=List[JobResponse])
async def list_jobs(
    status: Optional[str] = None,
    limit: int = Query(default=50, ge=1, le=100),
    current_user: Dict = Depends(verify_auth),
):
    """
    List video generation jobs for the authenticated user.

    Query parameters:
    - status: Filter by job status (optional)
    - limit: Maximum number of jobs to return (default: 50, max: 100)

    Returns jobs ordered by creation date (newest first).
    """
    try:
        client_id = current_user.get("username")

        if status:
            # Validate status
            try:
                VideoStatus(status)
            except ValueError:
                raise HTTPException(
                    status_code=400,
                    detail=f"Invalid status: {status}. Must be one of: {', '.join([s.value for s in VideoStatus])}",
                )
            jobs = get_jobs_by_client(client_id, status=status, limit=limit)
        else:
            jobs = get_jobs_by_client(client_id, limit=limit)

        return [db_job_to_response(job) for job in jobs]

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error listing jobs: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to list jobs: {str(e)}")


@app.post("/api/v2/jobs/{job_id}/approve", response_model=JobResponse)
async def approve_job_storyboard(
    job_id: int, current_user: Dict = Depends(verify_auth)
):
    """
    Approve a job's storyboard for rendering.

    This endpoint marks the storyboard as approved, allowing the
    rendering process to proceed. The job must:
    - Be in 'storyboard_ready' status
    - Have a valid storyboard
    - Belong to the current user
    """
    try:
        # Fetch job (use cache)
        job = get_job_with_cache(job_id)
        if not job:
            raise HTTPException(status_code=404, detail=f"Job {job_id} not found")

        # Verify ownership
        client_id = current_user.get("username")
        if job.get("client_id") != client_id:
            raise HTTPException(status_code=403, detail="Access denied")

        # Verify status
        if job.get("status") != "storyboard_ready":
            raise HTTPException(
                status_code=400,
                detail=f"Cannot approve job in status '{job.get('status')}'. Must be 'storyboard_ready'.",
            )

        # Verify storyboard exists
        if not job.get("storyboard_data"):
            raise HTTPException(
                status_code=400, detail="No storyboard available to approve"
            )

        # Approve storyboard
        success = approve_storyboard(job_id)
        if not success:
            raise HTTPException(status_code=500, detail="Failed to approve storyboard")

        # Invalidate cache after modification
        invalidate_job_cache(job_id)

        # Fetch updated job from database
        updated_job = get_job(job_id)
        if not updated_job:
            raise HTTPException(status_code=500, detail="Job approved but not found")

        return db_job_to_response(updated_job)

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error approving job {job_id}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to approve job: {str(e)}")


@app.post("/api/v2/jobs/{job_id}/render", response_model=JobResponse)
@limiter.limit("5/minute")
async def render_approved_video(
    request: Request,
    job_id: int,
    background_tasks: BackgroundTasks,
    current_user: Dict = Depends(verify_auth),
):
    """
    Trigger video rendering from an approved storyboard.

    This endpoint starts the video rendering process. The job must:
    - Have an approved storyboard
    - Be in 'storyboard_ready' status
    - Belong to the current user

    The rendering process runs as a background task.
    """
    try:
        # Fetch job (use cache)
        job = get_job_with_cache(job_id)
        if not job:
            raise HTTPException(status_code=404, detail=f"Job {job_id} not found")

        # Verify ownership
        client_id = current_user.get("username")
        if job.get("client_id") != client_id:
            raise HTTPException(status_code=403, detail="Access denied")

        # Verify approved
        if not job.get("approved"):
            raise HTTPException(
                status_code=400, detail="Storyboard must be approved before rendering"
            )

        # Verify storyboard exists
        if not job.get("storyboard_data"):
            raise HTTPException(
                status_code=400, detail="No storyboard available to render"
            )

        # Update status to rendering
        from .database import update_video_status

        update_video_status(job_id, status="rendering")

        # Invalidate cache after status change
        invalidate_job_cache(job_id)

        # Update progress (uses cache-aware function)
        update_job_progress_with_cache(
            job_id,
            {
                "current_stage": "rendering",
                "scenes_total": 0,
                "scenes_completed": 0,
                "message": "Starting video rendering...",
            },
        )

        # Queue background task for rendering (placeholder)
        # TODO: Implement actual video rendering
        logger.info(f"Job {job_id} queued for rendering")

        # Fetch updated job
        updated_job = get_job(job_id)
        if not updated_job:
            raise HTTPException(status_code=500, detail="Job updated but not found")

        return db_job_to_response(updated_job)

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error rendering job {job_id}: {str(e)}")
        raise HTTPException(
            status_code=500, detail=f"Failed to start rendering: {str(e)}"
        )


@app.get("/api/v2/jobs/{job_id}/video")
async def get_job_video(job_id: int):
    """
    Get the final rendered video for a completed job.

    This endpoint returns the video URL or redirects to the video file.
    Returns 404 if the video is not ready yet.

    This is a public endpoint (no authentication required).
    """
    try:
        # Use cache for job lookup
        job = get_job_with_cache(job_id)
        if not job:
            raise HTTPException(status_code=404, detail=f"Job {job_id} not found")

        # Check if video is ready
        if job.get("status") != "completed":
            raise HTTPException(
                status_code=404,
                detail=f"Video not ready. Current status: {job.get('status')}",
            )

        video_url = job.get("video_url")
        if not video_url:
            raise HTTPException(status_code=404, detail="Video URL not available")

        # Increment download count
        increment_download_count(job_id)

        # If it's a local path, serve the file
        if video_url.startswith("/data/"):
            from pathlib import Path

            video_path = Path(__file__).parent / video_url.lstrip("/")
            if video_path.exists():
                return FileResponse(str(video_path))

        # Otherwise redirect to external URL
        from fastapi.responses import RedirectResponse

        return RedirectResponse(url=video_url)

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error fetching video for job {job_id}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to fetch video: {str(e)}")


@app.get("/api/v2/jobs/{job_id}/export")
@limiter.limit("5/minute")
async def export_job_video(
    request: Request,
    job_id: int,
    format: str = Query("mp4", pattern="^(mp4|mov|webm)$"),
    quality: str = Query("medium", pattern="^(low|medium|high)$"),
    current_user: dict = Depends(verify_auth),
):
    """
    Export completed video in requested format and quality.

    Query Parameters:
    - format: Output format (mp4, mov, webm)
    - quality: Quality preset (low=480p, medium=720p, high=1080p)

    Returns:
    - The exported video file

    Authentication: Required
    """
    from .services.video_exporter import (
        export_video,
        get_export_path,
        check_ffmpeg_available,
    )
    from pathlib import Path

    try:
        # Check if ffmpeg is available
        if not check_ffmpeg_available():
            raise HTTPException(
                status_code=503,
                detail="Video export service unavailable (ffmpeg not installed)",
            )

        # Get job and validate
        job = get_job(job_id)
        if not job:
            raise HTTPException(status_code=404, detail=f"Job {job_id} not found")

        # Check if video is completed
        if job.get("status") != "completed":
            raise HTTPException(
                status_code=400,
                detail=f"Video not ready for export. Current status: {job.get('status')}",
            )

        video_url = job.get("video_url")
        if not video_url:
            raise HTTPException(status_code=404, detail="Video not available")

        # Determine input video path
        if video_url.startswith("/data/"):
            input_path = str(Path(__file__).parent / video_url.lstrip("/"))
        elif video_url.startswith("http"):
            # For remote URLs, we'd need to download first (not implemented in MVP)
            raise HTTPException(
                status_code=400,
                detail="Export is only available for locally stored videos",
            )
        else:
            input_path = video_url

        # Check if input file exists
        if not os.path.exists(input_path):
            raise HTTPException(status_code=404, detail="Source video file not found")

        # Generate output path
        output_path = get_export_path(
            settings.VIDEO_STORAGE_PATH, job_id, format, quality
        )

        # Check if export already exists
        if not os.path.exists(output_path):
            # Export the video
            logger.info(f"Exporting job {job_id} to {format}/{quality}")
            success, error_msg = export_video(input_path, output_path, format, quality)

            if not success:
                raise HTTPException(
                    status_code=500, detail=f"Export failed: {error_msg}"
                )

        # Increment download count
        increment_download_count(job_id)

        # Return the exported file
        return FileResponse(
            output_path,
            media_type=f"video/{format}",
            filename=f"video_{job_id}.{format}",
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error exporting video for job {job_id}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to export video: {str(e)}")


@app.post("/api/v2/jobs/{job_id}/refine", response_model=JobResponse)
async def refine_job_scene(
    job_id: int,
    scene_number: int = Query(..., ge=1),
    new_image_prompt: Optional[str] = Query(None, min_length=10, max_length=2000),
    new_description: Optional[str] = Query(None, min_length=10, max_length=1000),
    background_tasks: BackgroundTasks = BackgroundTasks(),
    current_user: dict = Depends(verify_auth),
):
    """
    Refine a specific scene in the storyboard.

    This endpoint allows regenerating a scene image with a new prompt or
    updating the scene description. After refinement, the storyboard must
    be re-approved before rendering.

    Query Parameters:
    - scene_number: Scene number to refine (1-indexed)
    - new_image_prompt: New prompt for image regeneration (optional)
    - new_description: New scene description (optional)

    Rate Limiting: Maximum 5 refinements per job

    Authentication: Required
    """
    from .services.replicate_client import ReplicateClient

    try:
        # Get job and validate
        job = get_job(job_id)
        if not job:
            raise HTTPException(status_code=404, detail=f"Job {job_id} not found")

        # Check if storyboard exists
        storyboard_data = job.get("storyboard_data")
        if not storyboard_data:
            raise HTTPException(
                status_code=400, detail="No storyboard available for refinement"
            )

        # Check refinement limit
        refinement_count = get_refinement_count(job_id)
        if refinement_count >= 5:
            raise HTTPException(
                status_code=429,
                detail="Maximum refinement limit (5) reached for this job",
            )

        # Validate at least one refinement type is provided
        if not new_image_prompt and not new_description:
            raise HTTPException(
                status_code=400,
                detail="Must provide either new_image_prompt or new_description",
            )

        # If regenerating image, do it now
        new_image_url = None
        if new_image_prompt:
            try:
                logger.info(
                    f"Regenerating image for job {job_id}, scene {scene_number}"
                )
                replicate_client = ReplicateClient()

                # Get aspect ratio from job parameters
                parameters = job.get("parameters", {})
                aspect_ratio = parameters.get("aspect_ratio", "16:9")

                # Generate new image
                image_url = replicate_client.generate_image(
                    new_image_prompt, aspect_ratio
                )
                new_image_url = image_url

                # Increment estimated cost (approximate cost for one image)
                increment_estimated_cost(job_id, 0.02)

                logger.info(f"Generated new image: {image_url}")
            except Exception as e:
                logger.error(f"Failed to regenerate image: {e}")
                raise HTTPException(
                    status_code=500, detail=f"Failed to regenerate image: {str(e)}"
                )

        # Update the scene in storyboard
        success = refine_scene_in_storyboard(
            job_id,
            scene_number,
            new_image_url=new_image_url,
            new_description=new_description,
            new_image_prompt=new_image_prompt,
        )

        if not success:
            raise HTTPException(status_code=500, detail="Failed to update storyboard")

        # Fetch updated job
        updated_job = get_job(job_id)
        if not updated_job:
            raise HTTPException(status_code=500, detail="Failed to fetch updated job")

        # Return the updated job response
        return db_job_to_response(updated_job)

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error refining scene for job {job_id}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to refine scene: {str(e)}")


@app.post("/api/v2/jobs/{job_id}/reorder", response_model=JobResponse)
async def reorder_job_scenes(
    job_id: int,
    scene_order: List[int] = Query(..., description="New order of scene numbers"),
    current_user: dict = Depends(verify_auth),
):
    """
    Reorder scenes in the storyboard.

    This endpoint allows changing the sequence of scenes. After reordering,
    the storyboard must be re-approved before rendering.

    Request Body:
    - scene_order: List of scene numbers in desired order (e.g., [1, 3, 2, 4])

    Authentication: Required
    """
    try:
        # Get job and validate
        job = get_job(job_id)
        if not job:
            raise HTTPException(status_code=404, detail=f"Job {job_id} not found")

        # Check if storyboard exists
        storyboard_data = job.get("storyboard_data")
        if not storyboard_data:
            raise HTTPException(
                status_code=400, detail="No storyboard available for reordering"
            )

        # Validate scene_order
        if not scene_order:
            raise HTTPException(status_code=400, detail="scene_order cannot be empty")

        # Reorder the scenes
        success = reorder_storyboard_scenes(job_id, scene_order)

        if not success:
            raise HTTPException(
                status_code=400,
                detail="Failed to reorder scenes. Check that all scene numbers are valid.",
            )

        # Fetch updated job
        updated_job = get_job(job_id)
        if not updated_job:
            raise HTTPException(status_code=500, detail="Failed to fetch updated job")

        # Return the updated job response
        return db_job_to_response(updated_job)

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error reordering scenes for job {job_id}: {str(e)}")
        raise HTTPException(
            status_code=500, detail=f"Failed to reorder scenes: {str(e)}"
        )


@app.get("/api/v2/jobs/{job_id}/metadata")
async def get_job_metadata(job_id: int):
    """
    Get comprehensive metadata for a video generation job.

    Returns detailed information including:
    - Scene count and details
    - Cost information (estimated and actual)
    - Generation times and statistics
    - Refinement count
    - Download count

    This is a public endpoint (no authentication required).
    """
    try:
        # Get job
        job = get_job(job_id)
        if not job:
            raise HTTPException(status_code=404, detail=f"Job {job_id} not found")

        # Build metadata response
        storyboard_data = job.get("storyboard_data", [])

        metadata = {
            "job_id": job_id,
            "status": job.get("status"),
            "created_at": job.get("created_at"),
            "updated_at": job.get("updated_at"),
            "approved": job.get("approved", False),
            "approved_at": job.get("approved_at"),
            # Scene information
            "scenes": {
                "total": len(storyboard_data),
                "completed": sum(
                    1
                    for s in storyboard_data
                    if s.get("generation_status") == "completed"
                ),
                "failed": sum(
                    1 for s in storyboard_data if s.get("generation_status") == "failed"
                ),
                "details": [
                    {
                        "scene_number": s.get("scene", {}).get("scene_number"),
                        "duration": s.get("scene", {}).get("duration"),
                        "status": s.get("generation_status"),
                        "has_image": bool(s.get("image_url")),
                    }
                    for s in storyboard_data
                ],
            },
            # Cost information
            "costs": {
                "estimated": job.get("estimated_cost", 0.0),
                "actual": job.get("actual_cost", 0.0),
                "currency": "USD",
            },
            # Generation metrics
            "metrics": {
                "refinement_count": get_refinement_count(job_id),
                "download_count": get_download_count(job_id),
            },
            # Video information
            "video": {
                "available": job.get("status") == "completed",
                "url": job.get("video_url")
                if job.get("status") == "completed"
                else None,
                "parameters": job.get("parameters", {}),
            },
            # Error information (if any)
            "error": job.get("error_message"),
        }

        return metadata

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error fetching metadata for job {job_id}: {str(e)}")
        raise HTTPException(
            status_code=500, detail=f"Failed to fetch metadata: {str(e)}"
        )


# ============================================================================
# Simple Image/Video Generation Endpoints
# ============================================================================


@app.post("/api/v2/generate/image")
@limiter.limit("10/minute")
async def generate_image(
    request: Request,
    gen_request: ImageGenerationRequest,
    background_tasks: BackgroundTasks,
    current_user: Dict = Depends(verify_auth),
):
    """
    Generate an image using nano-banana model.

    Supports text-to-image and image-to-image workflows.
    At least one of (prompt, asset_id, image_id, video_id) must be provided.

    Rate limit: 10 requests per minute per user
    """
    try:
        # Build nano-banana input parameters
        nano_banana_input = {}

        # Handle prompt
        if gen_request.prompt:
            nano_banana_input["prompt"] = gen_request.prompt
        else:
            # Default prompt if only image reference is provided
            nano_banana_input["prompt"] = "high quality image"

        # Handle image reference (if provided)
        image_url = None
        if any([gen_request.asset_id, gen_request.image_id, gen_request.video_id]):
            image_url = resolve_image_reference(
                asset_id=gen_request.asset_id,
                image_id=gen_request.image_id,
                video_id=gen_request.video_id,
            )
            nano_banana_input["image_input"] = [image_url]
            nano_banana_input["aspect_ratio"] = "match_input_image"
        else:
            # No image reference, use default aspect ratio
            nano_banana_input["image_input"] = []
            nano_banana_input["aspect_ratio"] = "1:1"

        # Set output format
        nano_banana_input["output_format"] = "jpg"

        # Create run request for nano-banana
        run_request = RunImageRequest(
            model_id="google/nano-banana",
            input=nano_banana_input,
            collection=None,
            version=None,
            brief_id=None,
        )

        # Call the existing run-image-model endpoint logic
        # Get the base URL for webhooks
        base_url = settings.BASE_URL

        # Only use webhooks if we have an HTTPS URL (production)
        use_webhooks = base_url.startswith("https://")

        # Create prediction using HTTP API
        payload = {
            "input": nano_banana_input,
        }
        if use_webhooks:
            payload["webhook"] = f"{base_url}/api/webhooks/replicate"
            payload["webhook_events_filter"] = ["completed"]

        url = "https://api.replicate.com/v1/models/google/nano-banana/predictions"

        headers = {
            "Authorization": f"Token {settings.REPLICATE_API_KEY}",
            "Content-Type": "application/json",
        }

        response = requests.post(url, json=payload, headers=headers)
        response.raise_for_status()
        prediction = response.json()

        # Save to database with client_id and campaign_id
        image_id = save_generated_image(
            prompt=nano_banana_input["prompt"],
            image_url="pending",
            model_id="google/nano-banana",
            parameters=nano_banana_input,
            collection=None,
            metadata={
                "replicate_id": prediction["id"],
                "prediction_url": prediction.get("urls", {}).get("get"),
            },
            status="processing",
            brief_id=None,
            client_id=gen_request.client_id,
            campaign_id=gen_request.campaign_id,
        )

        # Queue background processing
        background_tasks.add_task(
            process_image_generation_background,
            image_id=image_id,
            prediction_url=prediction.get("urls", {}).get("get"),
            api_key=settings.REPLICATE_API_KEY,
            model_id="google/nano-banana",
            input_params=nano_banana_input,
            collection=None,
        )

        # Return immediately with image ID
        return {"image_id": image_id, "status": "processing"}

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error generating image: {str(e)}")
        import traceback

        traceback.print_exc()
        raise HTTPException(
            status_code=500, detail=f"Failed to generate image: {str(e)}"
        )


@app.post("/api/v2/generate/video")
@limiter.limit("5/minute")
async def generate_video(
    request: Request,
    gen_request: VideoGenerationRequest,
    background_tasks: BackgroundTasks,
    current_user: Dict = Depends(verify_auth),
):
    """
    Generate a video using the selected model (default: bytedance/seedance-1-lite).

    Supports:
    - bytedance/seedance-1-lite (default)
    - kwaivgi/kling-v2.1

    Requires a start_image. If only a prompt is provided, an image will be auto-generated first.
    At least one of (prompt, asset_id, image_id, video_id) must be provided.

    Rate limit: 5 requests per minute per user
    """
    try:
        # Determine if we have an image reference
        has_image_ref = any(
            [gen_request.asset_id, gen_request.image_id, gen_request.video_id]
        )

        intermediate_image_id = None

        # If no image reference, auto-generate one using nano-banana
        if not has_image_ref:
            if not gen_request.prompt:
                raise HTTPException(
                    status_code=400,
                    detail="Either provide a prompt (for auto-image generation) or an image reference",
                )

            logger.info("No image reference provided, auto-generating image for video")

            # Generate image synchronously (with timeout)
            image_gen_request = ImageGenerationRequest(
                prompt=gen_request.prompt,
                client_id=gen_request.client_id,
                campaign_id=gen_request.campaign_id,
            )

            # Build nano-banana input
            nano_banana_input = {
                "prompt": gen_request.prompt,
                "image_input": [],
                "aspect_ratio": "16:9",  # Good for video
                "output_format": "jpg",
            }

            # Call Replicate for image generation
            payload = {"input": nano_banana_input}
            url = "https://api.replicate.com/v1/models/google/nano-banana/predictions"
            headers = {
                "Authorization": f"Token {settings.REPLICATE_API_KEY}",
                "Content-Type": "application/json",
            }

            response = requests.post(url, json=payload, headers=headers)
            response.raise_for_status()
            prediction = response.json()

            # Save intermediate image
            intermediate_image_id = save_generated_image(
                prompt=gen_request.prompt,
                image_url="pending",
                model_id="google/nano-banana",
                parameters=nano_banana_input,
                collection=None,
                metadata={
                    "replicate_id": prediction["id"],
                    "prediction_url": prediction.get("urls", {}).get("get"),
                },
                status="processing",
                brief_id=None,
                client_id=gen_request.client_id,
                campaign_id=gen_request.campaign_id,
            )

            # Wait for image completion (with timeout)
            import time

            max_wait = 60  # 60 seconds
            wait_interval = 2  # Check every 2 seconds
            elapsed = 0

            while elapsed < max_wait:
                prediction_url = prediction.get("urls", {}).get("get")
                pred_response = requests.get(prediction_url, headers=headers)
                pred_response.raise_for_status()
                pred_data = pred_response.json()

                if pred_data.get("status") == "succeeded":
                    # Download and save image
                    image_url = pred_data.get("output")
                    if isinstance(image_url, list):
                        image_url = image_url[0]

                    # Download image
                    download_url = download_and_save_image(
                        image_url, intermediate_image_id
                    )
                    break
                elif pred_data.get("status") in ["failed", "canceled"]:
                    raise HTTPException(
                        status_code=500,
                        detail=f"Image generation failed: {pred_data.get('error')}",
                    )

                time.sleep(wait_interval)
                elapsed += wait_interval

            if elapsed >= max_wait:
                raise HTTPException(
                    status_code=500,
                    detail="Image generation timed out after 60 seconds",
                )

            # Use the generated image as reference
            gen_request.image_id = intermediate_image_id

        # Now we have an image reference, resolve it
        start_image_url = resolve_image_reference(
            asset_id=gen_request.asset_id,
            image_id=gen_request.image_id,
            video_id=gen_request.video_id,
        )

        # Build model-specific input parameters
        model_id = gen_request.model.value

        if gen_request.model == VideoModel.SEEDANCE:
            # ByteDance Seedance-1-lite parameters
            model_input = {
                "prompt": gen_request.prompt or "high quality video",
                "image": start_image_url,
                "duration": 5,  # 2-12 seconds, default 5
                "resolution": "720p",  # 480p, 720p, or 1080p
                "aspect_ratio": "16:9",  # 16:9, 4:3, 1:1, 3:4, 9:16, 21:9, 9:21
                "fps": 24,  # Fixed at 24fps
                "camera_fixed": False,  # Whether to fix camera position
            }
        elif gen_request.model == VideoModel.KLING:
            # Kling v2.1 parameters
            model_input = {
                "prompt": gen_request.prompt or "high quality video",
                "start_image": start_image_url,
                "mode": "pro",
                "duration": 5,
                "negative_prompt": "",
            }
        else:
            raise HTTPException(
                status_code=400, detail=f"Unsupported model: {gen_request.model}"
            )

        # Call Replicate for video generation
        base_url = settings.BASE_URL
        use_webhooks = base_url.startswith("https://")

        payload = {
            "input": model_input,
        }
        if use_webhooks:
            payload["webhook"] = f"{base_url}/api/webhooks/replicate"
            payload["webhook_events_filter"] = ["completed"]

        url = f"https://api.replicate.com/v1/models/{model_id}/predictions"
        headers = {
            "Authorization": f"Token {settings.REPLICATE_API_KEY}",
            "Content-Type": "application/json",
        }

        response = requests.post(url, json=payload, headers=headers)
        response.raise_for_status()
        prediction = response.json()

        # Save to database with client_id and campaign_id
        video_id = save_generated_video(
            prompt=model_input["prompt"],
            video_url="pending",
            model_id=model_id,
            parameters=model_input,
            collection=None,
            metadata={
                "replicate_id": prediction["id"],
                "prediction_url": prediction.get("urls", {}).get("get"),
            },
            status="processing",
            brief_id=None,
            client_id=gen_request.client_id,
            campaign_id=gen_request.campaign_id,
        )

        # Queue background processing
        background_tasks.add_task(
            process_video_generation_background,
            video_id=video_id,
            prediction_url=prediction.get("urls", {}).get("get"),
            api_key=settings.REPLICATE_API_KEY,
            model_id=model_id,
            input_params=model_input,
            collection=None,
        )

        # Return immediately with video ID
        result = {"video_id": video_id, "status": "processing"}
        if intermediate_image_id:
            result["intermediate_image_id"] = intermediate_image_id

        return result

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error generating video: {str(e)}")
        import traceback

        traceback.print_exc()
        raise HTTPException(
            status_code=500, detail=f"Failed to generate video: {str(e)}"
        )


@app.post("/api/v2/generate/audio")
@limiter.limit("10/minute")
async def generate_audio(
    request: Request,
    gen_request: AudioGenerationRequest,
    background_tasks: BackgroundTasks,
    current_user: Dict = Depends(verify_auth),
):
    """
    Generate audio/music using the selected model (default: meta/musicgen).

    Supports:
    - meta/musicgen (default)
    - riffusion/riffusion

    Rate limit: 10 requests per minute per user
    """
    try:
        # Build model-specific input parameters
        model_id = gen_request.model.value

        if gen_request.model == AudioModel.MUSICGEN:
            # MusicGen parameters
            model_input = {
                "prompt": gen_request.prompt,
                "model_version": "stereo-melody-large",
                "duration": gen_request.duration or 8,
                "temperature": 1,
                "top_k": 250,
                "top_p": 0,
                "classifier_free_guidance": 3,
                "output_format": "mp3",
            }
        elif gen_request.model == AudioModel.RIFFUSION:
            # Riffusion parameters
            model_input = {
                "prompt_a": gen_request.prompt,
                "denoising": 0.75,
                "num_inference_steps": 50,
                "seed_image_id": "vibes",
            }
        else:
            raise HTTPException(
                status_code=400, detail=f"Unsupported model: {gen_request.model}"
            )

        # Call Replicate for audio generation
        base_url = settings.BASE_URL
        use_webhooks = base_url.startswith("https://")

        payload = {
            "input": model_input,
        }
        if use_webhooks:
            payload["webhook"] = f"{base_url}/api/webhooks/replicate"
            payload["webhook_events_filter"] = ["completed"]

        url = f"https://api.replicate.com/v1/models/{model_id}/predictions"
        headers = {
            "Authorization": f"Token {settings.REPLICATE_API_KEY}",
            "Content-Type": "application/json",
        }

        response = requests.post(url, json=payload, headers=headers)
        response.raise_for_status()
        prediction = response.json()

        # Save to database with client_id and campaign_id
        audio_id = save_generated_audio(
            prompt=model_input.get("prompt") or model_input.get("prompt_a"),
            audio_url="pending",
            model_id=model_id,
            parameters=model_input,
            collection=None,
            metadata={
                "replicate_id": prediction["id"],
                "prediction_url": prediction.get("urls", {}).get("get"),
            },
            status="processing",
            brief_id=None,
            client_id=gen_request.client_id,
            campaign_id=gen_request.campaign_id,
            duration=gen_request.duration,
        )

        # Launch background task to poll for completion and download audio
        background_tasks.add_task(
            process_audio_generation_background,
            audio_id=audio_id,
            prediction_url=prediction.get("urls", {}).get("get"),
            api_key=settings.REPLICATE_API_KEY,
            model_id=model_id,
            input_params=model_input,
            collection=None,
        )

        logger.info(
            f"Audio generation started: audio_id={audio_id}, model={model_id}, replicate_id={prediction['id']}"
        )

        # Return immediately with audio ID
        return {"audio_id": audio_id, "status": "processing", "model": model_id}

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error generating audio: {str(e)}")
        import traceback

        traceback.print_exc()
        raise HTTPException(
            status_code=500, detail=f"Failed to generate audio: {str(e)}"
        )


@app.get("/api/v2/clients/{client_id}/generated-images")
async def get_client_generated_images(
    client_id: str,
    status: Optional[str] = None,
    limit: int = 50,
    current_user: Dict = Depends(verify_auth),
):
    """
    Get all generated images for a specific client.

    Query parameters:
    - status: Optional filter by status (processing, completed, failed, canceled)
    - limit: Maximum number of images to return (default: 50)
    """
    try:
        images = get_generated_images_by_client(client_id, status, limit)
        return {"client_id": client_id, "count": len(images), "images": images}
    except Exception as e:
        logger.error(f"Error fetching images for client {client_id}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to fetch images: {str(e)}")


@app.get("/api/v2/clients/{client_id}/generated-videos")
async def get_client_generated_videos(
    client_id: str,
    status: Optional[str] = None,
    limit: int = 50,
    current_user: Dict = Depends(verify_auth),
):
    """
    Get all generated videos for a specific client.

    Query parameters:
    - status: Optional filter by status (processing, completed, failed, canceled)
    - limit: Maximum number of videos to return (default: 50)
    """
    try:
        videos = get_generated_videos_by_client(client_id, status, limit)
        return {"client_id": client_id, "count": len(videos), "videos": videos}
    except Exception as e:
        logger.error(f"Error fetching videos for client {client_id}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to fetch videos: {str(e)}")


@app.get("/api/v2/campaigns/{campaign_id}/generated-images")
async def get_campaign_generated_images(
    campaign_id: str,
    status: Optional[str] = None,
    limit: int = 50,
    current_user: Dict = Depends(verify_auth),
):
    """
    Get all generated images for a specific campaign.

    Query parameters:
    - status: Optional filter by status (processing, completed, failed, canceled)
    - limit: Maximum number of images to return (default: 50)
    """
    try:
        images = get_generated_images_by_campaign(campaign_id, status, limit)
        return {"campaign_id": campaign_id, "count": len(images), "images": images}
    except Exception as e:
        logger.error(f"Error fetching images for campaign {campaign_id}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to fetch images: {str(e)}")


@app.get("/api/v2/campaigns/{campaign_id}/generated-videos")
async def get_campaign_generated_videos(
    campaign_id: str,
    status: Optional[str] = None,
    limit: int = 50,
    current_user: Dict = Depends(verify_auth),
):
    """
    Get all generated videos for a specific campaign.

    Query parameters:
    - status: Optional filter by status (processing, completed, failed, canceled)
    - limit: Maximum number of videos to return (default: 50)
    """
    try:
        videos = get_generated_videos_by_campaign(campaign_id, status, limit)
        return {"campaign_id": campaign_id, "count": len(videos), "videos": videos}
    except Exception as e:
        logger.error(f"Error fetching videos for campaign {campaign_id}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to fetch videos: {str(e)}")


# ============================================================================
# Creative Brief Parsing Endpoints
# ============================================================================

# Include the prompt parser router
app.include_router(parse_api.router, prefix="/api/creative", tags=["creative"])
app.include_router(briefs_api.router, prefix="/api/creative", tags=["creative"])

# Include clients and campaigns router (for ad-video-gen frontend)
app.include_router(clients_campaigns_router, prefix="/api", tags=["Core Entities"])

# Include v3 API router (tags are defined within the router)
app.include_router(v3_router)
app.include_router(luigi_router)

# ============================================
# Video/Image Retry Endpoints
# ============================================


@app.post("/api/videos/{video_id}/retry")
async def retry_video_processing(
    video_id: int,
    background_tasks: BackgroundTasks,
    current_user: Dict = Depends(verify_auth),
):
    """
    Retry fetching a video from Replicate that may have failed webhook processing.
    Checks the Replicate prediction status and downloads the video if ready.
    """
    from .database import get_video_by_id

    video = get_video_by_id(video_id)
    if not video:
        raise HTTPException(status_code=404, detail="Video not found")

    # Get replicate prediction URL from metadata
    metadata = video.get("metadata", {})
    prediction_url = metadata.get("prediction_url")
    replicate_id = metadata.get("replicate_id")

    if not prediction_url and not replicate_id:
        raise HTTPException(
            status_code=400,
            detail="Video has no Replicate prediction URL or ID in metadata",
        )

    # Construct prediction URL if we only have ID
    if not prediction_url and replicate_id:
        prediction_url = f"https://api.replicate.com/v1/predictions/{replicate_id}"

    # Check current status
    if video["status"] == "completed":
        return {
            "message": "Video already completed",
            "video_id": video_id,
            "status": "completed",
        }

    # Retry the download in background
    def retry_task():
        import requests

        api_key = settings.REPLICATE_API_KEY
        if not api_key:
            print(f"Cannot retry video {video_id}: No Replicate API key")
            return

        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json",
        }

        try:
            # Check prediction status
            response = requests.get(prediction_url, headers=headers)
            response.raise_for_status()
            pred_data = response.json()

            status = pred_data.get("status")

            if status == "succeeded":
                output = pred_data.get("output", [])
                if isinstance(output, str):
                    output = [output]

                video_url = output[0] if output else ""

                if video_url:
                    # Try to download
                    db_url = download_and_save_video(video_url, video_id)
                    update_video_status(
                        video_id=video_id,
                        status="completed",
                        video_url=db_url,
                        metadata={
                            "replicate_id": pred_data.get("id"),
                            "prediction_url": prediction_url,
                            "original_url": video_url,
                            "retried": True,
                        },
                    )
                    print(f"Video {video_id} retry successful")
                else:
                    update_video_status(
                        video_id=video_id,
                        status="failed",
                        metadata={"error": "No video URL in response", "retried": True},
                    )
            elif status in ["failed", "canceled"]:
                error = pred_data.get("error", "Unknown error")
                update_video_status(
                    video_id=video_id,
                    status=status,
                    metadata={
                        "error": error,
                        "replicate_id": pred_data.get("id"),
                        "retried": True,
                    },
                )
            elif status == "processing":
                # Still processing, don't change status
                print(f"Video {video_id} still processing on Replicate")

        except Exception as e:
            print(f"Error retrying video {video_id}: {e}")
            import traceback

            traceback.print_exc()

    background_tasks.add_task(retry_task)

    return {
        "message": "Retry initiated",
        "video_id": video_id,
        "prediction_url": prediction_url,
    }


@app.post("/api/videos/retry-all-stuck")
async def retry_all_stuck_videos(
    background_tasks: BackgroundTasks,
    current_user: Dict = Depends(get_current_admin_user),
):
    """
    Admin endpoint: Retry all videos stuck in 'processing' status.
    Useful for recovering from webhook failures.
    """
    from .database import get_db

    # Find all videos stuck in processing
    stuck_videos = []
    with get_db() as conn:
        rows = conn.execute(
            """
            SELECT id, metadata FROM generated_videos
            WHERE status = 'processing'
            AND (json_extract(metadata, '$.prediction_url') IS NOT NULL
                 OR json_extract(metadata, '$.replicate_id') IS NOT NULL)
            """
        ).fetchall()

        for row in rows:
            stuck_videos.append(
                {
                    "id": row["id"],
                    "metadata": json.loads(row["metadata"]) if row["metadata"] else {},
                }
            )

    if not stuck_videos:
        return {"message": "No stuck videos found", "count": 0}

    # Retry each one
    def retry_all_task():
        import requests

        api_key = settings.REPLICATE_API_KEY
        if not api_key:
            print("Cannot retry videos: No Replicate API key")
            return

        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json",
        }

        for video in stuck_videos:
            video_id = video["id"]
            metadata = video["metadata"]

            prediction_url = metadata.get("prediction_url")
            replicate_id = metadata.get("replicate_id")

            if not prediction_url and replicate_id:
                prediction_url = (
                    f"https://api.replicate.com/v1/predictions/{replicate_id}"
                )

            if not prediction_url:
                print(f"Skipping video {video_id}: no prediction URL")
                continue

            try:
                response = requests.get(prediction_url, headers=headers)
                response.raise_for_status()
                pred_data = response.json()

                status = pred_data.get("status")

                if status == "succeeded":
                    output = pred_data.get("output", [])
                    if isinstance(output, str):
                        output = [output]

                    video_url = output[0] if output else ""

                    if video_url:
                        db_url = download_and_save_video(video_url, video_id)
                        update_video_status(
                            video_id=video_id,
                            status="completed",
                            video_url=db_url,
                            metadata={
                                "replicate_id": pred_data.get("id"),
                                "prediction_url": prediction_url,
                                "original_url": video_url,
                                "bulk_retried": True,
                            },
                        )
                        print(f"Bulk retry: Video {video_id} completed")
                elif status in ["failed", "canceled"]:
                    error = pred_data.get("error", "Unknown error")
                    update_video_status(
                        video_id=video_id,
                        status=status,
                        metadata={
                            "error": error,
                            "replicate_id": pred_data.get("id"),
                            "bulk_retried": True,
                        },
                    )
                    print(f"Bulk retry: Video {video_id} {status}")

            except Exception as e:
                print(f"Error bulk retrying video {video_id}: {e}")

    background_tasks.add_task(retry_all_task)

    return {
        "message": f"Retry initiated for {len(stuck_videos)} stuck videos",
        "count": len(stuck_videos),
        "video_ids": [v["id"] for v in stuck_videos],
    }


# ============================================
# Database Administration Endpoints
# ============================================


@app.get("/api/db/schema", tags=["Database"])
async def get_database_schema(current_user: Dict = Depends(get_current_admin_user)):
    """
    Get the complete database schema (all tables and their columns).
    Requires admin authentication.
    """
    from .database import get_db

    try:
        with get_db() as conn:
            cursor = conn.cursor()

            # Get all tables
            cursor.execute(
                "SELECT name FROM sqlite_master WHERE type='table' ORDER BY name"
            )
            tables = [row[0] for row in cursor.fetchall()]

            schema = {}
            for table in tables:
                # Get table info
                cursor.execute(f"PRAGMA table_info({table})")
                columns = []
                for row in cursor.fetchall():
                    columns.append(
                        {
                            "cid": row[0],
                            "name": row[1],
                            "type": row[2],
                            "notnull": bool(row[3]),
                            "default_value": row[4],
                            "primary_key": bool(row[5]),
                        }
                    )

                # Get indexes
                cursor.execute(f"PRAGMA index_list({table})")
                indexes = [
                    {"name": row[1], "unique": bool(row[2])}
                    for row in cursor.fetchall()
                ]

                # Get foreign keys
                cursor.execute(f"PRAGMA foreign_key_list({table})")
                foreign_keys = []
                for row in cursor.fetchall():
                    foreign_keys.append(
                        {"id": row[0], "table": row[2], "from": row[3], "to": row[4]}
                    )

                schema[table] = {
                    "columns": columns,
                    "indexes": indexes,
                    "foreign_keys": foreign_keys,
                }

            # Get triggers
            cursor.execute(
                "SELECT name, tbl_name, sql FROM sqlite_master WHERE type='trigger' ORDER BY name"
            )
            triggers = [
                {"name": row[0], "table": row[1], "sql": row[2]}
                for row in cursor.fetchall()
            ]

            return {"tables": schema, "triggers": triggers, "total_tables": len(tables)}
    except Exception as e:
        logger.error(f"Failed to get database schema: {e}")
        raise HTTPException(
            status_code=500, detail=f"Failed to get database schema: {str(e)}"
        )


@app.get("/api/db/download", tags=["Database"])
async def download_database(current_user: Dict = Depends(get_current_admin_user)):
    """
    Download the complete SQLite database file.
    Requires admin authentication.
    """
    from .database import DB_PATH
    import shutil
    from tempfile import NamedTemporaryFile

    try:
        # Create a temporary copy to avoid locking issues
        with NamedTemporaryFile(delete=False, suffix=".db") as tmp_file:
            shutil.copy2(DB_PATH, tmp_file.name)
            tmp_path = tmp_file.name

        # Return the database file
        return FileResponse(
            path=tmp_path,
            media_type="application/x-sqlite3",
            filename="scenes.db",
            headers={"Content-Disposition": "attachment; filename=scenes.db"},
        )
    except Exception as e:
        logger.error(f"Failed to download database: {e}")
        raise HTTPException(
            status_code=500, detail=f"Failed to download database: {str(e)}"
        )


class SQLQueryRequest(BaseModel):
    query: str
    params: Optional[List[Any]] = None


@app.post("/api/db/query", tags=["Database"])
async def execute_sql_query(
    request: SQLQueryRequest, current_user: Dict = Depends(get_current_admin_user)
):
    """
    Execute a raw SQL query against the database.

    ⚠️ WARNING: This is a powerful endpoint. Only SELECT queries are allowed for safety.

    Supports parameterized queries using ? placeholders.

    Example:
    ```json
    {
        "query": "SELECT * FROM users WHERE id = ?",
        "params": [1]
    }
    ```

    Requires admin authentication.
    """
    from .database import get_db

    # Security: Only allow SELECT queries (read-only)
    query_upper = request.query.strip().upper()
    if not query_upper.startswith("SELECT"):
        raise HTTPException(
            status_code=403,
            detail="Only SELECT queries are allowed. Use database tools for modifications.",
        )

    # Additional safety checks
    dangerous_keywords = ["ATTACH", "DETACH", "PRAGMA"]
    if any(keyword in query_upper for keyword in dangerous_keywords):
        raise HTTPException(
            status_code=403,
            detail=f"Query contains forbidden keywords: {', '.join(dangerous_keywords)}",
        )

    try:
        with get_db() as conn:
            cursor = conn.cursor()

            # Execute query with parameters if provided
            if request.params:
                cursor.execute(request.query, request.params)
            else:
                cursor.execute(request.query)

            # Fetch results
            rows = cursor.fetchall()

            # Get column names
            if cursor.description:
                columns = [desc[0] for desc in cursor.description]
            else:
                columns = []

            # Convert rows to list of dicts
            results = []
            for row in rows:
                results.append(dict(zip(columns, row)))

            return {
                "query": request.query,
                "row_count": len(results),
                "columns": columns,
                "results": results,
            }

    except Exception as e:
        logger.error(f"SQL query failed: {e}")
        raise HTTPException(status_code=400, detail=f"Query execution failed: {str(e)}")


# ============================================================================
# Frontend Serving (catch-all route - must be last)
# ============================================================================


@app.get("/{full_path:path}")
async def serve_frontend(full_path: str):
    """Serve the frontend application for all non-API routes."""
    # Don't intercept API routes - they should be handled by their specific endpoints
    if full_path.startswith("api/") or full_path.startswith("data/"):
        raise HTTPException(status_code=404, detail="Not found")

    # Check if we're in production mode with static files
    if STATIC_DIR.exists() and STATIC_DIR.is_dir():
        index_file = STATIC_DIR / "index.html"
        if index_file.exists():
            return FileResponse(str(index_file))

    # Fallback for development or if static files don't exist
    return {"message": "Frontend not built. Run 'npm run build' to build the frontend."}


if __name__ == "__main__":
    print("Starting Physics Simulator API server...")
    uvicorn.run(
        "backend.main:app",
        host=settings.HOST,
        port=settings.PORT,
        reload=False,  # Disable reload in production
    )
</file>

<file path="backend/api/v3/router.py">
"""
FastAPI router for v3 API endpoints.

This router provides a simplified, frontend-aligned API that matches
the data requirements of the Next.js frontend with proper Pydantic models.
"""

from fastapi import (
    APIRouter,
    Depends,
    HTTPException,
    UploadFile,
    File,
    Query,
    BackgroundTasks,
    Request,
)
from typing import List, Optional, Dict, Any, cast
from datetime import datetime
import logging
import json
import uuid
import mimetypes
import asyncio
from asyncio import Semaphore

# Configure logging
logger = logging.getLogger(__name__)

from .models import (
    APIResponse,
    Client,
    ClientCreateRequest,
    ClientUpdateRequest,
    Campaign,
    CampaignCreateRequest,
    CampaignUpdateRequest,
    Job,
    JobCreateRequest,
    JobActionRequest,
    JobStatus,
    JobAction,
    CostEstimate,
    DryRunRequest,
    Asset,
    UploadAssetInput,
    UnifiedAssetUploadInput,
    SceneAudioRequest,
    ScenePrompt,
    PropertyVideoRequest,
)
from ...schemas.assets import UploadAssetFromUrlInput, BulkAssetFromUrlInput

from ...services.scene_audio_generator import generate_scene_audio_track
from ...database_helpers import (
    create_client,
    get_client_by_id,
    list_clients,
    update_client,
    delete_client,
    get_client_stats,
    create_campaign,
    get_campaign_by_id,
    list_campaigns,
    update_campaign,
    delete_campaign,
    get_campaign_stats,
    create_asset,
    get_asset_by_id,
    list_assets,
    update_asset,
    delete_asset,
    create_job_scene,
    get_scenes_by_job,
    get_scene_by_id,
    update_job_scene,
    delete_job_scene,
)
from ...database import update_job_parameters
from ...auth import verify_auth
from ...services.storyboard_generator import generate_storyboard_task
from ...services.video_renderer import render_video_task
from ...services.replicate_client import ReplicateClient
from ...services.asset_downloader import (
    download_asset_from_url,
    store_blob,
    AssetDownloadError,
)
from ...services.scene_generator import (
    generate_scenes,
    regenerate_scene,
    SceneGenerationError,
)
from ...database import (
    get_job,
    update_job_progress,
    approve_storyboard,
    create_video_job,
    update_video_status,
    get_audio_by_id,
)
from ...config import get_settings

# Initialize router (tags are set per endpoint for better organization)
router = APIRouter(prefix="/api/v3")
settings = get_settings()


# ============================================================================
# Helper Functions
# ============================================================================


def get_current_timestamp() -> str:
    """Get current timestamp in ISO format"""
    return datetime.utcnow().isoformat() + "Z"


def create_api_meta(
    page: Optional[int] = None, total: Optional[int] = None
) -> Dict[str, Any]:
    """Create standard API meta object"""
    meta: Dict[str, Any] = {"timestamp": get_current_timestamp()}
    if page is not None:
        meta["page"] = page
    if total is not None:
        meta["total"] = total
    return meta


# ============================================================================
# Client Endpoints
# ============================================================================


@router.get("/clients", response_model=APIResponse, tags=["v3-clients"])
async def get_clients(
    limit: int = Query(100, ge=1, le=1000),
    offset: int = Query(0, ge=0),
    current_user: Dict = Depends(verify_auth),
) -> APIResponse:
    """Get all clients for the authenticated user"""
    logger.info(
        f"V3 clients endpoint called by user {current_user.get('id')} with limit={limit}, offset={offset}"
    )

    try:
        logger.info(f"Calling list_clients for user {current_user['id']}")
        clients = list_clients(current_user["id"], limit=limit, offset=offset)

        logger.info(f"Retrieved {len(clients)} clients from database")
        if clients:
            logger.info(f"First client sample: {clients[0]}")
            logger.info(
                f"Client keys: {list(clients[0].keys()) if clients[0] else 'No clients'}"
            )

        meta = create_api_meta(page=(offset // limit) + 1, total=len(clients))
        logger.info(f"Response meta: {meta}")

        # Try to validate the response against the Client model
        try:
            from .models import Client

            if clients:
                # Try to validate the first client
                test_client = Client(**clients[0])
                logger.info(
                    f"Client model validation successful for client {clients[0]['id']}"
                )
        except Exception as validation_error:
            logger.error(f"Client model validation failed: {validation_error}")
            logger.error(f"Client data: {clients[0] if clients else 'No clients'}")

        response = APIResponse.success(data=clients, meta=meta)
        logger.info(f"Returning successful response with {len(clients)} clients")

        return response
    except Exception as e:
        logger.error(f"Error in get_clients: {str(e)}", exc_info=True)
        return APIResponse.create_error(f"Failed to fetch clients: {str(e)}")


@router.get("/clients/{client_id}", response_model=APIResponse, tags=["v3-clients"])
async def get_client(
    client_id: str, current_user: Dict = Depends(verify_auth)
) -> APIResponse:
    """Get a specific client by ID"""
    logger.info(
        f"V3 get_client endpoint called for client_id={client_id} by user {current_user.get('id')}"
    )

    try:
        logger.info(f"Calling get_client_by_id for client {client_id}")
        client = get_client_by_id(client_id, current_user["id"])

        if not client:
            logger.warning(
                f"Client {client_id} not found for user {current_user['id']}"
            )
            return APIResponse.create_error("Client not found")

        logger.info(f"Retrieved client: {client}")
        logger.info(f"Client keys: {list(client.keys())}")

        # Try to validate the response against the Client model
        try:
            from .models import Client

            test_client = Client(**client)
            logger.info(f"Client model validation successful for client {client_id}")
        except Exception as validation_error:
            logger.error(
                f"Client model validation failed for client {client_id}: {validation_error}"
            )
            logger.error(f"Client data: {client}")

        response = APIResponse.success(data=client, meta=create_api_meta())
        logger.info(f"Returning successful response for client {client_id}")

        return response
    except Exception as e:
        logger.error(
            f"Error in get_client for client {client_id}: {str(e)}", exc_info=True
        )
        return APIResponse.create_error(f"Failed to fetch client: {str(e)}")


@router.post("/clients", response_model=APIResponse, tags=["v3-clients"])
async def create_new_client(
    request: ClientCreateRequest, current_user: Dict = Depends(verify_auth)
) -> APIResponse:
    """Create a new client"""
    try:
        client_id = create_client(
            user_id=current_user["id"],
            name=request.name,
            description=request.description or "",
            homepage=request.homepage,
            brand_guidelines=request.brandGuidelines.dict()
            if request.brandGuidelines
            else None,
            metadata=request.metadata,
        )

        # Fetch the created client
        client = get_client_by_id(client_id, current_user["id"])
        return APIResponse.success(data=client, meta=create_api_meta())
    except Exception as e:
        return APIResponse.create_error(f"Failed to create client: {str(e)}")


@router.put("/clients/{client_id}", response_model=APIResponse, tags=["v3-clients"])
async def update_existing_client(
    client_id: str,
    request: ClientUpdateRequest,
    current_user: Dict = Depends(verify_auth),
) -> APIResponse:
    """Update an existing client"""
    try:
        success = update_client(
            client_id=client_id,
            user_id=current_user["id"],
            name=request.name,
            description=request.description,
            homepage=request.homepage,
            brand_guidelines=request.brandGuidelines.dict()
            if request.brandGuidelines
            else None,
            metadata=request.metadata,
        )

        if not success:
            return APIResponse.create_error("Client not found or update failed")

        # Fetch the updated client
        client = get_client_by_id(client_id, current_user["id"])
        return APIResponse.success(data=client, meta=create_api_meta())
    except Exception as e:
        return APIResponse.create_error(f"Failed to update client: {str(e)}")


@router.delete("/clients/{client_id}", response_model=APIResponse, tags=["v3-clients"])
async def delete_existing_client(
    client_id: str, current_user: Dict = Depends(verify_auth)
) -> APIResponse:
    """Delete a client"""
    try:
        success = delete_client(client_id, current_user["id"])
        if not success:
            return APIResponse.create_error("Client not found")

        return APIResponse.success(
            data={"message": "Client deleted successfully"}, meta=create_api_meta()
        )
    except Exception as e:
        return APIResponse.create_error(f"Failed to delete client: {str(e)}")


@router.get(
    "/clients/{client_id}/stats", response_model=APIResponse, tags=["v3-clients"]
)
async def get_client_statistics(
    client_id: str, current_user: Dict = Depends(verify_auth)
) -> APIResponse:
    """Get statistics for a client"""
    try:
        stats = get_client_stats(client_id, current_user["id"])
        if stats is None:
            return APIResponse.create_error("Client not found")

        return APIResponse.success(data=stats, meta=create_api_meta())
    except Exception as e:
        return APIResponse.create_error(f"Failed to fetch client stats: {str(e)}")


# ============================================================================
# Campaign Endpoints
# ============================================================================


@router.get("/campaigns", response_model=APIResponse, tags=["v3-campaigns"])
async def get_campaigns(
    client_id: Optional[str] = Query(None),
    limit: int = Query(100, ge=1, le=1000),
    offset: int = Query(0, ge=0),
    current_user: Dict = Depends(verify_auth),
) -> APIResponse:
    """Get campaigns, optionally filtered by client"""
    try:
        # Strip whitespace from client_id if provided
        clean_client_id = client_id.strip() if client_id else None

        campaigns = list_campaigns(
            user_id=None,  # Allow access to all campaigns
            client_id=clean_client_id, limit=limit, offset=offset
        )
        meta = create_api_meta(page=(offset // limit) + 1, total=len(campaigns))
        return APIResponse.success(data=campaigns, meta=meta)
    except Exception as e:
        return APIResponse.create_error(f"Failed to fetch campaigns: {str(e)}")


@router.get(
    "/campaigns/{campaign_id}", response_model=APIResponse, tags=["v3-campaigns"]
)
async def get_campaign(
    campaign_id: str, current_user: Dict = Depends(verify_auth)
) -> APIResponse:
    """Get a specific campaign by ID"""
    try:
        campaign = get_campaign_by_id(campaign_id, current_user["id"])
        if not campaign:
            return APIResponse.create_error("Campaign not found")

        return APIResponse.success(data=campaign, meta=create_api_meta())
    except Exception as e:
        return APIResponse.create_error(f"Failed to fetch campaign: {str(e)}")


@router.post("/campaigns", response_model=APIResponse, tags=["v3-campaigns"])
async def create_new_campaign(
    request: CampaignCreateRequest, current_user: Dict = Depends(verify_auth)
) -> APIResponse:
    """Create a new campaign"""
    try:
        campaign_id = create_campaign(
            user_id=current_user["id"],
            client_id=request.clientId,
            name=request.name,
            goal=request.goal,
            status=request.status,
            product_url=request.productUrl,
            brief=request.brief,
            metadata=request.metadata,
        )

        # Fetch the created campaign
        campaign = get_campaign_by_id(campaign_id, current_user["id"])
        return APIResponse.success(data=campaign, meta=create_api_meta())
    except Exception as e:
        return APIResponse.create_error(f"Failed to create campaign: {str(e)}")


@router.put(
    "/campaigns/{campaign_id}", response_model=APIResponse, tags=["v3-campaigns"]
)
async def update_existing_campaign(
    campaign_id: str,
    request: CampaignUpdateRequest,
    current_user: Dict = Depends(verify_auth),
) -> APIResponse:
    """Update an existing campaign"""
    try:
        success = update_campaign(
            campaign_id=campaign_id,
            user_id=current_user["id"],
            name=request.name,
            goal=request.goal,
            status=request.status,
            product_url=request.productUrl,
            brief=request.brief,
            metadata=request.metadata,
        )

        if not success:
            return APIResponse.create_error("Campaign not found or update failed")

        # Fetch the updated campaign
        campaign = get_campaign_by_id(campaign_id, current_user["id"])
        return APIResponse.success(data=campaign, meta=create_api_meta())
    except Exception as e:
        return APIResponse.create_error(f"Failed to update campaign: {str(e)}")


@router.delete(
    "/campaigns/{campaign_id}", response_model=APIResponse, tags=["v3-campaigns"]
)
async def delete_existing_campaign(
    campaign_id: str, current_user: Dict = Depends(verify_auth)
) -> APIResponse:
    """Delete a campaign"""
    try:
        success = delete_campaign(campaign_id, current_user["id"])
        if not success:
            return APIResponse.create_error("Campaign not found")

        return APIResponse.success(
            data={"message": "Campaign deleted successfully"}, meta=create_api_meta()
        )
    except Exception as e:
        return APIResponse.create_error(f"Failed to delete campaign: {str(e)}")


@router.get(
    "/campaigns/{campaign_id}/stats", response_model=APIResponse, tags=["v3-campaigns"]
)
async def get_campaign_statistics(
    campaign_id: str, current_user: Dict = Depends(verify_auth)
) -> APIResponse:
    """Get statistics for a campaign"""
    try:
        stats = get_campaign_stats(campaign_id, current_user["id"])
        if stats is None:
            return APIResponse.create_error("Campaign not found")

        return APIResponse.success(data=stats, meta=create_api_meta())
    except Exception as e:
        return APIResponse.create_error(f"Failed to fetch campaign stats: {str(e)}")


# ============================================================================
# Asset Endpoints
# ============================================================================


@router.get("/assets", response_model=APIResponse, tags=["v3-assets"])
async def get_assets(
    client_id: Optional[str] = Query(None),
    campaign_id: Optional[str] = Query(None),
    asset_type: Optional[str] = Query(None),
    limit: int = Query(100, ge=1, le=1000),
    offset: int = Query(0, ge=0),
    current_user: Dict = Depends(verify_auth),
) -> APIResponse:
    """Get assets with optional filtering"""
    try:
        assets = list_assets(
            user_id=None,  # Allow access to all campaigns
            client_id=client_id,
            campaign_id=campaign_id,
            asset_type=asset_type,
            limit=limit,
            offset=offset,
        )
        meta = create_api_meta(page=(offset // limit) + 1, total=len(assets))
        return APIResponse.success(data=assets, meta=meta)
    except Exception as e:
        return APIResponse.create_error(f"Failed to fetch assets: {str(e)}")


@router.get("/assets/{asset_id}", response_model=APIResponse, tags=["v3-assets"])
async def get_asset(
    asset_id: str, current_user: Dict = Depends(verify_auth)
) -> APIResponse:
    """Get a specific asset by ID"""
    try:
        asset = get_asset_by_id(asset_id)
        if not asset:
            return APIResponse.create_error("Asset not found")

        return APIResponse.success(data=asset, meta=create_api_meta())
    except Exception as e:
        return APIResponse.create_error(f"Failed to fetch asset: {str(e)}")


@router.get("/assets/{asset_id}/data", tags=["v3-assets"])
async def get_asset_data(
    asset_id: str,
    request: Request,
):
    """
    Serve the binary asset data.

    This is a public endpoint with no authentication required.
    Allows external services (like Replicate) to access assets directly.
    """
    from fastapi.responses import Response
    from ...database_helpers import get_db

    # Public endpoint - no authentication required
    # This allows external services (like Replicate) to access assets directly

    try:
        # Query asset directly from database to get blob_id and blob_data
        with get_db() as conn:
            row = conn.execute(
                """
                SELECT blob_id, blob_data, format, name
                FROM assets
                WHERE id = ?
                """,
                (asset_id,),
            ).fetchone()

            if not row:
                return APIResponse.create_error("Asset not found")

            blob_id = row["blob_id"]
            blob_data = row["blob_data"]
            asset_format = row["format"]
            asset_name = row["name"]

        # Check if asset has blob_id (V3 blob storage)
        if blob_id:
            from ...services.asset_downloader import get_blob_by_id

            blob_result = get_blob_by_id(blob_id)

            if blob_result:
                data, content_type = blob_result
                return Response(
                    content=data,
                    media_type=content_type,
                    headers={
                        "Content-Disposition": f'inline; filename="{asset_name}"',
                        "Cache-Control": "public, max-age=31536000",
                    },
                )

        # Fallback to blob_data column (legacy storage)
        if blob_data:
            # Determine content type from format
            format_to_mime = {
                "jpg": "image/jpeg",
                "jpeg": "image/jpeg",
                "png": "image/png",
                "webp": "image/webp",
                "gif": "image/gif",
                "mp4": "video/mp4",
                "webm": "video/webm",
                "mov": "video/quicktime",
                "mp3": "audio/mpeg",
                "wav": "audio/wav",
                "ogg": "audio/ogg",
                "pdf": "application/pdf",
            }

            content_type = format_to_mime.get(
                asset_format.lower(), "application/octet-stream"
            )

            return Response(
                content=bytes(blob_data),
                media_type=content_type,
                headers={
                    "Content-Disposition": f'inline; filename="{asset_name}"',
                    "Cache-Control": "public, max-age=31536000",
                },
            )

        # No binary data available
        return APIResponse.create_error("Asset data not available")

    except Exception as e:
        return APIResponse.create_error(f"Failed to serve asset data: {str(e)}")


@router.get("/assets/{asset_id}/thumbnail", tags=["v3-assets"])
async def get_asset_thumbnail(asset_id: str, current_user: Dict = Depends(verify_auth)):
    """Serve the asset thumbnail"""
    from fastapi.responses import Response
    from ...database_helpers import get_db

    try:
        # Query asset to get thumbnail_blob_id
        with get_db() as conn:
            row = conn.execute(
                "SELECT thumbnail_blob_id, name FROM assets WHERE id = ?", (asset_id,)
            ).fetchone()

            if not row or not row["thumbnail_blob_id"]:
                return APIResponse.create_error("Thumbnail not available")

            thumbnail_blob_id = row["thumbnail_blob_id"]
            asset_name = row["name"]

        # Get thumbnail data from blob storage
        from ...services.asset_downloader import get_blob_by_id

        blob_result = get_blob_by_id(thumbnail_blob_id)

        if blob_result:
            data, content_type = blob_result
            return Response(
                content=data,
                media_type=content_type,
                headers={
                    "Content-Disposition": f'inline; filename="{asset_name}_thumbnail.jpg"',
                    "Cache-Control": "public, max-age=31536000",
                },
            )

        return APIResponse.create_error("Thumbnail data not available")

    except Exception as e:
        return APIResponse.create_error(f"Failed to serve thumbnail: {str(e)}")


@router.post("/assets", response_model=APIResponse, tags=["v3-assets"])
async def upload_asset(
    file: UploadFile = File(...),
    name: Optional[str] = None,
    type: Optional[str] = None,
    clientId: Optional[str] = None,
    campaignId: Optional[str] = None,
    tags: Optional[str] = None,  # JSON string
    current_user: Dict = Depends(verify_auth),
) -> APIResponse:
    """Upload a new asset"""
    try:
        # Parse tags if provided
        tags_list = None
        if tags:
            try:
                tags_list = json.loads(tags)
            except:
                tags_list = [tags]  # Single tag as string

        # Read file content
        file_content = await file.read()

        # Determine format
        filename = file.filename or "unknown"
        format_ext = filename.split(".")[-1] if "." in filename else "bin"

        # Generate ID first
        asset_id = str(uuid.uuid4())

        # Construct the serving URL (pointing to the existing v2 data endpoint)
        asset_url = f"/api/v2/assets/{asset_id}/data"

        # Create asset with specific ID and valid URL
        create_asset(
            asset_id=asset_id,
            name=name or filename,
            asset_type=type or "document",
            url=asset_url,
            format=format_ext,
            size=len(file_content),
            user_id=current_user["id"],
            client_id=clientId,
            campaign_id=campaignId,
            tags=tags_list,
            blob_data=file_content,
        )

        # Fetch the created asset
        asset = get_asset_by_id(asset_id)
        return APIResponse.success(data=asset, meta=create_api_meta())
    except Exception as e:
        return APIResponse.create_error(f"Failed to upload asset: {str(e)}")


@router.post("/assets/from-url", response_model=APIResponse, tags=["v3-assets"])
async def upload_asset_from_url(
    request: UploadAssetFromUrlInput, current_user: Dict = Depends(verify_auth)
) -> APIResponse:
    """Upload an asset by downloading it from a URL"""
    try:
        # Download asset from URL
        asset_data, content_type, metadata = download_asset_from_url(
            url=request.url, asset_type=request.type
        )

        # Store as blob in database
        blob_id = store_blob(asset_data, content_type)

        # Generate asset ID
        asset_id = str(uuid.uuid4())

        # Construct V3 serving URL
        asset_url = f"/api/v3/assets/{asset_id}/data"

        # Create asset record with blob reference
        create_asset(
            asset_id=asset_id,
            name=request.name,
            asset_type=request.type,
            url=asset_url,
            format=metadata.get("format", "unknown"),
            size=metadata.get("size", len(asset_data)),
            user_id=current_user["id"],
            client_id=request.clientId,
            campaign_id=request.campaignId,
            tags=request.tags,
            blob_id=blob_id,
            source_url=request.url,
            width=metadata.get("width"),
            height=metadata.get("height"),
            duration=metadata.get("duration"),
        )

        # Fetch the created asset
        asset = get_asset_by_id(asset_id)
        return APIResponse.success(data=asset, meta=create_api_meta())
    except AssetDownloadError as e:
        return APIResponse.create_error(f"Failed to download asset: {str(e)}")
    except Exception as e:
        return APIResponse.create_error(f"Failed to upload asset from URL: {str(e)}")


@router.post("/assets/from-urls", response_model=APIResponse, tags=["v3-assets"])
async def upload_assets_from_urls(
    request: BulkAssetFromUrlInput, current_user: Dict = Depends(verify_auth)
) -> APIResponse:
    """Upload multiple assets by downloading them from URLs"""

    async def process_single_asset(asset_item, semaphore):
        """Process a single asset upload"""
        async with semaphore:
            try:
                # Download asset from URL
                asset_data, content_type, metadata = download_asset_from_url(
                    url=asset_item.url, asset_type=asset_item.type
                )

                # Store as blob in database
                blob_id = store_blob(asset_data, content_type)

                # Generate thumbnail if applicable
                thumbnail_blob_id = None
                if asset_item.type in ["image", "video"]:
                    from ...services.asset_downloader import (
                        generate_and_store_thumbnail,
                    )

                    thumbnail_blob_id = generate_and_store_thumbnail(
                        asset_data, content_type, asset_item.type
                    )

                # Generate asset ID
                asset_id = str(uuid.uuid4())

                # Construct V3 serving URL
                asset_url = f"/api/v3/assets/{asset_id}/data"

                # Create asset record with blob reference
                create_asset(
                    asset_id=asset_id,
                    name=asset_item.name,
                    asset_type=asset_item.type,
                    url=asset_url,
                    format=metadata.get("format", "unknown"),
                    size=metadata.get("size", len(asset_data)),
                    user_id=current_user["id"],
                    client_id=request.clientId,
                    campaign_id=request.campaignId,
                    tags=asset_item.tags,
                    blob_id=blob_id,
                    source_url=asset_item.url,
                    thumbnail_blob_id=thumbnail_blob_id,
                    width=metadata.get("width"),
                    height=metadata.get("height"),
                    duration=metadata.get("duration"),
                )

                # Fetch the created asset
                asset = get_asset_by_id(asset_id)
                return {"asset": asset, "success": True, "error": None}

            except AssetDownloadError as e:
                logger.warning(
                    f"Failed to download asset from {asset_item.url}: {str(e)}"
                )
                return {
                    "asset": None,
                    "success": False,
                    "error": f"Failed to download: {str(e)}",
                }
            except Exception as e:
                logger.error(f"Failed to process asset {asset_item.name}: {str(e)}")
                return {
                    "asset": None,
                    "success": False,
                    "error": f"Processing failed: {str(e)}",
                }

    try:
        if not request.assets:
            return APIResponse.create_error("No assets provided")

        if len(request.assets) > 100:  # Limit bulk uploads to prevent abuse
            return APIResponse.create_error(
                "Maximum 100 assets allowed per bulk upload"
            )

        logger.info(
            f"Bulk uploading {len(request.assets)} assets for user {current_user['id']}"
        )

        # Create semaphore to limit concurrent downloads (max 2 simultaneous for production)
        # Lower limit prevents overwhelming the server during bulk uploads
        semaphore = Semaphore(2)

        # Process all assets concurrently
        tasks = [
            process_single_asset(asset_item, semaphore) for asset_item in request.assets
        ]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # Process results and handle exceptions
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"Exception in asset {i}: {str(result)}")
                processed_results.append(
                    {
                        "asset": None,
                        "success": False,
                        "error": f"Unexpected error: {str(result)}",
                    }
                )
            else:
                processed_results.append(result)

        # Calculate summary
        successful = sum(1 for r in processed_results if r["success"])
        failed = len(processed_results) - successful

        logger.info(f"Bulk upload completed: {successful} successful, {failed} failed")

        response_data = {
            "results": processed_results,
            "summary": {
                "total": len(processed_results),
                "successful": successful,
                "failed": failed,
            },
        }

        return APIResponse.success(data=response_data, meta=create_api_meta())

    except Exception as e:
        logger.error(f"Bulk asset upload failed: {str(e)}", exc_info=True)
        return APIResponse.create_error(f"Failed to upload assets: {str(e)}")


@router.post("/audio/generate-scenes", response_model=APIResponse, tags=["v3-audio"])
async def generate_scene_audio(
    request: SceneAudioRequest, current_user: Dict = Depends(verify_auth)
) -> APIResponse:
    """Generate continuous audio track from scene prompts using MusicGen continuation"""
    try:
        logger.info(
            f"Generating scene audio for user {current_user['id']}: {len(request.scenes)} scenes"
        )

        # Generate the audio track
        result = await generate_scene_audio_track(
            scenes=[scene.dict() for scene in request.scenes],
            default_duration=request.default_duration,
            model_id=request.model_id,
        )

        logger.info(f"Scene audio generation completed: {result}")

        return APIResponse.success(data=result, meta=create_api_meta())

    except Exception as e:
        logger.error(f"Scene audio generation failed: {str(e)}", exc_info=True)
        return APIResponse.create_error(f"Failed to generate scene audio: {str(e)}")


@router.post("/assets/unified", response_model=APIResponse, tags=["v3-assets"])
async def upload_asset_unified(
    request: UnifiedAssetUploadInput,
    file: Optional[UploadFile] = File(None),
    current_user: Dict = Depends(verify_auth),
) -> APIResponse:
    """Unified asset upload endpoint supporting both file uploads and URL downloads"""
    try:
        asset_data = None
        content_type = None
        metadata = {}
        source_url = None

        if request.uploadType == "file":
            # Handle direct file upload
            if not file:
                return APIResponse.create_error("File is required for file upload type")

            # Read file content
            asset_data = await file.read()

            # Determine content type and metadata
            filename = file.filename or "unknown"
            content_type = (
                file.content_type
                or mimetypes.guess_type(filename)[0]
                or "application/octet-stream"
            )

            # Basic metadata extraction
            metadata = {
                "size": len(asset_data),
                "format": filename.split(".")[-1] if "." in filename else "bin",
            }

            # Try to extract additional metadata for images
            if request.type == "image" and content_type.startswith("image/"):
                try:
                    from PIL import Image
                    import io

                    image = Image.open(io.BytesIO(asset_data))
                    metadata["width"] = image.width
                    metadata["height"] = image.height
                except:
                    pass

        elif request.uploadType == "url":
            # Handle URL download
            if not request.sourceUrl:
                return APIResponse.create_error(
                    "sourceUrl is required for URL upload type"
                )

            # Download asset from URL
            asset_data, content_type, metadata = download_asset_from_url(
                url=request.sourceUrl, asset_type=request.type
            )
            source_url = request.sourceUrl

        else:
            return APIResponse.create_error(f"Invalid uploadType: {request.uploadType}")

        # Store asset data as blob
        blob_id = store_blob(asset_data, content_type)

        # Generate asset ID
        asset_id = str(uuid.uuid4())

        # Construct V3 serving URL
        asset_url = f"/api/v3/assets/{asset_id}/data"

        # Generate thumbnail if requested and applicable
        thumbnail_blob_id = None
        if request.generateThumbnail and request.type in ["image", "video"]:
            from ...services.asset_downloader import generate_and_store_thumbnail

            thumbnail_blob_id = generate_and_store_thumbnail(
                asset_data, content_type, request.type
            )

        # Create asset record
        create_asset(
            asset_id=asset_id,
            name=request.name,
            asset_type=request.type,
            url=asset_url,
            format=metadata.get("format", "unknown"),
            size=metadata.get("size", len(asset_data)),
            user_id=current_user["id"],
            client_id=request.clientId,
            campaign_id=request.campaignId,
            tags=request.tags,
            blob_id=blob_id,
            source_url=source_url,
            thumbnail_blob_id=thumbnail_blob_id,
            width=metadata.get("width"),
            height=metadata.get("height"),
            duration=metadata.get("duration"),
        )

        # Fetch the created asset
        asset = get_asset_by_id(asset_id)
        return APIResponse.success(data=asset, meta=create_api_meta())

    except AssetDownloadError as e:
        return APIResponse.create_error(f"Failed to download asset: {str(e)}")
    except Exception as e:
        logger.error(f"Unified asset upload failed: {e}", exc_info=True)
        return APIResponse.create_error(f"Failed to upload asset: {str(e)}")


@router.delete("/assets/{asset_id}", response_model=APIResponse, tags=["v3-assets"])
async def delete_asset_v3(
    asset_id: str, current_user: Dict = Depends(verify_auth)
) -> APIResponse:
    """Delete an asset"""
    try:
        # Check if asset exists and belongs to user
        asset = get_asset_by_id(asset_id)
        if not asset:
            return APIResponse.create_error("Asset not found")

        # Delete the asset
        success = delete_asset(asset_id, current_user["id"])
        if not success:
            return APIResponse.create_error("Failed to delete asset or asset not found")

        return APIResponse.success(
            data={"message": "Asset deleted successfully"}, meta=create_api_meta()
        )
    except Exception as e:
        return APIResponse.create_error(f"Failed to delete asset: {str(e)}")


# ============================================================================
# Job Endpoints (Generation Workflow)
# ============================================================================


@router.post("/jobs", response_model=APIResponse, tags=["v3-jobs"])
async def create_job(
    request: JobCreateRequest,
    background_tasks: BackgroundTasks,
    current_user: Dict = Depends(verify_auth),
) -> APIResponse:
    """Create a new generation job"""
    try:
        # Process assets if provided
        processed_asset_ids = []
        if request.creative.assets:
            logger.info(
                f"Processing {len(request.creative.assets)} assets for job creation"
            )

            for asset_input in request.creative.assets:
                # If asset has a URL, download and store it
                if asset_input.url:
                    logger.info(
                        f"Downloading asset from URL: {asset_input.url[:50]}..."
                    )

                    # Determine asset type
                    asset_type = asset_input.type or "image"  # Default to image

                    # Download asset from URL
                    asset_data, content_type, metadata = download_asset_from_url(
                        url=asset_input.url, asset_type=asset_type
                    )

                    # Store as blob
                    blob_id = store_blob(asset_data, content_type)

                    # Generate asset ID
                    asset_id = str(uuid.uuid4())
                    asset_url = f"/api/v3/assets/{asset_id}/data"

                    # Generate thumbnail if applicable
                    thumbnail_blob_id = None
                    if asset_type in ["image", "video"]:
                        from ...services.asset_downloader import (
                            generate_and_store_thumbnail,
                        )

                        thumbnail_blob_id = generate_and_store_thumbnail(
                            asset_data, content_type, asset_type
                        )

                    # Create asset record
                    create_asset(
                        asset_id=asset_id,
                        name=asset_input.name or f"{asset_type}-{asset_id[:8]}",
                        asset_type=asset_type,
                        url=asset_url,
                        format=metadata.get("format", "unknown"),
                        size=metadata.get("size", len(asset_data)),
                        user_id=current_user["id"],
                        client_id=request.context.clientId,
                        campaign_id=request.context.campaignId,
                        blob_id=blob_id,
                        source_url=asset_input.url,
                        thumbnail_blob_id=thumbnail_blob_id,
                        width=metadata.get("width"),
                        height=metadata.get("height"),
                        duration=metadata.get("duration"),
                    )

                    processed_asset_ids.append(asset_id)
                    logger.info(f"Created asset {asset_id} from URL")

                # If asset has an ID, just use the existing asset
                elif asset_input.assetId:
                    # Verify asset exists
                    existing_asset = get_asset_by_id(asset_input.assetId)
                    if not existing_asset:
                        return APIResponse.create_error(
                            f"Asset not found: {asset_input.assetId}"
                        )
                    processed_asset_ids.append(asset_input.assetId)
                    logger.info(f"Using existing asset {asset_input.assetId}")

        # Build prompt from request data
        prompt = f"""
        Product: {request.adBasics.product}
        Target Audience: {request.adBasics.targetAudience}
        Key Message: {request.adBasics.keyMessage}
        Call to Action: {request.adBasics.callToAction}
        Style: {request.creative.direction.style}
        """

        # Create job using existing video job function
        audio_cost = 2.0 if request.generateAudio else 0.0
        job_id = create_video_job(
            prompt=prompt,
            model_id="v3-job",  # Placeholder model
            parameters={
                "context": request.context.dict(),
                "ad_basics": request.adBasics.dict(),
                "creative": request.creative.dict(),
                "advanced": request.advanced.dict() if request.advanced else None,
                "processed_asset_ids": processed_asset_ids,
                "generate_audio": request.generateAudio,
                "audio_cost": audio_cost,
            },
            estimated_cost=5.0 + audio_cost,
            client_id=request.context.clientId,
            status="scene_generation",
        )

        logger.info(
            f"Created job {job_id} (audio: {request.generateAudio}, cost: ${5.0 + audio_cost})"
        )

        logger.info(f"Created job {job_id} with audio enabled: {request.generateAudio}")

        # Generate scenes using AI
        logger.info(f"Generating scenes for job {job_id}")
        try:
            scenes = generate_scenes(
                ad_basics=request.adBasics.dict(),
                creative_direction=request.creative.direction.dict(),
                assets=processed_asset_ids,
                duration=request.creative.videoSpecs.duration,
                num_scenes=None,  # Auto-determine based on duration
            )

            # Store generated scenes in database
            for scene in scenes:
                create_job_scene(
                    job_id=job_id,
                    scene_number=scene["sceneNumber"],
                    duration=scene["duration"],
                    description=scene["description"],
                    script=scene.get("script"),
                    shot_type=scene.get("shotType"),
                    transition=scene.get("transition"),
                    assets=scene.get("assets", []),
                    metadata=scene.get("metadata", {}),
                )

            logger.info(f"Generated and stored {len(scenes)} scenes for job {job_id}")

            # Generate audio track if requested
            audio_info = None
            actual_cost = 5.0
            if request.generateAudio:
                try:
                    # Prepare scene prompts for audio generation
                    scene_prompts = []
                    for scene in scenes:
                        combined_prompt = f"{scene['description']}"
                        if scene.get("script"):
                            combined_prompt += f". Voiceover: {scene['script']}"

                        scene_prompts.append(
                            {
                                "scene_number": scene["sceneNumber"],
                                "prompt": combined_prompt,
                                "duration": scene["duration"],
                            }
                        )

                    logger.info(
                        f"🎵 Generating audio track for {len(scene_prompts)} scenes (job {job_id})"
                    )

                    # Generate the audio track
                    audio_result = await generate_scene_audio_track(
                        scenes=scene_prompts,
                        default_duration=4.0,
                        model_id="meta/musicgen",
                        user_id=current_user["id"],
                    )

                    audio_info = {
                        "status": "completed",
                        "audio_id": audio_result["audio_id"],
                        "audio_url": audio_result["audio_url"],
                        "total_duration": audio_result["total_duration"],
                        "scenes_processed": audio_result["scenes_processed"],
                        "model_used": audio_result["model_used"],
                    }

                    actual_cost += 2.0  # Audio cost
                    logger.info(
                        f"✓ Audio track generated successfully for job {job_id}: {audio_info['audio_id']}"
                    )

                    # Store audio info in job parameters for rendering
                    current_params = {
                        "context": request.context.dict(),
                        "ad_basics": request.adBasics.dict(),
                        "creative": request.creative.dict(),
                        "advanced": request.advanced.dict()
                        if request.advanced
                        else None,
                        "processed_asset_ids": processed_asset_ids,
                        "scenes": [
                            {k: v for k, v in s.items() if k != "metadata"}
                            for s in scenes
                        ],  # Store scenes without large metadata
                        "audio_info": audio_info,
                    }
                    update_job_parameters(job_id, current_params)

                except Exception as audio_error:
                    logger.error(
                        f"⚠️ Audio generation failed for job {job_id}: {audio_error}"
                    )
                    audio_info = {
                        "status": "failed",
                        "error": str(audio_error),
                        "requested": True,
                        "fallback_available": False,
                    }
                    # No additional cost for failed audio
            else:
                audio_info = {"status": "not_requested"}

            auto_approve = request.advanced.autoApprove if request.advanced else False

            if auto_approve:
                approve_storyboard(job_id)
                background_tasks.add_task(render_video_task, job_id)
                update_video_status(job_id, "video_processing")
                response_status = JobStatus.VIDEO_PROCESSING
            else:
                update_video_status(job_id, "storyboard_ready")
                response_status = JobStatus.STORYBOARD_READY

            # Prepare response with audio info and actual cost
            job_response = {
                "id": str(job_id),
                "status": response_status,
                "assetIds": processed_asset_ids,
                "scenes": scenes,
                "audio": audio_info,
                "estimatedCost": actual_cost,
                "createdAt": get_current_timestamp(),
                "updatedAt": get_current_timestamp(),
            }

            logger.info(
                f"Job {job_id} ready: {len(scenes)} scenes, audio: {audio_info.get('status', 'none')}, cost: ${actual_cost}"
            )
            return APIResponse.success(data=job_response, meta=create_api_meta())

        except SceneGenerationError as e:
            logger.error(f"Scene generation failed for job {job_id}: {e}")
            update_video_status(job_id, "failed")
            return APIResponse.create_error(f"Failed to generate scenes: {str(e)}")
    except AssetDownloadError as e:
        logger.error(f"Asset download error: {e}", exc_info=True)
        return APIResponse.create_error(f"Failed to download asset: {str(e)}")
    except Exception as e:
        logger.error(f"Job creation failed: {e}", exc_info=True)
        return APIResponse.create_error(f"Failed to create job: {str(e)}")


@router.get("/jobs/{job_id}", response_model=APIResponse, tags=["v3-jobs"])
async def get_job_status(
    job_id: str, current_user: Dict = Depends(verify_auth)
) -> APIResponse:
    """Get job status and progress"""
    try:
        job_raw = get_job(int(job_id))
        if job_raw is None:
            return APIResponse.create_error("Job not found")
        job_dict = cast(Dict[str, Any], job_raw)

        # Map database status (v2 style) to frontend enum (v3 style)
        status_mapping = {
            "pending": JobStatus.PENDING,
            "parsing": JobStatus.STORYBOARD_PROCESSING,
            "generating_storyboard": JobStatus.STORYBOARD_PROCESSING,
            "storyboard_ready": JobStatus.STORYBOARD_READY,
            "rendering": JobStatus.VIDEO_PROCESSING,
            "processing": JobStatus.VIDEO_PROCESSING,
            "completed": JobStatus.COMPLETED,
            "failed": JobStatus.FAILED,
            "canceled": JobStatus.CANCELLED,
            "cancelled": JobStatus.CANCELLED,
        }

        # Default to FAILED if unknown status
        v3_status = status_mapping.get(
            job_dict["status"] if "status" in job_dict else "failed", JobStatus.FAILED
        )

        # Get scenes from job_scenes table
        scenes = get_scenes_by_job(int(job_id))

        # Extract audio info from job parameters
        audio_info = {"status": "not_requested"}
        if "parameters" in job_dict:
            try:
                params = (
                    json.loads(job_dict["parameters"])
                    if isinstance(job_dict["parameters"], str)
                    else job_dict["parameters"]
                )

                if "audio_info" in params:
                    audio_info = params["audio_info"]
                elif "audio" in params:
                    audio_info = params["audio"]
                elif "generate_audio" in params and params["generate_audio"]:
                    audio_info = {"status": "processing", "requested": True}

            except (json.JSONDecodeError, KeyError) as param_error:
                logger.debug(
                    f"Could not parse audio info from job params: {param_error}"
                )

        job_data = {
            "id": str(job_dict["id"]),
            "status": v3_status,
            "progress": job_dict.get("progress"),
            "storyboard": job_dict.get("storyboard_data"),
            "scenes": scenes,
            "audio": audio_info,
            "videoUrl": job_dict.get("video_url"),
            "error": job_dict.get("error_message"),
            "estimatedCost": job_dict.get("estimated_cost"),
            "actualCost": job_dict.get("actual_cost"),
            "createdAt": job_dict.get("created_at", ""),
            "updatedAt": job_dict.get("updated_at", ""),
        }

        # Extract comprehensive audio information from job parameters and database
        audio_info = {"status": "not_requested"}

        if "parameters" in job_dict:
            try:
                params = (
                    json.loads(job_dict["parameters"])
                    if isinstance(job_dict["parameters"], str)
                    else job_dict["parameters"]
                )

                # Check for audio info in parameters
                if "audio_info" in params:
                    audio_info = params["audio_info"].copy()
                    audio_info["source"] = "job_parameters"
                elif "audio" in params:
                    audio_info = params["audio"].copy()
                    audio_info["source"] = "legacy_parameters"
                elif params.get("generate_audio", False):
                    # Audio was requested but may not be complete yet
                    audio_info = {
                        "status": "processing"
                        if v3_status in ["scene_generation", "storyboard_processing"]
                        else "requested",
                        "requested": True,
                        "source": "job_parameters",
                    }

                # Enhance with current status info
                if audio_info.get("status") == "completed" and "audio_id" in audio_info:
                    # Verify audio still exists in database
                    audio_record = get_audio_by_id(audio_info["audio_id"])
                    if audio_record:
                        audio_info["verified"] = True
                        audio_info["current_status"] = audio_record.get(
                            "status", "unknown"
                        )
                    else:
                        audio_info["status"] = "missing"
                        audio_info["verified"] = False

            except (json.JSONDecodeError, KeyError, AttributeError) as param_error:
                logger.debug(
                    f"Could not parse audio info from job {job_id} params: {param_error}"
                )
                audio_info["parse_error"] = str(param_error)

        # If audio was requested but no info found, check job metadata
        if (
            audio_info.get("requested", False)
            and audio_info.get("status") == "processing"
        ):
            # Check if audio generation is still in progress
            job_status = job_dict.get("status", "")
            if job_status in ["storyboard_ready", "video_processing", "completed"]:
                audio_info["status"] = "available_but_not_found"
                audio_info["recommendation"] = (
                    "Check job parameters or regenerate audio"
                )

        job_data = {
            "id": str(job_dict["id"]),
            "status": v3_status,
            "progress": job_dict.get("progress", 0.0),
            "storyboard": job_dict.get("storyboard_data"),
            "scenes": scenes or [],  # Ensure scenes is always a list
            "audio": audio_info,  # Comprehensive audio information
            "videoUrl": job_dict.get("video_url"),
            "error": job_dict.get("error_message"),
            "estimatedCost": job_dict.get("estimated_cost", 0.0),
            "actualCost": job_dict.get("actual_cost", None),
            "createdAt": job_dict.get("created_at", ""),
            "updatedAt": job_dict.get("updated_at", job_dict.get("created_at", "")),
        }

        # Handle storyboard data formatting with better error handling
        if job_data["storyboard"]:
            sb_data = job_data["storyboard"]
            if isinstance(sb_data, str):
                try:
                    parsed = json.loads(sb_data)
                    job_data["storyboard"] = (
                        parsed if isinstance(parsed, dict) else {"scenes": parsed}
                    )
                except json.JSONDecodeError as e:
                    logger.warning(
                        f"Failed to parse storyboard JSON for job {job_id}: {e}"
                    )
                    job_data["storyboard"] = {"error": "Invalid JSON format"}
            elif isinstance(sb_data, list):
                job_data["storyboard"] = {"scenes": sb_data}
            # If it's already a dict, leave it as-is

        # Add debugging info in development
        if get_settings().ENVIRONMENT == "development":
            job_data["_debug"] = {
                "raw_parameters": str(job_dict.get("parameters", ""))[:200] + "..."
                if len(str(job_dict.get("parameters", ""))) > 200
                else str(job_dict.get("parameters", "")),
                "parameter_parse_success": "parameters" in job_dict,
                "audio_debug": audio_info,
            }

        logger.debug(
            f"Job {job_id} status response prepared: audio status={audio_info.get('status', 'unknown')}"
        )
        return APIResponse.success(data=job_data, meta=create_api_meta())
    except Exception as e:
        import traceback

        traceback.print_exc()
        return APIResponse.create_error(f"Failed to get job status: {str(e)}")


@router.post("/jobs/{job_id}/actions", response_model=APIResponse, tags=["v3-jobs"])
async def perform_job_action(
    job_id: str,
    request: JobActionRequest,
    background_tasks: BackgroundTasks,
    current_user: Dict = Depends(verify_auth),
) -> APIResponse:
    """Perform an action on a job (approve, cancel, regenerate)"""
    try:
        job_id_int = int(job_id)

        if request.action == JobAction.APPROVE:
            # Approve storyboard and start video rendering
            success = approve_storyboard(job_id_int)
            if success:
                background_tasks.add_task(render_video_task, job_id_int)
                return APIResponse.success(
                    data={"message": "Storyboard approved, video rendering started"},
                    meta=create_api_meta(),
                )
            else:
                return APIResponse.create_error("Failed to approve storyboard")

        elif request.action == JobAction.CANCEL:
            # Cancel the job by updating status
            update_video_status(job_id_int, "cancelled")
            return APIResponse.success(
                data={"message": "Job cancelled successfully"}, meta=create_api_meta()
            )

        elif request.action == JobAction.REGENERATE_SCENE:
            # Regenerate a specific scene using payload
            payload = request.payload or {}
            scene_id = payload.get("sceneId")

            if not scene_id:
                return APIResponse.create_error(
                    "sceneId is required in payload for REGENERATE_SCENE action"
                )

            scene = get_scene_by_id(scene_id)
            if not scene:
                return APIResponse.create_error(f"Scene not found: {scene_id}")
            if str(scene["jobId"]) != job_id:
                return APIResponse.create_error("Scene does not belong to this job")

            # Get all scenes and job details for regeneration
            all_scenes = get_scenes_by_job(job_id_int)
            job = get_job(job_id_int)
            if not job:
                return APIResponse.create_error("Job not found")

            job_params = (
                json.loads(job["parameters"])
                if isinstance(job["parameters"], str)
                else job["parameters"]
            )
            ad_basics = job_params.get("ad_basics", {})
            creative_direction = job_params.get("creative", {}).get("direction", {})

            # Regenerate scene with optional feedback from payload
            feedback = payload.get("feedback", "")
            constraints = payload.get("constraints", {})

            new_scene = regenerate_scene(
                scene_number=scene["sceneNumber"],
                original_scene=scene,
                all_scenes=all_scenes,
                ad_basics=ad_basics,
                creative_direction=creative_direction,
                feedback=feedback,
                constraints=constraints,
            )

            # Update scene in database
            update_job_scene(
                scene_id=scene_id,
                description=new_scene["description"],
                script=new_scene.get("script"),
                shot_type=new_scene.get("shotType"),
                transition=new_scene.get("transition"),
                duration=new_scene.get("duration"),
                assets=new_scene.get("assets"),
                metadata=new_scene.get("metadata", {}),
            )

            updated_scene = get_scene_by_id(scene_id)
            return APIResponse.success(
                data={
                    "message": "Scene regenerated successfully",
                    "scene": updated_scene,
                },
                meta=create_api_meta(),
            )

        else:
            return APIResponse.create_error(f"Unknown action: {request.action}")

    except Exception as e:
        return APIResponse.create_error(f"Failed to perform job action: {str(e)}")


# ============================================================================
# Cost Estimation Endpoints
# ============================================================================


@router.post("/jobs/dry-run", response_model=APIResponse, tags=["v3-cost"])
async def estimate_job_cost(
    request: DryRunRequest, current_user: Dict = Depends(verify_auth)
) -> APIResponse:
    """Estimate cost for a job without creating it"""
    try:
        # Use ReplicateClient to estimate cost
        replicate_client = ReplicateClient()

        # Estimate cost (simplified: assume 5 images, 30 second video)
        estimated_cost = replicate_client.estimate_cost(num_images=5, video_duration=30)

        estimate = CostEstimate(
            estimatedCost=estimated_cost,
            currency="USD",
            breakdown={
                "storyboard_generation": estimated_cost * 0.3,
                "image_generation": estimated_cost * 0.5,
                "video_rendering": estimated_cost * 0.2,
            },
            validUntil=(
                datetime.utcnow().replace(hour=23, minute=59, second=59)
            ).isoformat()
            + "Z",
        )

        return APIResponse.success(data=estimate.dict(), meta=create_api_meta())
    except Exception as e:
        return APIResponse.create_error(f"Failed to estimate cost: {str(e)}")


# ============================================================================
# SCENE MANAGEMENT ENDPOINTS (Phase 2.5)
# ============================================================================


@router.get("/jobs/{job_id}/scenes", response_model=APIResponse, tags=["v3-scenes"])
async def list_job_scenes(
    job_id: str, current_user: Dict = Depends(verify_auth)
) -> APIResponse:
    """Get all scenes for a job"""
    try:
        scenes = get_scenes_by_job(int(job_id))
        return APIResponse.success(data={"scenes": scenes}, meta=create_api_meta())
    except Exception as e:
        logger.error(f"Failed to list scenes: {e}")
        return APIResponse.create_error(f"Failed to list scenes: {str(e)}")


@router.get(
    "/jobs/{job_id}/scenes/{scene_id}", response_model=APIResponse, tags=["v3-scenes"]
)
async def get_scene(
    job_id: str, scene_id: str, current_user: Dict = Depends(verify_auth)
) -> APIResponse:
    """Get a specific scene by ID"""
    try:
        scene = get_scene_by_id(scene_id)
        if not scene:
            return APIResponse.create_error("Scene not found")

        # Verify scene belongs to the job
        if str(scene["jobId"]) != job_id:
            return APIResponse.create_error("Scene does not belong to this job")

        return APIResponse.success(data=scene, meta=create_api_meta())
    except Exception as e:
        logger.error(f"Failed to get scene: {e}")
        return APIResponse.create_error(f"Failed to get scene: {str(e)}")


@router.put(
    "/jobs/{job_id}/scenes/{scene_id}", response_model=APIResponse, tags=["v3-scenes"]
)
async def update_scene(
    job_id: str,
    scene_id: str,
    request: Dict[str, Any],
    current_user: Dict = Depends(verify_auth),
) -> APIResponse:
    """Update a scene's details"""
    try:
        # Verify scene exists and belongs to job
        scene = get_scene_by_id(scene_id)
        if not scene:
            return APIResponse.create_error("Scene not found")
        if str(scene["jobId"]) != job_id:
            return APIResponse.create_error("Scene does not belong to this job")

        # Update scene with provided fields
        success = update_job_scene(
            scene_id=scene_id,
            description=request.get("description"),
            script=request.get("script"),
            shot_type=request.get("shotType"),
            transition=request.get("transition"),
            duration=request.get("duration"),
            assets=request.get("assets"),
            metadata=request.get("metadata"),
        )

        if not success:
            return APIResponse.create_error("Failed to update scene")

        # Get updated scene
        updated_scene = get_scene_by_id(scene_id)
        return APIResponse.success(data=updated_scene, meta=create_api_meta())
    except Exception as e:
        logger.error(f"Failed to update scene: {e}")
        return APIResponse.create_error(f"Failed to update scene: {str(e)}")


@router.post(
    "/jobs/{job_id}/scenes/{scene_id}/regenerate",
    response_model=APIResponse,
    tags=["v3-scenes"],
)
async def regenerate_scene_endpoint(
    job_id: str,
    scene_id: str,
    request: Dict[str, Any],
    current_user: Dict = Depends(verify_auth),
) -> APIResponse:
    """Regenerate a specific scene with optional feedback"""
    try:
        # Get current scene
        scene = get_scene_by_id(scene_id)
        if not scene:
            return APIResponse.create_error("Scene not found")
        if str(scene["jobId"]) != job_id:
            return APIResponse.create_error("Scene does not belong to this job")

        # Get all scenes for context
        all_scenes = get_scenes_by_job(int(job_id))

        # Get job details for ad basics and creative direction
        job = get_job(int(job_id))
        if not job:
            return APIResponse.create_error("Job not found")

        job_params = (
            json.loads(job["parameters"])
            if isinstance(job["parameters"], str)
            else job["parameters"]
        )
        ad_basics = job_params.get("ad_basics", {})
        creative_direction = job_params.get("creative", {}).get("direction", {})

        # Regenerate scene with AI
        feedback = request.get("feedback", "")
        constraints = request.get("constraints", {})

        new_scene = regenerate_scene(
            scene_number=scene["sceneNumber"],
            original_scene=scene,
            all_scenes=all_scenes,
            ad_basics=ad_basics,
            creative_direction=creative_direction,
            feedback=feedback,
            constraints=constraints,
        )

        # Update scene in database
        update_job_scene(
            scene_id=scene_id,
            description=new_scene["description"],
            script=new_scene.get("script"),
            shot_type=new_scene.get("shotType"),
            transition=new_scene.get("transition"),
            duration=new_scene.get("duration"),
            assets=new_scene.get("assets"),
            metadata=new_scene.get("metadata", {}),
        )

        # Get updated scene
        updated_scene = get_scene_by_id(scene_id)
        return APIResponse.success(data=updated_scene, meta=create_api_meta())
    except SceneGenerationError as e:
        logger.error(f"Failed to regenerate scene: {e}")
        return APIResponse.create_error(f"Failed to regenerate scene: {str(e)}")
    except Exception as e:
        logger.error(f"Failed to regenerate scene: {e}")
        return APIResponse.create_error(f"Failed to regenerate scene: {str(e)}")


@router.delete(
    "/jobs/{job_id}/scenes/{scene_id}", response_model=APIResponse, tags=["v3-scenes"]
)
async def delete_scene(
    job_id: str, scene_id: str, current_user: Dict = Depends(verify_auth)
) -> APIResponse:
    """Delete a scene"""
    try:
        # Verify scene exists and belongs to job
        scene = get_scene_by_id(scene_id)
        if not scene:
            return APIResponse.create_error("Scene not found")
        if str(scene["jobId"]) != job_id:
            return APIResponse.create_error("Scene does not belong to this job")

        # Delete scene
        success = delete_job_scene(scene_id)
        if not success:
            return APIResponse.create_error("Failed to delete scene")

        return APIResponse.success(
            data={"message": "Scene deleted successfully"}, meta=create_api_meta()
        )
    except Exception as e:
        logger.error(f"Failed to delete scene: {e}")
        return APIResponse.create_error(f"Failed to delete scene: {str(e)}")


# ============================================================================
# IMAGE PAIR SELECTION & VIDEO GENERATION ENDPOINTS (New Feature)
# ============================================================================


@router.post("/jobs/from-image-pairs", response_model=APIResponse, tags=["v3-jobs"])
async def create_job_from_image_pairs(
    request: Dict[str, Any],
    background_tasks: BackgroundTasks,
    current_user: Dict = Depends(verify_auth),
) -> APIResponse:
    """
    Create a new job that selects image pairs from campaign assets and generates videos.

    Workflow:
    1. Fetch campaign assets (images only)
    2. Use xAI Grok to select optimal image pairs
    3. Create main job with status "image_pair_selection"
    4. Store selected pairs in job parameters
    5. Trigger parallel video generation for all pairs
    6. Return job ID for polling

    Request body:
    {
        "campaignId": "campaign-uuid",
        "clientId": "client-uuid" (optional),
        "clipDuration": 5.0 (optional, seconds per clip),
        "numPairs": 10 (optional, target number of pairs)
    }
    """
    from ...services.xai_client import XAIClient
    from ...database_helpers import list_assets, get_campaign_by_id, get_client_by_id

    try:
        campaign_id = request.get("campaignId")
        client_id = request.get("clientId")
        clip_duration = request.get("clipDuration")
        num_pairs = request.get("numPairs")

        if not campaign_id:
            return APIResponse.create_error("campaignId is required")

        # Fetch campaign assets (images only)
        assets = list_assets(
            user_id=None,  # Allow access to all campaigns
            campaign_id=campaign_id,
            asset_type="image",
            limit=1000,
            offset=0,
        )

        if len(assets) < 2:
            return APIResponse.create_error(
                f"Need at least 2 image assets, but campaign has {len(assets)}"
            )

        print(f"[STDOUT DEBUG] Found {len(assets)} image assets for campaign {campaign_id}", flush=True)
        logger.info(f"Found {len(assets)} image assets for campaign {campaign_id}")

        # Get campaign context for AI selection
        print("[DEBUG 1] Getting campaign context", flush=True)
        campaign = get_campaign_by_id(campaign_id, current_user["id"])
        campaign_context = None
        if campaign:
            campaign_context = {
                "goal": campaign.get("goal"),
                "name": campaign.get("name"),
            }

        # Get client brand guidelines if available
        print("[DEBUG 2] Getting brand guidelines", flush=True)
        brand_guidelines = None
        if client_id:
            client = get_client_by_id(client_id, current_user["id"])
            if client and client.get("brandGuidelines"):
                brand_guidelines = client["brandGuidelines"]

        # Update job status
        print("[DEBUG 3] Creating job", flush=True)
        job_id = create_video_job(
            prompt=f"Image pair selection and video generation for campaign {campaign_id}",
            model_id="image-pair-workflow",
            parameters={
                "campaign_id": campaign_id,
                "client_id": client_id,
                "clip_duration": clip_duration,
                "num_pairs": num_pairs,
            },
            estimated_cost=0.0,  # Will be calculated during generation
            client_id=client_id,
            status="image_pair_selection",
        )

        print(f"[DEBUG 4] Created job {job_id}", flush=True)
        logger.info(f"Created job {job_id} for image pair workflow")
        logger.info("[DEBUG] About to initialize XAIClient - this should appear in logs!")

        # Use xAI Grok to select image pairs
        xai_client = XAIClient()

        # Prepare asset data for Grok
        logger.info(f"[IMAGE PAIRING] Preparing asset data for {len(assets)} images")

        # Image pairing is now handled by Luigi pipeline:
        # 1. AssetGroupingTask - Groups assets by room (e.g., "Bedroom 1", "Kitchen")
        # 2. GroupSelectionTask - Uses Claude to select 7 groups in narrative order
        # 3. ImagePairSelectionTask - Takes first 2 images from each selected group
        # 4. SubJobCreationTask - Creates sub-jobs for each pair
        # 5. ParallelVideoGenerationTask - Generates videos in parallel
        # 6. VideoCombinationTask - Combines all clips
        # 7. AudioGenerationTask - Generates background music
        # 8. AudioMergingTask - Merges audio with video
        # 9. VideoStorageTask - Stores final video in database
        logger.info("[PIPELINE] Triggering Luigi workflow for campaign video generation")

        # Launch Luigi workflow in background using FastAPI BackgroundTasks
        from ...workflows.runner import run_pipeline_async

        async def run_luigi_workflow():
            print(f"[DEBUG BACKGROUND] run_luigi_workflow called for job {job_id}", flush=True)
            try:
                # Log asset details in background (moved here to avoid blocking API response)
                debug_log_path = "/tmp/image_pairing_debug.log"
                with open(debug_log_path, "a") as debug_file:
                    debug_file.write(f"\n{'='*80}\n")
                    debug_file.write(f"NEW JOB - Campaign: {campaign_id}\n")
                    debug_file.write(f"Timestamp: {__import__('datetime').datetime.now().isoformat()}\n")
                    debug_file.write(f"Total assets: {len(assets)}\n")
                    debug_file.write(f"{'='*80}\n\n")

                asset_data = []
                for i, asset in enumerate(assets):
                    # Parse tags if they're stored as JSON string
                    tags = getattr(asset, "tags", [])
                    if isinstance(tags, str):
                        try:
                            tags = json.loads(tags)
                        except:
                            tags = []
                    elif tags is None:
                        tags = []

                    # Create meaningful description from available metadata
                    name = getattr(asset, "name", "")
                    description_parts = [name]
                    if tags:
                        description_parts.append(f"Tags: {', '.join(tags)}")
                    description = " | ".join(description_parts) if description_parts else "No description"

                    asset_dict = {
                        "id": asset.id,
                        "name": name,
                        "description": description,
                        "tags": tags,
                        "type": "image",
                        "url": getattr(asset, "url", ""),
                    }
                    asset_data.append(asset_dict)

                    # Log each asset
                    logger.info(
                        f"[IMAGE PAIRING] Asset {i+1}/{len(assets)}: "
                        f"id={asset.id[:12]}... name='{name}' tags={tags}"
                    )

                    # Write to debug file
                    with open(debug_log_path, "a") as debug_file:
                        debug_file.write(f"Asset {i+1}/{len(assets)}:\n")
                        debug_file.write(f"  ID: {asset.id}\n")
                        debug_file.write(f"  Name: {name}\n")
                        debug_file.write(f"  Tags: {tags}\n")
                        debug_file.write(f"  Description: {description}\n\n")

                # Log summary of asset data
                logger.info(
                    f"[ASSET SUMMARY] {len(asset_data)} assets available for campaign {campaign_id}"
                )

                print(f"[DEBUG BACKGROUND] About to call run_pipeline_async for job {job_id}", flush=True)
                logger.info(f"Starting Luigi pipeline for job {job_id}, campaign {campaign_id}")
                result = await run_pipeline_async(
                    job_id=job_id,
                    campaign_id=campaign_id,
                    clip_duration=clip_duration,
                    num_pairs=num_pairs,
                    workers=10,
                    use_local_scheduler=False  # Use central scheduler daemon
                )
                if not result.get("success"):
                    logger.error(f"Luigi workflow failed for job {job_id}: {result.get('message', 'Unknown error')}")
                    update_video_status(job_id, "failed", metadata={"error": result.get("message", "Luigi workflow failed")})
                else:
                    logger.info(f"Luigi pipeline completed successfully for job {job_id}")
            except Exception as e:
                logger.error(f"Luigi workflow exception for job {job_id}: {e}", exc_info=True)
                update_video_status(job_id, "failed", metadata={"error": str(e)})

        # Use FastAPI's background_tasks to trigger Luigi workflow
        # This ensures Luigi runs in a proper context and doesn't block the response
        print(f"[DEBUG 5] Adding background task for job {job_id}", flush=True)
        background_tasks.add_task(run_luigi_workflow)

        print(f"[DEBUG 6] Background task added for job {job_id}", flush=True)
        logger.info(f"Job {job_id} created, Luigi workflow scheduled in background")

        # Return job ID immediately for polling
        # The Luigi pipeline will handle all image pairing and video generation
        print(f"[DEBUG 7] Returning success response for job {job_id}", flush=True)
        return APIResponse.success(
            data={
                "jobId": str(job_id),
                "status": "image_pair_selection",
                "campaignId": campaign_id,
                "totalAssets": len(assets),
                "message": f"Job created successfully. Luigi pipeline is processing {len(assets)} assets from campaign {campaign_id}.",
            },
            meta=create_api_meta(),
        )

    except Exception as e:
        logger.error(f"Failed to create job from image pairs: {e}", exc_info=True)
        return APIResponse.create_error(f"Failed to create job: {str(e)}")


@router.post("/jobs/from-property-photos", response_model=APIResponse, tags=["v3-jobs"])
async def create_job_from_property_photos(
    request: PropertyVideoRequest,
    background_tasks: BackgroundTasks,
    current_user: Dict = Depends(verify_auth),
) -> APIResponse:
    """
    Create video generation job from luxury lodging property photos.

    This endpoint uses AI (Grok) to intelligently select 7 image pairs
    from crawled property photos based on predefined scene types for
    luxury hospitality marketing videos (35 seconds total, 5s per scene).

    Workflow:
    1. Call Grok to analyze all photos and select optimal pairs per scene type
    2. Store photos as assets in the campaign
    3. Create video job with 7 sub-jobs (one per scene)
    4. Launch parallel video generation
    5. Return job ID for progress tracking

    Args:
        request: PropertyVideoRequest with property info and photos
        current_user: Authenticated user from API key

    Returns:
        APIResponse with job details and Grok's selection metadata
    """
    from ...services.property_photo_selector import PropertyPhotoSelector
    from ...services.sub_job_orchestrator import process_image_pairs_to_videos
    from ...database_helpers import create_asset, get_campaign_by_id, get_client_by_id

    try:
        logger.info(
            f"Creating property video job for '{request.propertyInfo.name}' "
            f"with {len(request.photos)} photos"
        )

        # Validate campaign exists
        campaign = get_campaign_by_id(request.campaignId, current_user["id"])
        if not campaign:
            return APIResponse.create_error(f"Campaign not found: {request.campaignId}")

        # Initialize property photo selector
        selector = PropertyPhotoSelector()

        # Convert Pydantic models to dicts for selector
        property_info_dict = request.propertyInfo.model_dump()
        photos_dict = [photo.model_dump() for photo in request.photos]

        # Call Grok to select scene-based image pairs
        logger.info(f"Calling Grok to select scene pairs...")
        selection_result = selector.select_scene_image_pairs(
            property_info=property_info_dict, photos=photos_dict
        )

        logger.info(
            f"Grok selected {len(selection_result['scene_pairs'])} scene pairs "
            f"with confidence {selection_result.get('selection_metadata', {}).get('selection_confidence', 'unknown')}"
        )

        # Store photos as assets in the database
        logger.info(f"Storing {len(request.photos)} photos as assets...")
        photo_id_to_asset_id = {}

        for photo in request.photos:
            # Create asset record (photo URL will be used for video generation)
            asset_id = create_asset(
                user_id=current_user["id"],
                name=photo.filename or f"{request.propertyInfo.name}_{photo.id}",
                asset_type="image",
                url=photo.url,
                format="jpg",  # Default, can be enhanced
                client_id=campaign.get("clientId"),
                campaign_id=request.campaignId,
                tags=photo.tags or [],
                metadata={
                    "dominant_colors": photo.dominantColors or [],
                    "detected_objects": photo.detectedObjects or [],
                    "composition": photo.composition,
                    "lighting": photo.lighting,
                    "property_name": request.propertyInfo.name,
                    "property_type": request.propertyInfo.propertyType,
                },
            )
            photo_id_to_asset_id[photo.id] = asset_id

        logger.info(f"Created {len(photo_id_to_asset_id)} asset records")

        # Convert selection result to video generation format
        # Map photo IDs to asset IDs
        image_pairs = []
        for scene_pair in selection_result["scene_pairs"]:
            first_photo_id = scene_pair["first_image"]["id"]
            last_photo_id = scene_pair["last_image"]["id"]

            first_asset_id = photo_id_to_asset_id.get(first_photo_id)
            last_asset_id = photo_id_to_asset_id.get(last_photo_id)

            if not first_asset_id or not last_asset_id:
                logger.warning(
                    f"Scene {scene_pair['scene_number']}: Could not map photo IDs to assets, skipping"
                )
                continue

            score = (
                scene_pair.get("transition_analysis", {}).get(
                    "interpolation_confidence", 8.0
                )
                / 10.0
            )

            reasoning = (
                f"Scene {scene_pair['scene_number']}: {scene_pair['scene_type']}. "
                f"{scene_pair['first_image'].get('reasoning', '')} → "
                f"{scene_pair['last_image'].get('reasoning', '')}"
            )

            image_pairs.append((first_asset_id, last_asset_id, score, reasoning))

        if len(image_pairs) != 7:
            logger.warning(
                f"Expected 7 image pairs, got {len(image_pairs)}. "
                "Video may be shorter than expected."
            )

        # Create video job
        from ...database import create_video_job, update_video_status

        job_id = create_video_job(
            user_id=current_user["id"],
            prompt=f"Luxury lodging video for {request.propertyInfo.name}",
            duration=35.0,  # 7 scenes * 5 seconds
            parameters={
                "property_info": property_info_dict,
                "campaign_id": request.campaignId,
                "video_model": request.videoModel,
                "clip_duration": request.clipDuration,
                "selection_metadata": selection_result.get("selection_metadata", {}),
                "scene_pairs": selection_result["scene_pairs"],
            },
        )

        logger.info(f"Created job {job_id} for property '{request.propertyInfo.name}'")

        # Update status to indicate AI selection complete
        update_video_status(job_id, "image_pair_selection")

        # Launch parallel video generation in background using FastAPI BackgroundTasks
        async def run_orchestration():
            try:
                await process_image_pairs_to_videos(
                    job_id, image_pairs, request.clipDuration
                )
            except Exception as e:
                logger.error(f"Orchestration failed for job {job_id}: {e}", exc_info=True)
                update_video_status(job_id, "failed", metadata={"error": str(e)})

        # Schedule orchestration in background (FastAPI ensures it completes)
        background_tasks.add_task(run_orchestration)

        # Return job details with Grok's selection metadata
        return APIResponse.success(
            data={
                "jobId": job_id,
                "status": "image_pair_selection",
                "propertyName": request.propertyInfo.name,
                "totalScenes": len(image_pairs),
                "selectionMetadata": selection_result.get("selection_metadata", {}),
                "scenePairs": selection_result["scene_pairs"],
                "message": f"Job created with {len(image_pairs)} scene pairs. Video generation started.",
            },
            meta=create_api_meta(),
        )

    except Exception as e:
        logger.error(f"Failed to create property video job: {e}", exc_info=True)
        return APIResponse.create_error(
            f"Failed to create property video job: {str(e)}"
        )


@router.get("/jobs/{job_id}/sub-jobs", response_model=APIResponse, tags=["v3-jobs"])
async def get_job_sub_jobs(
    job_id: str, current_user: Dict = Depends(verify_auth)
) -> APIResponse:
    """
    Get all sub-jobs for a job with their individual status.

    This endpoint provides detailed progress tracking for parallel video generation.

    Returns:
    {
        "data": {
            "subJobs": [...],  // Array of SubJob objects
            "summary": {
                "total": 10,
                "pending": 2,
                "processing": 3,
                "completed": 4,
                "failed": 1
            }
        }
    }
    """
    from ...database import get_sub_jobs_by_job, get_sub_job_progress_summary

    try:
        job_id_int = int(job_id)

        # Get all sub-jobs
        sub_jobs = get_sub_jobs_by_job(job_id_int)

        # Get summary
        summary = get_sub_job_progress_summary(job_id_int)

        return APIResponse.success(
            data={"subJobs": sub_jobs, "summary": summary}, meta=create_api_meta()
        )

    except ValueError:
        return APIResponse.create_error("Invalid job ID format")
    except Exception as e:
        logger.error(f"Failed to get sub-jobs for job {job_id}: {e}")
        return APIResponse.create_error(f"Failed to get sub-jobs: {str(e)}")


# ============================================================================
# VIDEO SERVING ENDPOINTS
# ============================================================================


@router.get("/videos/{job_id}/clips/{clip_filename}", tags=["v3-videos"])
async def get_video_clip(job_id: str, clip_filename: str):
    """Serve individual video clips from storage"""
    from fastapi.responses import FileResponse
    from pathlib import Path

    try:
        video_path = Path(settings.VIDEO_STORAGE_PATH) / job_id / "clips" / clip_filename

        if not video_path.exists():
            raise HTTPException(status_code=404, detail="Video clip not found")

        return FileResponse(
            path=str(video_path),
            media_type="video/mp4",
            headers={
                "Accept-Ranges": "bytes",
                "Cache-Control": "public, max-age=31536000",
            },
        )
    except Exception as e:
        logger.error(f"Failed to serve video clip {clip_filename} for job {job_id}: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to serve video: {str(e)}")


@router.get("/videos/{job_id}/combined", tags=["v3-videos"])
async def get_combined_video(job_id: str):
    """Serve combined video from database blob storage"""
    from fastapi.responses import Response
    from ...database import get_db

    try:
        # Get video blob from database
        with get_db() as conn:
            row = conn.execute(
                """
                SELECT video_data
                FROM generated_videos
                WHERE id = ?
                """,
                (job_id,),
            ).fetchone()

            if not row or not row["video_data"]:
                raise HTTPException(status_code=404, detail="Combined video not found")

            video_data = row["video_data"]

        return Response(
            content=video_data,
            media_type="video/mp4",
            headers={
                "Accept-Ranges": "bytes",
                "Cache-Control": "public, max-age=31536000",
                "Content-Length": str(len(video_data)),
            },
        )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to serve combined video for job {job_id}: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to serve video: {str(e)}")


@router.get("/ai-videos", response_model=APIResponse, tags=["v3-jobs"])
async def list_ai_generated_videos(
    limit: int = 20,
    offset: int = 0,
    status: str = "completed",
    current_user: Dict = Depends(verify_auth),
) -> APIResponse:
    """
    List AI-generated videos from the image pair selection pipeline.

    Query parameters:
    - limit: Number of videos to return (default: 20, max: 100)
    - offset: Pagination offset (default: 0)
    - status: Filter by status (default: "completed", options: "all", "completed", "processing", "failed")

    Returns:
    {
        "data": {
            "videos": [...],  // Array of video records
            "total": 42       // Total count matching filter
        }
    }
    """
    from ...database import get_db

    try:
        # Validate and limit pagination
        limit = min(limit, 100)
        offset = max(offset, 0)

        with get_db() as conn:
            # Build query based on status filter
            where_clause = "WHERE status = ?" if status != "all" else ""
            params = [status] if status != "all" else []

            # Get total count
            count_query = f"""
                SELECT COUNT(*) FROM video_sub_jobs {where_clause}
            """
            cursor = conn.execute(count_query, params)
            total = cursor.fetchone()[0]

            # Get videos with pagination
            query = f"""
                SELECT
                    id,
                    job_id,
                    sub_job_number,
                    model_id,
                    video_url,
                    status,
                    duration_seconds,
                    progress,
                    error_message,
                    created_at,
                    completed_at,
                    input_parameters
                FROM video_sub_jobs
                {where_clause}
                ORDER BY created_at DESC
                LIMIT ? OFFSET ?
            """
            params.extend([limit, offset])
            cursor = conn.execute(query, params)

            videos = []
            for row in cursor.fetchall():
                try:
                    input_params = json.loads(row[11]) if row[11] else {}
                except json.JSONDecodeError:
                    logger.warning(
                        f"Invalid JSON in input_parameters for sub-job {row[0]}"
                    )
                    input_params = {}

                # Handle potential None values
                safe_row = [r if r is not None else "" for r in row]

                video_record = {
                    "id": str(safe_row[0]),
                    "jobId": safe_row[1],
                    "subJobNumber": safe_row[2],
                    "modelId": safe_row[3] or "unknown",
                    "videoUrl": safe_row[4] or "",
                    "status": safe_row[5] or "unknown",
                    "durationSeconds": safe_row[6] or 0.0,
                    "progress": safe_row[7] or 0.0,
                    "errorMessage": safe_row[8],
                    "createdAt": safe_row[9] or "",
                    "completedAt": safe_row[10] or "",
                    "prompt": input_params.get(
                        "prompt", f"Job {safe_row[1]} - Clip {safe_row[2]}"
                    ),
                    "thumbnailUrl": safe_row[4]
                    or "",  # Use video URL as thumbnail for now
                    "assetIds": input_params.get(
                        "asset_ids", []
                    ),  # Include source assets
                }

                # Add audio info if available in parameters
                if "audio_info" in input_params:
                    video_record["audioInfo"] = input_params["audio_info"]

                videos.append(video_record)

            logger.info(
                f"Retrieved {len(videos)} AI videos for offset {offset}, limit {limit}"
            )

            return APIResponse.success(
                data={"videos": videos, "total": total}, meta=create_api_meta()
            )

    except Exception as e:
        logger.error(f"Failed to list AI-generated videos: {e}", exc_info=True)
        return APIResponse.create_error(f"Failed to list AI-generated videos: {str(e)}")
</file>

</files>
